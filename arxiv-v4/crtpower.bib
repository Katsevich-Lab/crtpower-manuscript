Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Barber2020,
author = {Barber, Rina Foygel and Janson, Lucas},
file = {:Users/ekatsevi/papers/Annals of Statistics, to appear/Barber, Janson - 2022 - Testing goodness-of-fit and conditional independence with approximate co-sufficient sampling.pdf:pdf},
journal = {Annals of Statistics, to appear},
keywords = {approximate sufficiency,co-sufficiency,conditional independence testing,conditional ran-,conditional randomization test,domization test,goodness-of-fit test,high-dimensional inference,high-dimensional regression,model-X,model-x},
mendeley-tags = {conditional randomization test,high-dimensional regression,model-X},
title = {{Testing goodness-of-fit and conditional independence with approximate co-sufficient sampling}},
year = {2022}
}
@book{Imbens2015,
abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including, matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
author = {Imbens, Guido W. and Rubin, Donald B.},
doi = {10.1017/CBO9781139025751},
file = {:Users/ekatsevi/papers/Unknown/Imbens, Rubin - 2015 - Causal inference For statistics, social, and biomedical sciences an introduction.pdf:pdf},
isbn = {9781139025751},
keywords = {causality},
mendeley-tags = {causality},
publisher = {Cambridge University Press},
title = {{Causal inference: For statistics, social, and biomedical sciences an introduction}},
year = {2015}
}
@article{Li2021,
abstract = {This paper develops a method based on model-X knockoffs to find conditional associations that are consistent across diverse environments, controlling the false discovery rate. The motivation for this problem is that large data sets may contain numerous associations that are statistically significant and yet misleading, as they are induced by confounders or sampling imperfections. However, associations consistently replicated under different conditions may be more interesting. In fact, consistency sometimes provably leads to valid causal inferences even if conditional associations do not. While the proposed method is flexible and can be deployed in a wide range of applications, this paper highlights its relevance to genome-wide association studies, in which consistency across populations with diverse ancestries mitigates confounding due to unmeasured variants. The effectiveness of this approach is demonstrated by simulations and applications to the UK Biobank data.},
archivePrefix = {arXiv},
arxivId = {2106.04118},
author = {Li, Shuangning and Sesia, Matteo and Romano, Yaniv and Cand{\`{e}}s, Emmanuel and Sabatti, Chiara},
eprint = {2106.04118},
file = {:Users/ekatsevi/papers/arXiv/Li et al. - 2021 - Searching for consistent associations with a multi-environment knockoff filter.pdf:pdf},
journal = {arXiv},
keywords = {FDR,GWAS,causality,conditional independence,false discovery rate,genome-wide association studies,knockoffs},
mendeley-tags = {FDR,GWAS,causality,knockoffs},
title = {{Searching for consistent associations with a multi-environment knockoff filter}},
url = {http://arxiv.org/abs/2106.04118},
year = {2021}
}
@book{VanderLaan2011,
address = {New York},
author = {van der Laan, Mark J. and Rose, Sherri},
doi = {10.1007/978-3-319-65304-4},
file = {:Users/ekatsevi/papers/Unknown/van der Laan, Rose - 2011 - Targeted learning Causal inference for observational and experimental data.pdf:pdf},
isbn = {978-3-319-65303-7},
keywords = {causality},
mendeley-tags = {causality},
publisher = {Springer},
title = {{Targeted learning: Causal inference for observational and experimental data}},
url = {https://www.springer.com/gp/book/9783319653037{\%}0Ahttp://link.springer.com/10.1007/978-3-319-65304-4},
year = {2011}
}
@article{T96,
author = {Tibshirani, Robert},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Lasso,canonical,high-dimensional regression},
mendeley-tags = {Lasso,canonical,high-dimensional regression},
number = {1},
pages = {267--288},
publisher = {JSTOR},
title = {{Regression shrinkage and selection via the lasso}},
volume = {58},
year = {1996}
}
@article{Bayati2011,
abstract = {We consider the problem of learning a coefficient vector x 0 ∈ ℝ N from noisy linear observation y = Ax 0 + w ∈ ℝ n. In many contexts (ranging from model selection to image processing), it is desirable to construct a sparse estimator x̂. In this case, a popular approach consists in solving an ℓ 1 -penalized least-squares problem known as the LASSO or basis pursuit denoising. For sequences of matrices of increasing dimensions,with independent Gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical model ideas. Simulations on real data matrices suggest that our results can be relevant in a broad array of practical applications. {\textcopyright} 2006 IEEE.},
archivePrefix = {arXiv},
arxivId = {1008.2581},
author = {Bayati, Mohsen and Montanari, Andrea},
doi = {10.1109/TIT.2011.2174612},
eprint = {1008.2581},
file = {:Users/ekatsevi/papers/IEEE Transactions on Information Theory/Bayati, Montanari - 2011 - The LASSO risk for Gaussian matrices.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Compressed sensing,graphical models,message passing algorithms,random matrix theory,state evolution,statistical learning},
number = {4},
pages = {1997--2017},
publisher = {IEEE},
title = {{The LASSO risk for Gaussian matrices}},
volume = {58},
year = {2011}
}
@article{Kim2020,
abstract = {When data analysts train a classifier and check if its accuracy is significantly different from a half, they are implicitly performing a two-sample test. We investigate the statistical optimality of this indirect but flexible method in the high-dimensional setting of {\$}d/n \backslashto c \backslashin (0,\backslashinfty){\$}. We provide a concrete answer for the case of distinguishing Gaussians with mean-difference {\$}\backslashdelta{\$} and common (known or unknown) covariance {\$}\backslashSigma{\$}, by contrasting the indirect approach using variants of linear discriminant analysis (LDA) such as naive Bayes, with the direct approach using corresponding variants of Hotelling's test. Somewhat surprisingly, the indirect approach achieves the same power as the direct approach in terms of {\$}n,d,\backslashdelta,\backslashSigma{\$}, and is only worse by a constant factor, achieving an asymptotic relative efficiency of {\$}1/\backslashpi{\$} for the balanced sample case. Other results of independent interest are provided, such as minimax lower bounds, and optimality of Hotelling's test when {\$}d=o(n){\$}. Simulation results validate our theory, and we present practical takeaway messages along with several open problems.},
archivePrefix = {arXiv},
arxivId = {1602.02210},
author = {Kim, Ilmun and Ramdas, Aaditya and Singh, Aarti and Wasserman, Larry},
eprint = {1602.02210},
file = {:Users/ekatsevi/papers/Annals of Statistics, to appear/Kim et al. - 2020 - Classification accuracy as a proxy for two sample testing.pdf:pdf},
journal = {Annals of Statistics, to appear},
title = {{Classification accuracy as a proxy for two sample testing}},
url = {http://arxiv.org/abs/1602.02210},
year = {2020}
}
@article{Robins1992,
author = {Robins, James M. and Mark, Steven D. and Newey, Whitney K.},
file = {:Users/ekatsevi/papers/Biometrics/Robins, Mark, Newey - 1992 - Estimating Exposure Effects by Modelling the Expectation of Exposure Conditional on Confounders.pdf:pdf},
journal = {Biometrics},
keywords = {causality},
mendeley-tags = {causality},
number = {2},
pages = {479--495},
title = {{Estimating Exposure Effects by Modelling the Expectation of Exposure Conditional on Confounders}},
volume = {48},
year = {1992}
}
@article{Zhao2021,
abstract = {Fisher's randomization test (FRT) delivers exact p-values under the strong null hypothesis of no treatment effect on any units whatsoever and allows for flexible covariate adjustment to improve the power. Of interest is whether the resulting covariate-adjusted procedure could also be valid for testing the weak null hypothesis of zero average treatment effect. To this end, we evaluate two general strategies for conducting covariate adjustment in FRTs: the pseudo-outcome strategy that uses the residuals from an outcome model with only the covariates as the pseudo, covariate-adjusted outcomes to form the test statistic, and the model-output strategy that directly uses the output from an outcome model with both the treatment and covariates as the covariate-adjusted test statistic. Based on theory and simulation, we recommend using the ordinary least squares (OLS) fit of the observed outcome on the treatment, centered covariates, and their interactions for covariate adjustment, and conducting FRT with the robust t-value of the treatment as the test statistic. The resulting FRT is finite-sample exact for testing the strong null hypothesis, asymptotically valid for testing the weak null hypothesis, and more powerful than the unadjusted counterpart under alternatives, all irrespective of whether the linear model is correctly specified or not. We start with complete randomization, and then extend the theory to cluster randomization, stratified randomization, and rerandomization, respectively, giving a recommendation for the test procedure and test statistic under each design. Our theory is design-based, also known as randomization-based, in which we condition on the potential outcomes but average over the random treatment assignment.},
archivePrefix = {arXiv},
arxivId = {2010.14555},
author = {Zhao, Anqi and Ding, Peng},
doi = {10.1016/j.jeconom.2021.04.007},
eprint = {2010.14555},
file = {:Users/ekatsevi/papers/Journal of Econometrics/Zhao, Ding - 2021 - Covariate-adjusted Fisher randomization tests for the average treatment effect.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Finite-population inference,Permutation test,Randomization distribution,Robust standard error,Studentization,Super-population inference},
title = {{Covariate-adjusted Fisher randomization tests for the average treatment effect}},
volume = {94720},
year = {2021}
}
@article{FoygelBarber2019a,
abstract = {We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally rather than marginally. Existing methods such as conformal prediction offer marginal coverage guarantees, where predictive coverage holds on average over all possible test points, but this is not sufficient for many practical applications where we would like to know that our predictions are valid for a given individual, not merely on average over a population. On the other hand, exact conditional inference guarantees are known to be impossible without imposing assumptions on the underlying distribution. In this work we aim to explore the space in between these two, and examine what types of relaxations of the conditional coverage property would alleviate some of the practical concerns with marginal coverage guarantees while still being possible to achieve in a distribution-free setting.},
archivePrefix = {arXiv},
arxivId = {1903.04684v1},
author = {{Foygel Barber}, Rina and Cand{\`{e}}s, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
eprint = {1903.04684v1},
file = {:Users/ekatsevi/papers/arXiv/Foygel Barber et al. - 2019 - The limits of distribution-free conditional predictive inference.pdf:pdf},
journal = {arXiv},
keywords = {conformal prediction,to-skim,unpublished},
mendeley-tags = {conformal prediction,to-skim,unpublished},
title = {{The limits of distribution-free conditional predictive inference}},
year = {2019}
}
@article{CetL16,
author = {Cand{\`{e}}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Cand{\`{e}}s et al. - 2018 - Panning for gold `model-X' knockoffs for high dimensional controlled variable selection.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {FDR,Multiple testing,high-dimensional regression,knockoffs,model-X,variable selection},
mendeley-tags = {FDR,Multiple testing,high-dimensional regression,knockoffs,model-X,variable selection},
number = {3},
pages = {551--577},
publisher = {Wiley Online Library},
title = {{Panning for gold: `model-X' knockoffs for high dimensional controlled variable selection}},
volume = {80},
year = {2018}
}
@article{Robinson1988,
author = {Robinson, P. M.},
file = {:Users/ekatsevi/papers/Econometrica/Robinson - 1988 - Root-N-Consistent Semiparametric Regression.pdf:pdf},
journal = {Econometrica},
keywords = {causality},
mendeley-tags = {causality},
number = {4},
pages = {931--954},
title = {{Root-N-Consistent Semiparametric Regression}},
volume = {56},
year = {1988}
}
@article{Lang1986,
author = {Lang, Robert},
doi = {10.1007/BF01202504},
file = {:Users/ekatsevi/papers/Archiv der Mathematik/Lang - 1986 - A note on the measurability of convex sets.pdf:pdf},
issn = {0003889X},
journal = {Archiv der Mathematik},
number = {1},
pages = {90--92},
title = {{A note on the measurability of convex sets}},
volume = {47},
year = {1986}
}
@article{Buja2019,
author = {Buja, Andreas and Brown, Lawrence and Kuchibhotla, Arun Kumar and Berk, Richard and George, Edward and Zhao, Linda},
file = {:Users/ekatsevi/papers/Statistical Science/Buja et al. - 2019 - Models as Approximations II A Model-Free Theory of Parametric.pdf:pdf},
journal = {Statistical Science},
keywords = {Ancillarity of regressors,ancillarity of regressors,bagging,bootstrap,metrics,misspecification,sandwich estimator},
number = {4},
pages = {545--565},
title = {{Models as Approximations II : A Model-Free Theory of Parametric}},
volume = {34},
year = {2019}
}
@article{Zhang2020,
author = {Zhang, Lu and Janson, Lucas},
file = {:Users/ekatsevi/papers/arXiv/Zhang, Janson - 2020 - Floodgate inference for model-free variable importance.pdf:pdf},
journal = {arXiv},
keywords = {confidence intervals,effect size,heritability,heterogeneous treatment effects,high-dimensional regression,model-X,model-x,variable importance},
mendeley-tags = {confidence intervals,high-dimensional regression,model-X},
pages = {1--67},
title = {{Floodgate : inference for model-free variable importance}},
year = {2020}
}
@article{lei2016adapt,
author = {Lei, Lihua and Fithian, William},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {FDR,Multiple testing,structured multiple testing},
mendeley-tags = {FDR,Multiple testing,structured multiple testing},
number = {4},
pages = {649--679},
publisher = {Wiley Online Library},
title = {{AdaPT: an interactive procedure for multiple testing with side information}},
volume = {80},
year = {2018}
}
@article{Shah2018,
abstract = {It is a common saying that testing for conditional independence, i.e., testing whether whether two random vectors {\$}X{\$} and {\$}Y{\$} are independent, given {\$}Z{\$}, is a hard statistical problem if {\$}Z{\$} is a continuous random variable (or vector). In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Valid statistical tests are required to have a size that is smaller than a predefined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the non-existence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem may be judged easily. To address this need, we propose in the case where {\$}X{\$} and {\$}Y{\$} are univariate to nonlinearly regress {\$}X{\$} on {\$}Z{\$}, and {\$}Y{\$} on {\$}Z{\$} and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means {\$}X{\$} given {\$}Z{\$}, and {\$}Y{\$} given {\$}Z{\$}, at a slow rate. We extend the methodology to handle settings where {\$}X{\$} and {\$}Y{\$} may be multivariate or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code is available as the R package GeneralisedCovarianceMeasure on CRAN.},
archivePrefix = {arXiv},
arxivId = {1804.07203},
author = {Shah, Rajen D. and Peters, Jonas},
eprint = {1804.07203},
file = {:Users/ekatsevi/papers/Annals of Statistics, to appear/Shah, Peters - 2020 - The Hardness of Conditional Independence Testing and the Generalised Covariance Measure.pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics, to appear},
keywords = {causality,conditional randomization test},
mendeley-tags = {causality,conditional randomization test},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
url = {http://arxiv.org/abs/1804.07203},
year = {2020}
}
@article{Rosenbaum1983,
abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
author = {Rosenbaum, Paul R. and Rubin, Donald B.},
doi = {10.1017/CBO9780511810725.016},
file = {:Users/ekatsevi/papers/Biometrika/Rosenbaum, Rubin - 1983 - The central role of the propensity score in observational studies for causal effects.pdf:pdf},
isbn = {9780511810725},
journal = {Biometrika},
keywords = {causality},
mendeley-tags = {causality},
number = {1},
pages = {41--55},
title = {{The central role of the propensity score in observational studies for causal effects}},
volume = {70},
year = {1983}
}
@book{Hayashi2000,
abstract = {This book is designed to serve as the textbook for a first-year graduate course in econometrics. It has two distinguishing features. First, it covers a full range of techniques with the estimation method called the Generalized Method of Moments (GMM) as the organizing principle. I believe this unified approach is the most efficient way to cover the first-year materials in an accessible yet rigorous manner. Second, most chapters include a section examining in detail original applied arti- cles from such diverse fields in economics as industrial organization, labor, finance, international, and macroeconomics. So the reader will know how to use the tech- niques covered in the chapter and under what conditions they are applicable. Over the last several years, the lecture notes on which this book is based have been used at the University of Pennsylvania, Columbia University, Prince- ton University, the University of Tokyo, Boston College, Harvard University, and Ohio State University. Students seem to like the book a lot. My own experience from teaching out of the book is that students think the book is better than the instructor.},
author = {Hayashi, Fumio},
file = {:Users/ekatsevi/papers/Unknown/Hayashi - 2000 - Econometrics.pdf:pdf},
isbn = {9780691010182},
publisher = {Princeton University Press},
title = {{Econometrics}},
year = {2000}
}
@article{Celentano2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2007.13716v1},
author = {Celentano, Michael and Montanari, Andrea and Wei, Yuting},
eprint = {arXiv:2007.13716v1},
file = {:Users/ekatsevi/papers/arXiv/Celentano, Montanari, Wei - 2020 - The Lasso with general Gaussian designs with applications to hypothesis testing.pdf:pdf},
journal = {arXiv},
keywords = {AMP,Lasso,conditional randomization test,high-dimensional regression},
mendeley-tags = {AMP,Lasso,conditional randomization test,high-dimensional regression},
title = {{The Lasso with general Gaussian designs with applications to hypothesis testing}},
year = {2020}
}
@article{Tansey2018,
abstract = {We consider the problem of feature selection using black box predictive models. For example, high-throughput devices in science are routinely used to gather thousands of features for each sample in an experiment. The scientist must then sift through the many candidate features to find explanatory signals in the data, such as which genes are associated with sensitivity to a prospective therapy. Often, predictive models are used for this task: the model is fit, error on held out data is measured, and strong performing models are assumed to have discovered some fundamental properties of the system. A model-specific heuristic is then used to inspect the model parameters and rank important features, with top features reported as "discoveries." However, such heuristics provide no statistical guarantees and can produce unreliable results. We propose the holdout randomization test (HRT) as a principled approach to feature selection using black box predictive models. The HRT is model agnostic and produces a valid p-value for each feature, enabling control over the false discovery rate (or Type I error) for any predictive model. Further, the HRT is computationally efficient and, in simulations, has greater power than a competing knockoffs-based approach. Code is available at https://github.com/tansey/hrt.},
archivePrefix = {arXiv},
arxivId = {1811.00645},
author = {Tansey, Wesley and Veitch, Victor and Zhang, Haoran and Rabadan, Raul and Blei, David M.},
eprint = {1811.00645},
file = {:Users/ekatsevi/papers/Journal of Computational and Graphical Statistics/Tansey et al. - 2022 - The Holdout Randomization Test for Feature Selection in Black Box Models.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
keywords = {conditional randomization test,high-dimensional regression,variable selection},
mendeley-tags = {conditional randomization test,high-dimensional regression,variable selection},
number = {1},
pages = {151--162},
title = {{The Holdout Randomization Test for Feature Selection in Black Box Models}},
url = {http://arxiv.org/abs/1811.00645},
volume = {31},
year = {2022}
}
@article{Davies2019,
abstract = {In this paper we propose a completely new approach to the problem of covariate selection in linear regression. It is intuitive, very simple, fast and powerful, non-frequentist and non-Bayesian. It does not overfit, there is no shrinkage of the least squares coefficients, and it is model-free. A covariate or a set of covariates is included only if they are better in the sense of least squares than the same number of Gaussian covariates consisting of i.i.d. {\$}N(0,1){\$} random variables. The degree to which they are better is measured by the P-value which is the probability that the Gaussian covariates are better. This probability is given in terms of the Beta distribution, it is exact and it holds for the data at hand whatever this may be. The idea extends to a stepwise procedure, the main contribution of the paper, where the best of the remaining covariates is only accepted if it is better than the best of the same number of random Gaussian covariates. Again this probability is given in terms of the Beta distribution, it is exact and it holds for the data at hand whatever this may be. We use a version with default parameters which works for a large collection of known data sets with up to a few hundred thousand covariates. The computing time for the largest data sets was about four seconds, and it outperforms all other selection procedures of which we are aware. The paper gives the results of simulations, applications to real data sets and theorems on the asymptotic behaviour under the standard linear model. An R-package {\{}$\backslash$it gausscov{\}} is available. $\backslash$},
archivePrefix = {arXiv},
arxivId = {1906.01990},
author = {Davies, Laurie and D{\"{u}}mbgen, Lutz},
eprint = {1906.01990},
file = {:Users/ekatsevi/papers/arXiv/Davies, D{\"{u}}mbgen - 2019 - Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities.pdf:pdf},
journal = {arXiv},
title = {{Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities}},
url = {http://arxiv.org/abs/1906.01990},
year = {2019}
}
@article{Hemerik2018,
abstract = {When permutation methods are used in practice, often a limited number of random permutations are used to decrease the computational burden. However, most theoretical literature assumes that the whole permutation group is used, and methods based on random permutations tend to be seen as approximate. There exists a very limited amount of literature on exact testing with random permutations, and only recently a thorough proof of exactness was given. In this paper, we provide an alternative proof, viewing the test as a “conditional Monte Carlo test” as it has been called in the literature. We also provide extensions of the result. Importantly, our results can be used to prove properties of various multiple testing procedures based on random permutations.},
archivePrefix = {arXiv},
arxivId = {1411.7565},
author = {Hemerik, Jesse and Goeman, Jelle},
doi = {10.1007/s11749-017-0571-1},
eprint = {1411.7565},
file = {:Users/ekatsevi/papers/Test/Hemerik, Goeman - 2018 - Exact testing with random permutations.pdf:pdf},
isbn = {1174901705711},
issn = {11330686},
journal = {Test},
keywords = {Nonparametric test,Permutation test,Resampling},
number = {4},
pages = {811--825},
publisher = {Springer Berlin Heidelberg},
title = {{Exact testing with random permutations}},
url = {https://doi.org/10.1007/s11749-017-0571-1},
volume = {27},
year = {2018}
}
@article{Weinstein2017,
abstract = {Knockoffs is a new framework for controlling the false discovery rate (FDR) in multiple hypothesis testing problems involving complex statistical models. While there has been great emphasis on Type-I error control, Type-II errors have been far less studied. In this paper we analyze the false negative rate or, equivalently, the power of a knockoff procedure associated with the Lasso solution path under an i.i.d. Gaussian design, and find that knockoffs asymptotically achieve close to optimal power with respect to an omniscient oracle. Furthermore, we demonstrate that for sparse signals, performing model selection via knockoff filtering achieves nearly ideal prediction errors as compared to a Lasso oracle equipped with full knowledge of the distribution of the unknown regression coefficients. The i.i.d. Gaussian design is adopted to leverage results concerning the empirical distribution of the Lasso estimates, which makes power calculation possible for both knockoff and oracle procedures.},
archivePrefix = {arXiv},
arxivId = {1712.06465},
author = {Weinstein, Asaf and Barber, Rina and Candes, Emmanuel},
eprint = {1712.06465},
file = {:Users/ekatsevi/papers/arXiv/Weinstein, Barber, Candes - 2017 - A power analysis for knockoffs under Gaussian designs.pdf:pdf},
journal = {arXiv},
title = {{A power analysis for knockoffs under Gaussian designs}},
url = {http://arxiv.org/abs/1712.06465},
year = {2017}
}
@book{TSH,
address = {New York},
author = {Lehmann, E. L. and Romano, Joseph P.},
doi = {10.2307/2982206},
edition = {Third},
file = {:Users/ekatsevi/papers/Unknown/Lehmann, Romano - 2005 - Testing Statistical Hypotheses.pdf:pdf},
isbn = {0387988645},
issn = {09641998},
publisher = {Springer},
title = {{Testing Statistical Hypotheses}},
year = {2005}
}
@article{Lin2013,
abstract = {Freedman [Adv. in Appl. Math. 40 (2008) 180-193; Ann. Appl. Stat. 2 (2008) 176-196] critiqued ordinary least squares regression adjustment of estimated treatment effects in randomized experiments, using Neyman's model for randomization inference. Contrary to conventional wisdom, he argued that adjustment can lead to worsened asymptotic precision, invalid measures of precision, and small-sample bias. This paper shows that in sufficiently large samples, those problems are either minor or easily fixed. OLS adjustment cannot hurt asymptotic precision when a full set of treatment-covariate interactions is included. Asymptotically valid confidence intervals can be constructed with the Huber-White sandwich standard error estimator. Checks on the asymptotic approximations are illustrated with data from Angrist, Lang, and Oreopoulos's [Am. Econ. J.: Appl. Econ. 1:1 (2009) 136-163] evaluation of strategies to improve college students' achievement. The strongest reasons to support Freedman's preference for unadjusted estimates are transparency and the dangers of specification search. {\textcopyright} 2013 Institute of Mathematical Statistics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.2301v2},
author = {Lin, Winston},
doi = {10.1214/12-AOAS583},
eprint = {arXiv:1208.2301v2},
file = {:Users/ekatsevi/papers/Annals of Applied Statistics/Lin - 2013 - Agnostic notes on regression adjustments to experimental data Reexamining Freedman's critique.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Analysis of covariance,Covariate adjustment,Program evaluation,Randomization inference,Robust standard errors,Sandwich estimator,Social experiments,causality},
mendeley-tags = {causality},
number = {1},
pages = {295--318},
title = {{Agnostic notes on regression adjustments to experimental data: Reexamining Freedman's critique}},
volume = {7},
year = {2013}
}
@article{Rosenbaum2002,
abstract = {By slightly reframing the concept of covariance adjustment in randomized experiments, a method of exact permutation inference is derived that is entirely free of distributional assumptions and uses the random assignment of treatments as the "reasoned basis for inference." This method of exact permutation inference may be used with many forms of covariance adjustment, including robust regression and locally weighted smoothers. The method is then generalized to observational studies where treatments were not randomly assigned, so that sensitivity to hidden biases must be examined. Adjustments using an instrumental variable are also discussed. The methods are illustrated using data from two observational studies.},
author = {Rosenbaum, Paul R.},
doi = {10.1214/ss/1042727942},
file = {:Users/ekatsevi/papers/Statistical Science/Rosenbaum - 2002 - Covariance adjustment in randomized experiments and observational studies.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Covariance adjustment,Matching,Observational studies,Permutation inference,Propensity score,Randomization inference,Sensitivity analysis},
number = {3},
pages = {286--327},
title = {{Covariance adjustment in randomized experiments and observational studies}},
volume = {17},
year = {2002}
}
@book{VanderLaan2003,
address = {New York},
author = {van der Laan, Mark J. and Robins, James M.},
file = {:Users/ekatsevi/papers/Unknown/van der Laan, Robins - 2003 - Unified methods for censored longitudinal data and causality.pdf:pdf},
isbn = {1-4419-0319-8},
publisher = {Springer-Verlag},
title = {{Unified methods for censored longitudinal data and causality}},
year = {2003}
}
@article{Li2011,
abstract = {We describe a novel approach to nonparametric point and interval estimation of a treatment effect in the presence of many continuous confounders. We show that the problem can be reduced to that of point and interval estimation of the expected conditional covariance between treatment and response given the confounders. Our estimators are higher order U-statistics. The approach applies equally to the regular case where the expected conditional covariance is root-n estimable and to the irregular case where slower nonparametric rates prevail.},
author = {Li, Lingling and {Tchetgen Tchetgen}, Eric and van der Vaart, Aad and Robins, James M.},
doi = {10.1016/j.spl.2011.02.030},
file = {:Users/ekatsevi/papers/Statistics and Probability Letters/Li et al. - 2011 - Higher order inference on a treatment effect under low regularity conditions.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Influence functions,Minimax,Nonparametric,Semiparametric,U-statistics,causality},
mendeley-tags = {causality},
number = {7},
pages = {821--828},
title = {{Higher order inference on a treatment effect under low regularity conditions}},
volume = {81},
year = {2011}
}
@article{Bates2019,
abstract = {Model-X knockoffs is a wrapper that transforms essentially any feature importance measure into a variable selection algorithm, which discovers true effects while rigorously controlling the expected fraction of false positives. A frequently discussed challenge to apply this method is to construct knockoff variables, which are synthetic variables obeying a crucial exchangeabil-ity property with the explanatory variables under study. This paper introduces techniques for knockoff generation in great generality: we provide a sequential characterization of all possible knockoff distributions, which leads to a Metropolis-Hastings formulation of an exact knockoff sampler. We further show how to use conditional independence structure to speed up computations. Combining these two threads, we introduce an explicit set of sequential algorithms and empirically demonstrate their effectiveness. Our theoretical analysis proves that our algorithms achieve near-optimal computational complexity in certain cases. The techniques we develop are sufficiently rich to enable knockoff sampling in challenging models including cases where the covariates are continuous and heavy-tailed, and follow a graphical model such as the Ising model.},
archivePrefix = {arXiv},
arxivId = {1903.00434v1},
author = {Bates, Stephen and Cand{\`{e}}s, Emmanuel and Janson, Lucas and Wang, Wenshuo},
eprint = {1903.00434v1},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Bates et al. - 2020 - Metropolized Knockoff Sampling.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {FDR,False discovery rate (FDR),Ising model,Markov chain,Metropolis-Hastings,Multiple testing,graphical models,junction tree,knockoffs,model-X,to-skim,treewidth},
mendeley-tags = {FDR,Multiple testing,knockoffs,model-X,to-skim},
title = {{Metropolized Knockoff Sampling}},
year = {2020}
}
@article{Chernozhukov2018,
abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter $\theta$0 in the presence of high-dimensional nuisance parameters $\eta$0. We depart from the classical setting by allowing for $\eta$0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate $\eta$0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating $\eta$0 cause a heavy bias in estimators of $\theta$0 that are obtained by naively plugging ML estimators of $\eta$0 into estimating equations for $\theta$0. This bias results in the naive estimator failing to be N-1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest $\theta$0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate $\theta$0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N-1 -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
file = {:Users/ekatsevi/papers/Econometrics Journal/Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and structural parameters.pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Liu2020,
author = {Liu, Molei and Katsevich, Eugene and Ramdas, Aaditya and Janson, Lucas},
file = {:Users/ekatsevi/papers/Biometrika/Liu et al. - 2021 - Fast and Powerful Conditional Randomization Testing via Distillation(2).pdf:pdf},
journal = {Biometrika},
keywords = {conditional independence testing,conditional randomization test,crt,high-dimensional inference,high-dimensional regression,machine learning,model-X,model-x},
mendeley-tags = {conditional randomization test,high-dimensional regression,model-X},
title = {{Fast and Powerful Conditional Randomization Testing via Distillation}},
url = {https://arxiv.org/abs/2006.03980},
year = {2021}
}
@article{Wu2020a,
abstract = {The Fisher randomization test (FRT) is appropriate for any test statistic, under a sharp null hypothesis that can recover all missing potential outcomes. However, it is often sought after to test a weak null hypothesis that the treatment does not affect the units on average. To use the FRT for a weak null hypothesis, we must address two issues. First, we need to impute the missing potential outcomes although the weak null hypothesis cannot determine all of them. Second, we need to choose a proper test statistic. For a general weak null hypothesis, we propose an approach to imputing missing potential outcomes under a compatible sharp null hypothesis. Building on this imputation scheme, we advocate a studentized statistic. The resulting FRT has multiple desirable features. First, it is model-free. Second, it is finite-sample exact under the sharp null hypothesis that we use to impute the potential outcomes. Third, it conservatively controls large-sample Type I error under the weak null hypothesis of interest. Therefore, our FRT is agnostic to the treatment effect heterogeneity. We establish a unified theory for general factorial experiments and extend it to stratified and clustered experiments. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1809.07419},
author = {Wu, Jason and Ding, Peng},
doi = {10.1080/01621459.2020.1750415},
eprint = {1809.07419},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Wu, Ding - 2020 - Randomization Tests for Weak Null Hypotheses in Randomized Experiments.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Causal inference,Finite population asymptotics,Potential outcomes,Randomization-based inference,Sharp null hypothesis,Studentization},
title = {{Randomization Tests for Weak Null Hypotheses in Randomized Experiments}},
year = {2020}
}
@article{Fan2020,
abstract = {Power and reproducibility are key to enabling refined scientific discoveries in contemporary big data applications with general high-dimensional nonlinear models. In this article, we provide theoretical foundations on the power and robustness for the model-X knockoffs procedure introduced recently in Cand{\`{e}}s, Fan, Janson and Lv in high-dimensional setting when the covariate distribution is characterized by Gaussian graphical model. We establish that under mild regularity conditions, the power of the oracle knockoffs procedure with known covariate distribution in high-dimensional linear models is asymptotically one as sample size goes to infinity. When moving away from the ideal case, we suggest the modified model-X knockoffs method called graphical nonlinear knockoffs (RANK) to accommodate the unknown covariate distribution. We provide theoretical justifications on the robustness of our modified procedure by showing that the false discovery rate (FDR) is asymptotically controlled at the target level and the power is asymptotically one with the estimated covariate distribution. To the best of our knowledge, this is the first formal theoretical result on the power for the knockoffs procedure. Simulation results demonstrate that compared to existing approaches, our method performs competitively in both FDR control and power. A real dataset is analyzed to further assess the performance of the suggested knockoffs procedure. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1709.00092},
author = {Fan, Yingying and Demirkaya, Emre and Li, Gaorong and Lv, Jinchi},
doi = {10.1080/01621459.2018.1546589},
eprint = {1709.00092},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Fan et al. - 2020 - RANK Large-Scale Inference With Graphical Nonlinear Knockoffs.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Big data,Graphical nonlinear knockoffs,High-dimensional nonlinear models,Large-scale inference and FDR,Power,Reproducibility,Robustness},
month = {jan},
number = {529},
pages = {362--379},
publisher = {American Statistical Association},
title = {{RANK: Large-Scale Inference With Graphical Nonlinear Knockoffs}},
volume = {115},
year = {2020}
}
@article{FoygelBarber2019,
abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jack-knife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to K-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk [2015] and we discuss connections.},
author = {{Foygel Barber}, Rina and Cand{\`{e}}s, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
file = {:Users/ekatsevi/papers/Annals of Statistics, to appear/Foygel Barber et al. - 2020 - Predictive inference with the jackknife.pdf:pdf},
journal = {Annals of Statistics, to appear},
keywords = {conformal prediction,to-skim,unpublished},
mendeley-tags = {conformal prediction,to-skim,unpublished},
title = {{Predictive inference with the jackknife+}},
year = {2020}
}
@article{Robins2001,
author = {Robins, James M. and Rotnitzky, Andrea},
file = {:Users/ekatsevi/papers/Statistica Sinica/Robins, Rotnitzky - 2001 - Comment on the Bickel and Kwon article, Inference for semiparametric models Some questions and an answer.pdf:pdf},
journal = {Statistica Sinica},
keywords = {causality},
mendeley-tags = {causality},
number = {4},
pages = {920--936},
title = {{Comment on the Bickel and Kwon article, "Inference for semiparametric models: Some questions and an answer"}},
volume = {11},
year = {2001}
}
@inproceedings{Liu2019,
abstract = {The knockoff filter introduced by Barber and Cand$\backslash$`es 2016 is an elegant framework for controlling the false discovery rate in variable selection. While empirical results indicate that this methodology is not too conservative, there is no conclusive theoretical result on its power. When the predictors are i.i.d. Gaussian, it is known that as the signal to noise ratio tend to infinity, the knockoff filter is consistent in the sense that one can make FDR go to 0 and power go to 1 simultaneously. In this work we study the case where the predictors have a general covariance matrix {\$}\backslashSigma{\$}. We introduce a simple functional called effective signal deficiency (ESD) of the covariance matrix {\$}\backslashSigma{\$} that predicts consistency of various variable selection methods. In particular, ESD reveals that the structure of the precision matrix {\$}\backslashSigma{\^{}}{\{}-1{\}}{\$} plays a central role in consistency and therefore, so does the conditional independence structure of the predictors. To leverage this connection, we introduce Conditional Independence knockoff, a simple procedure that is able to compete with the more sophisticated knockoff filters and that is defined when the predictors obey a Gaussian tree graphical models (or when the graph is sufficiently sparse). Our theoretical results are supported by numerical evidence on synthetic data.},
archivePrefix = {arXiv},
arxivId = {1910.12428},
author = {Liu, Jingbo and Rigollet, Philippe},
booktitle = {33rd Conference on Neural Information Processing Systems},
eprint = {1910.12428},
file = {:Users/ekatsevi/papers/33rd Conference on Neural Information Processing Systems/Liu, Rigollet - 2019 - Power analysis of knockoff filters for correlated designs.pdf:pdf},
title = {{Power analysis of knockoff filters for correlated designs}},
url = {http://arxiv.org/abs/1910.12428},
year = {2019}
}
@article{Bates2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2002.09644v1},
author = {Bates, Stephen and Sesia, Matteo and Sabatti, Chiara and Candes, Emmanuel},
eprint = {arXiv:2002.09644v1},
file = {:Users/ekatsevi/papers/Proceedings of the National Academy of Sciences/Bates et al. - 2020 - Causal Inference in Genetic Trio Studies.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
keywords = {causal discovery,conditional independence,false discovery rate,family-based association test,fbat,fdr,genome-,gwas,tdt,transmission disequilibrium test,trio,wide association study},
number = {39},
pages = {24117--24126},
title = {{Causal Inference in Genetic Trio Studies}},
volume = {117},
year = {2020}
}
@article{LeCam1960,
author = {{Le Cam}, Lucien},
file = {:Users/ekatsevi/papers/Univ. California Publ. Stat/Le Cam - 1960 - Locally asymptotically normal families of distributions.pdf:pdf},
journal = {Univ. California Publ. Stat.},
keywords = {asymptotics,monography experiment,statistics},
mendeley-tags = {asymptotics},
pages = {37--98},
title = {{Locally asymptotically normal families of distributions}},
volume = {3},
year = {1960}
}
@article{Hennessy2016,
author = {Hennessy, Jonathan and Dasgupta, Tirthankar and Miratrix, Luke and Pattanayak, Cassandra},
doi = {10.1515/jci-2015-0018},
file = {:Users/ekatsevi/papers/Journal of Causal Inference/Hennessy et al. - 2016 - A Conditional Randomization Test to Account for Covariate Imbalance in Randomized Experiments.pdf:pdf},
journal = {Journal of Causal Inference},
keywords = {balance function,causality,conditional randomization test,potential outcomes,sharp null hypothesis},
mendeley-tags = {causality,conditional randomization test},
number = {1},
pages = {61--80},
title = {{A Conditional Randomization Test to Account for Covariate Imbalance in Randomized Experiments}},
volume = {4},
year = {2016}
}
@article{Barber2018,
abstract = {We consider the variable selection problem, which seeks to identify important variables influencing a response {\$}Y{\$} out of many candidate features {\$}X{\_}1, \backslashldots, X{\_}p{\$}. We wish to do so while offering finite-sample guarantees about the fraction of false positives - selected variables {\$}X{\_}j{\$} that in fact have no effect on {\$}Y{\$} after the other features are known. When the number of features {\$}p{\$} is large (perhaps even larger than the sample size {\$}n{\$}), and we have no prior knowledge regarding the type of dependence between {\$}Y{\$} and {\$}X{\$}, the model-X knockoffs framework nonetheless allows us to select a model with a guaranteed bound on the false discovery rate, as long as the distribution of the feature vector {\$}X=(X{\_}1,\backslashdots,X{\_}p){\$} is exactly known. This model selection procedure operates by constructing "knockoff copies'" of each of the {\$}p{\$} features, which are then used as a control group to ensure that the model selection algorithm is not choosing too many irrelevant features. In this work, we study the practical setting where the distribution of {\$}X{\$} could only be estimated, rather than known exactly, and the knockoff copies of the {\$}X{\_}j{\$}'s are therefore constructed somewhat incorrectly. Our results, which are free of any modeling assumption whatsoever, show that the resulting model selection procedure incurs an inflation of the false discovery rate that is proportional to our errors in estimating the distribution of each feature {\$}X{\_}j{\$} conditional on the remaining features {\$}\backslash{\{}X{\_}k:k\backslashneq j\backslash{\}}{\$}. The model-X knockoff framework is therefore robust to errors in the underlying assumptions on the distribution of {\$}X{\$}, making it an effective method for many practical applications, such as genome-wide association studies, where the underlying distribution on the features {\$}X{\_}1,\backslashdots,X{\_}p{\$} is estimated accurately but not known exactly.},
archivePrefix = {arXiv},
arxivId = {1801.03896},
author = {Barber, Rina Foygel and Cand{\`{e}}s, Emmanuel J. and Samworth, Richard J.},
eprint = {1801.03896},
file = {:Users/ekatsevi/papers/Annals of Statistics, to appear/Barber, Cand{\`{e}}s, Samworth - 2020 - Robust inference with knockoffs.pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics, to appear},
title = {{Robust inference with knockoffs}},
url = {http://arxiv.org/abs/1801.03896},
year = {2020}
}
@book{VDV1998,
address = {Cambridge},
author = {van der Vaart, A. W.},
file = {:Users/ekatsevi/papers/Unknown/Vaart - 1998 - Asymptotic Statistics.pdf:pdf},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
publisher = {Cambridge University Press},
title = {{Asymptotic Statistics}},
year = {1998}
}
@article{Kennedy2017,
author = {Kennedy, Edward H and Ma, Zongming and Mchugh, Matthew D and Small, Dylan S},
doi = {10.1111/rssb.12212},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society, Series B (Methodological)/Kennedy et al. - 2017 - Non-parametric methods for doubly robust estimation of continuous treatment effects.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series B (Methodological)},
keywords = {causal inference,dose,efficient influence function,kernel smoothing,response,semiparametric estimation},
pages = {1229--1245},
title = {{Non-parametric methods for doubly robust estimation of continuous treatment effects}},
volume = {4},
year = {2017}
}
@article{BC15,
author = {Barber, Rina Foygel and Cand{\`{e}}s, Emmanuel J},
file = {:Users/ekatsevi/papers/The Annals of Statistics/Barber, Cand{\`{e}}s - 2015 - Controlling the false discovery rate via knockoffs.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {FDR,Multiple testing,canonical,high-dimensional regression,knockoffs},
mendeley-tags = {FDR,Multiple testing,canonical,high-dimensional regression,knockoffs},
number = {5},
pages = {2055--2085},
publisher = {Institute of Mathematical Statistics},
title = {{Controlling the false discovery rate via knockoffs}},
volume = {43},
year = {2015}
}
@article{Duchi2018,
author = {Duchi, John},
file = {:Users/ekatsevi/papers/Technical Report, Stanford University/Duchi - 2018 - A few notes on contiguity, asymptotics, and local asymptotic normality.pdf:pdf},
journal = {Technical Report, Stanford University},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
title = {{A few notes on contiguity, asymptotics, and local asymptotic normality}},
year = {2018}
}
@article{Zheng2008,
abstract = {The purpose of this paper is to investigate and develop methods for analysis of multi-center randomized clinical trials which only rely on the randomization process as a basis of inference. Our motivation is prompted by the fact that most current statistical procedures used in the analysis of randomized multi-center studies are model based. The randomization feature of the trials is usually ignored. An important characteristic of model based analysis is that it is straightforward to model covariates. Nevertheless, in nearly all model based analyses, the effects due to different centers and, in general, the design of the clinical trials are ignored. An alternative to a model based analysis is to have analyses guided by the design of the trial. Our development of design based methods allows the incorporation of centers as well as other features of the trial design. The methods make use of conditioning on the ancillary statistics in the sample space generated by the randomization process. We have investigated the power of the methods and have found that, in the presence of center variation, there is a significant increase in power. The methods have been extended to group sequential trials with similar increases in power. {\textcopyright} Institute of Mathematical Statistics.},
author = {Zheng, Lu and Zelen, Marvin},
doi = {10.1214/07-AOAS151},
file = {:Users/ekatsevi/papers/Annals of Applied Statistics/Zheng, Zelen - 2008 - Multi-center clinical trials Randomization and ancillary statistics.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Design based analyses,Multi-center trials,Permuted blocks design,Randomization,causality,conditional permutation test},
mendeley-tags = {causality,conditional permutation test},
number = {2},
pages = {582--600},
title = {{Multi-center clinical trials: Randomization and ancillary statistics}},
volume = {2},
year = {2008}
}
@article{Tsiatis2008,
abstract = {This paper presents the synchronization between a pair of identical susceptible- infected-recovered (SIR) epidemic chaotic systems and fractional-order time derivative using active control method. The fractional derivative is described in Caputo sense. Numerical simulation results show that the method is effective and reliable for synchronizing the fractional-order chaotic systems while it allows the system to remain in chaotic state. The striking features of this paper are: the successful presentation of the stability of the equilibrium state and the revelation that time for synchronization varies with the variation in fractional-order derivatives close to the standard one for different specified values of the parameters of the system.},
author = {Tsiatis, Anastasios A. and Davidian, Marie and Zhang, Min and Lu, Xiaomin},
doi = {10.1002/sim},
file = {:Users/ekatsevi/papers/Statistics in Medicine/Tsiatis et al. - 2008 - Covariate adjustment for two-sample treatment comparisons in randomized clinical trials A principled yet flexibl.pdf:pdf},
journal = {Statistics in Medicine},
keywords = {causality,publication bias,selection bias,selection model,sensitivity analysis,unpublished studies},
mendeley-tags = {causality},
pages = {4267--4278},
title = {{Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: A principled yet flexible approach}},
volume = {27},
year = {2008}
}
@article{Katsevich2020a,
abstract = {For testing conditional independence (CI) of a response {\$}Y{\$} and a predictor {\$}X{\$} given covariates {\$}Z{\$}, the recently introduced model-X (MX) framework has been the subject of active methodological research, especially in the context of MX knockoffs and their successful application to genome-wide association studies. In this paper, we build a theoretical foundation for the MX CI problem, yielding quantitative explanations for empirically observed phenomena and novel insights to guide the design of MX methodology. We focus our analysis on the conditional randomization test (CRT), whose validity conditional on {\$}Y,Z{\$} allows us to view it as a test of a point null hypothesis involving the conditional distribution of {\$}X{\$}. We use the Neyman-Pearson lemma to derive an intuitive most-powerful CRT statistic against a point alternative as well as an analogous result for MX knockoffs. We define MX analogs of {\$}t{\$}- and {\$}F{\$}- tests and derive their power against local semiparametric alternatives using Le Cam's local asymptotic normality theory, explicitly capturing the prediction error of the underlying machine learning procedure. Importantly, all our results hold conditionally on {\$}Y,Z{\$}, almost surely in {\$}Y,Z{\$}. Finally, we define nonparametric notions of effect size and derive consistent estimators inspired by semiparametric statistics. Thus, this work forms explicit, and underexplored, bridges from MX to both classical statistics (testing) and modern causal inference (estimation).},
archivePrefix = {arXiv},
arxivId = {2005.05506},
author = {Katsevich, Eugene and Ramdas, Aaditya},
eprint = {2005.05506},
file = {:Users/ekatsevi/papers/arXiv/Katsevich, Ramdas - 2020 - A theoretical treatment of conditional independence testing under Model-X.pdf:pdf},
journal = {arXiv},
title = {{A theoretical treatment of conditional independence testing under Model-X}},
url = {https://arxiv.org/abs/2005.05506v1},
year = {2020}
}
@article{Romano2019a,
abstract = {This paper introduces a machine for sampling approximate model-X knockoffs for arbitrary and unspecified data distributions using deep generative models. The main idea is to iteratively refine a knockoff sampling mechanism until a criterion measuring the validity of the produced knockoffs is optimized; this criterion is inspired by the popular maximum mean discrepancy in machine learning and can be thought of as measuring the distance to pairwise exchangeability between original and knockoff features. By building upon the existing model-X framework, we thus obtain a flexible and model-free statistical tool to perform controlled variable selection. Extensive numerical experiments and quantitative tests confirm the generality, effectiveness, and power of our deep knockoff machines. Finally, we apply this new method to a real study of mutations linked to changes in drug resistance in the human immunodeficiency virus.},
author = {Romano, Yaniv and Sesia, Matteo and Cand{\`{e}}s, Emmanuel},
doi = {10.1080/01621459.2019.1660174},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Romano, Sesia, Cand{\`{e}}s - 2019 - Deep Knockoffs.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {FDR,Variable selection,false discovery rate,generative models,high-dimensional regression,knockoffs,neural networks,nonparametric methods,variable selection},
mendeley-tags = {FDR,high-dimensional regression,knockoffs,variable selection},
number = {0},
pages = {1--27},
publisher = {Taylor {\&} Francis},
title = {{Deep Knockoffs}},
url = {http://dx.doi.org/10.1080/01621459.2019.1660174},
volume = {0},
year = {2019}
}
@article{Philippou1973,
author = {Philippou, A. N. and Roussas, G. G},
file = {:Users/ekatsevi/papers/Annals of Statistics/Philippou, Roussas - 1973 - Asymptotic distribution of the likelihood function in the independent not identically distributed case.pdf:pdf},
journal = {Annals of Statistics},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
number = {3},
pages = {454--471},
title = {{Asymptotic distribution of the likelihood function in the independent not identically distributed case}},
volume = {1},
year = {1973}
}
@article{Wang2020b,
abstract = {In many scientific problems, researchers try to relate a response variable {\$}Y{\$} to a set of potential explanatory variables {\$}X = (X{\_}1,\backslashdots,X{\_}p){\$}, and start by trying to identify variables that contribute to this relationship. In statistical terms, this goal can be posed as trying to identify {\$}X{\_}j{\$}'s upon which {\$}Y{\$} is conditionally dependent. Sometimes it is of value to simultaneously test for each {\$}j{\$}, which is more commonly known as variable selection. The conditional randomization test (CRT) and model-X knockoffs are two recently proposed methods that respectively perform conditional independence testing and variable selection by, for each {\$}X{\_}j{\$}, computing any test statistic on the data and assessing that test statistic's significance by comparing it to test statistics computed on synthetic variables generated using knowledge of {\$}X{\$}'s distribution. Our main contribution is to analyze their power in a high-dimensional linear model where the ratio of the dimension {\$}p{\$} and the sample size {\$}n{\$} converge to a positive constant. We give explicit expressions of the asymptotic power of the CRT, variable selection with CRT {\$}p{\$}-values, and model-X knockoffs, each with a test statistic based on either the marginal covariance, the least squares coefficient, or the lasso. One useful application of our analysis is the direct theoretical comparison of the asymptotic powers of variable selection with CRT {\$}p{\$}-values and model-X knockoffs; in the instances with independent covariates that we consider, the CRT provably dominates knockoffs. We also analyze the power gain from using unlabeled data in the CRT when limited knowledge of {\$}X{\$}'s distribution is available, and the power of the CRT when samples are collected retrospectively.},
author = {Wang, Wenshuo and Janson, Lucas},
file = {:Users/ekatsevi/papers/Biometrika, to appear/Wang, Janson - 2022 - A Power Analysis of the Conditional Randomization Test and Knockoffs.pdf:pdf},
journal = {Biometrika, to appear},
keywords = {approximate message passing,benjamini,conditional randomization test,conditional randomization testing,hochberg,knockoffs,model-X,model-x,power analysis,retrospective,sampling},
mendeley-tags = {conditional randomization test,knockoffs,model-X,power analysis},
title = {{A Power Analysis of the Conditional Randomization Test and Knockoffs}},
url = {http://arxiv.org/abs/2010.02304},
year = {2022}
}
@article{Hirano2004,
author = {Hirano, Keisuke and Imbens, Guido W.},
doi = {10.1002/0470090456.ch7},
file = {:Users/ekatsevi/papers/Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives/Hirano, Imbens - 2004 - The Propensity Score with Continuous Treatments.pdf:pdf},
isbn = {9780470090459},
journal = {Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives},
keywords = {Asymptotic standard errors,Binary treatment case,Conventional regression estimates,Dose-response function,Generalized propensity score (GPS),Marginal structural model (MSM),Propensity score methodology,Weak unconfoundedness,causality},
mendeley-tags = {causality},
pages = {73--84},
title = {{The Propensity Score with Continuous Treatments}},
year = {2004}
}
@article{SetS19,
annote = {$\backslash$href{\{}https://www.biorxiv.org/content/10.1101/631390v2{\}}{\{}https://www.biorxiv.org/content/10.1101/631390v2{\}}},
author = {Sesia, Matteo and Katsevich, Eugene and Bates, Stephen and Cand{\`{e}}s, Emmanuel and Sabatti, Chiara},
file = {:Users/ekatsevi/papers/Nature Communications/Sesia et al. - 2020 - Multi-resolution localization of causal variants across the genome.pdf:pdf},
journal = {Nature Communications},
keywords = {GWAS,groups,knockoffs,model-X,unpublished},
mendeley-tags = {GWAS,groups,knockoffs,model-X,unpublished},
pages = {1093},
title = {{Multi-resolution localization of causal variants across the genome}},
volume = {11},
year = {2020}
}
@article{SetC17,
abstract = {In this paper we deepen and enlarge the reflection on the possible advantages of a knockoff approach to genome wide association studies (Sesia et al., 2018), starting from the discussions in Bottolo {\&} Richardson (2019); Jewell {\&} Witten (2019); Rosenblatt et al. (2019) and Marchini (2019). The discussants bring up a number of important points, either related to the knockoffs methodology in general, or to its specific application to genetic studies. In the following we offer some clarifications, mention relevant recent developments and highlight some of the still open problems.},
annote = {Forthcoming, preprint arXiv:1706.04677},
author = {Sesia, M. and Sabatti, C. and Cand{\`{e}}s, E. J.},
doi = {10.1093/biomet/asy033},
file = {:Users/ekatsevi/papers/Biometrika/Sesia, Sabatti, Cand{\`{e}}s - 2019 - Gene hunting with hidden Markov model knockoffs.pdf:pdf},
issn = {14643510},
journal = {Biometrika},
keywords = {FDR,False discovery rate,Genome-wide association study,Knockoff,Multiple testing,Variable selection,genetics,high-dimensional regression,knockoffs,model-X},
mendeley-tags = {FDR,Multiple testing,genetics,high-dimensional regression,knockoffs,model-X},
number = {1},
pages = {1--18},
title = {{Gene hunting with hidden Markov model knockoffs}},
volume = {106},
year = {2019}
}
@article{Dukes2020,
abstract = {After variable selection, standard inferential procedures for regression parameters may not be uniformly valid; there is no finite sample size at which a standard test is guaranteed to attain its nominal size (within pre-specified error margins). This problem is exacerbated in high-dimensional settings, where variable selection becomes unavoidable. This has prompted a flurry of activity in developing uniformly valid hypothesis tests for a low-dimensional regression parameter (e.g. the causal effect of an exposure A on an outcome Y) in high-dimensional models. So far there has been limited focus on model misspecification, although this is inevitable in high-dimensional settings. We propose tests of the null that are uniformly valid under sparsity conditions weaker than those typically invoked in the literature, assuming working models for the exposure and outcome are both correctly specified. When one of the models is misspecified, by amending the procedure for estimating the nuisance parameters, our tests continue to be valid; hence they are then doubly robust. Our proposals are straightforward to implement using existing software for penalized maximum likelihood estimation and do not require sample-splitting. We illustrate them in simulations and an analysis of data obtained from the Ghent University Intensive Care Unit.},
author = {Dukes, Oliver and Avagyan, Vahe and Vansteelandt, Stijn},
file = {:Users/ekatsevi/papers/Biometrics/Dukes, Avagyan, Vansteelandt - 2020 - High-dimensional doubly robust tests for regression parameters.pdf:pdf},
journal = {Biometrics},
keywords = {double robustness,generalized linear models},
mendeley-tags = {double robustness,generalized linear models},
pages = {1--53},
title = {{High-dimensional doubly robust tests for regression parameters}},
url = {http://arxiv.org/abs/1805.06714},
year = {2020}
}
@article{Imai2004,
abstract = {In this article we develop the theoretical properties of the propensity function, which is a generalization of the propensity score of Rosenbaum and Rubin. Methods based on the propensity score have long been used for causal inference in observational studies; they are easy to use and can effectively reduce the bias caused by nonrandom treatment assignment. Although treatment regimes need not be binary in practice, the propensity score methods are generally confined to binary treatment scenarios. Two possible exceptions have been suggested for ordinal and categorical treatments. In this article we develop theory and methods that encompass all of these techniques and widen their applicability by allowing for arbitrary treatment regimes. We illustrate our propensity function methods by applying them to two datasets; we estimate the effect of smoking on medical expenditure and the effect of schooling on wages. We also conduct simulation studies to investigate the performance of our methods.},
author = {Imai, Kosuke and {Van Dyk}, David A.},
doi = {10.1198/016214504000001187},
file = {:Users/ekatsevi/papers/Journal of the American Statistical Association/Imai, Van Dyk - 2004 - Causal inference with general treatment regimes Generalizing the propensity score.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Medical expenditure,Nonrandom treatment assignment,Observational studies,Return to schooling,Subclassification,Treatment effect,causality},
mendeley-tags = {causality},
number = {467},
pages = {854--866},
title = {{Causal inference with general treatment regimes: Generalizing the propensity score}},
volume = {99},
year = {2004}
}
@article{Kasy2019,
abstract = {When are asymptotic approximations using the delta-method uniformly valid? We provide sufficient conditions as well as closely related necessary conditions for uniform negligibility of the remainder of such approximations. These conditions are easily verified by empirical practitioners and permit to identify settings and parameter regions where pointwise asymptotic approximations perform poorly. Our framework allows for a unified and transparent discussion of uniformity issues in various sub-fields of statistics and econometrics. Our conditions involve uniform bounds on the remainder of a first-order approximation for the function of interest.},
archivePrefix = {arXiv},
arxivId = {1507.05731},
author = {Kasy, Maximilian},
doi = {10.1515/jem-2018-0001},
eprint = {1507.05731},
file = {:Users/ekatsevi/papers/Journal of Econometric Methods/Kasy - 2019 - Uniformity and the Delta Method.pdf:pdf},
issn = {21566674},
journal = {Journal of Econometric Methods},
keywords = {asymptotic theory,delta method,uniformity},
title = {{Uniformity and the Delta Method}},
year = {2019}
}
@article{Berrett2019,
abstract = {We propose a general new method, the conditional permutation test, for testing the conditional independence of variables X and Y given a potentially high-dimensional random vector Z that may contain confounding factors. The proposed test permutes entries of X non-uniformly, so as to respect the existing dependence between X and Z and thus account for the presence of these con-founders. Like the conditional randomization test of Cand{\`{e}}s et al. [7], our test relies on the availability of an approximation to the distribution of X|Z-while Cand{\`{e}}s et al. [7]'s test uses this estimate to draw new X values, for our test we use this approximation to design an appropriate non-uniform distribution on permutations of the X values already seen in the true data. We provide an efficient Markov Chain Monte Carlo sampler for the implementation of our method, and establish bounds on the Type I error in terms of the error in the approximation of the conditional distribution of X|Z, finding that, for the worst case test statistic, the inflation in Type I error of the conditional permutation test is no larger than that of the conditional randomization test. We validate these theoretical results with experiments on simulated data and on the Capital Bikeshare data set.},
author = {Berrett, Thomas B and Wang, Yi and {Foygel Barber}, Rina and Samworth, Richard J},
file = {:Users/ekatsevi/papers/Journal of the Royal Statistical Society. Series B Statistical Methodology/Berrett et al. - 2020 - The conditional permutation test for independence while controlling for confounders.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {conditional permutation test,high-dimensional regression,multiple testing,resampling,unpublished},
mendeley-tags = {conditional permutation test,high-dimensional regression,multiple testing,resampling,unpublished},
number = {1},
pages = {175--197},
title = {{The conditional permutation test for independence while controlling for confounders}},
volume = {82},
year = {2020}
}
@book{Robins2020,
abstract = {Causal Inference is an admittedly pretentious title for a book. Causal inference is a complex scientific task that relies on triangulating evidence from multiple sources and on the application of a variety of methodological approaches. No book can possibly provide a comprehensive description of methodologies for causal inference across the sciences. The authors of any Causal Inference book will have to choose which aspects of causal inference methodology they want to emphasize. The title of this introduction reflects our own choices: a book that helps scientists–especially health and social scientists–generate and analyze data to make causal inferences that are explicit about both the causal question and the assumptions underlying the data analysis. Unfortunately, the scientific literature is plagued by studies in which the causal question is not explicitly stated and the investigators' unverifiable assumptions are not declared. This casual attitude towards causal inference has led to a great deal of confusion. For example, it is not uncommon to find studies in which the effect estimates are hard to interpret because the data analysis methods cannot appropriately answer the causal question (were it explicitly stated) under the investigators' assumptions (were they declared).},
address = {Boca Raton},
author = {Robins, James M. and Hern{\'{a}}n, Miguel A.},
file = {:Users/ekatsevi/papers/Unknown/Robins, Hern{\'{a}}n - 2020 - Causal Inference What If.pdf:pdf},
keywords = {causality},
mendeley-tags = {causality},
publisher = {Chapman {\&} Hall/CRC},
title = {{Causal Inference: What If}},
year = {2020}
}
@article{Hoadley1971,
author = {Hoadley, Bruce},
file = {:Users/ekatsevi/papers/The Annals of Mathematical Statistics/Hoadley - 1971 - Asymptotic Properties of Maximum Likelihood Estimators for the Independent Not Identically Distributed Case.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
number = {6},
pages = {1977--1991},
title = {{Asymptotic Properties of Maximum Likelihood Estimators for the Independent Not Identically Distributed Case}},
volume = {42},
year = {1971}
}
@article{Berk2019a,
abstract = {It is well known that with observational data, models used in conventional regression analyses are commonly misspecified. Yet in practice, one tends to proceed with interpretations and inferences that rely on correct specification. Even those who invoke Box's maxim that all models are wrong proceed as if results were generally useful. Misspecification, however, has implications that affect practice. Regression models are approximations to a true response surface and should be treated as such. Accordingly, regression parameters should be interpreted as statistical functionals. Importantly, the regressor distribution affects targets of estimation and regressor randomness affects the sampling variability of estimates. As a consequence, inference should be based on sandwich estimators or the pairs (x–y) bootstrap. Traditional prediction intervals lose their pointwise coverage guarantees, but empirically calibrated intervals can be justified for future populations. We illustrate the key concepts with an empirical application.},
archivePrefix = {arXiv},
arxivId = {1806.09014},
author = {Berk, Richard and Buja, Andreas and Brown, Lawrence and George, Edward and Kuchibhotla, Arun Kumar and Su, Weijie and Zhao, Linda},
doi = {10.1080/00031305.2019.1592781},
eprint = {1806.09014},
file = {:Users/ekatsevi/papers/American Statistician/Berk et al. - 2019 - Assumption Lean Regression.pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Foundational issues,Generalized linear models,Linear regression,Misspecified regression models,Regression functionals},
title = {{Assumption Lean Regression}},
year = {2019}
}
@article{Ding2017,
abstract = {Under the potential outcomes framework, causal effects are defined as comparisons between potential outcomes under treatment and control. To infer causal effects from randomized experiments, Neyman proposed to test the null hypothesis of zero average causal effect (Neyman's null), and Fisher proposed to test the null hypothesis of zero individual causal effect (Fisher's null). Although the subtle difference between Neyman's null and Fisher's null has caused a lot of controversies and confusions for both theoretical and practical statisticians, a careful comparison between the two approaches has been lacking in the literature for more than eighty years. We fill this historical gap by making a theoretical comparison between them and highlighting an intriguing paradox that has not been recognized by previous researchers. Logically, Fisher's null implies Neyman's null. It is therefore surprising that, in actual completely randomized experiments, rejection of Neyman's null does not imply rejection of Fisher's null for many realistic situations, including the case with constant causal effect. Furthermore, we show that this paradox also exists in other commonly-used experiments, such as stratified experiments, matched-pair experiments and factorial experiments. Asymptotic analyses, numerical examples and real data examples all support this surprising phenomenon. Besides its historical and theoretical importance, this paradox also leads to useful practical implications for modern researchers.},
archivePrefix = {arXiv},
arxivId = {1402.0142},
author = {Ding, Peng},
doi = {10.1214/16-STS571},
eprint = {1402.0142},
file = {:Users/ekatsevi/papers/Statistical Science/Ding - 2017 - A paradox from randomization-based causal inference.pdf:pdf;:Users/ekatsevi/papers/Statistical Science/Ding - 2017 - A paradox from randomization-based causal inference(2).pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Average null hypothesis,Fisher randomization test,Potential outcome,Randomized experiment,Repeated sampling property,Sharp null hypothesis,causality,conditional randomization test,permutation test},
mendeley-tags = {causality,conditional randomization test,permutation test},
number = {3},
pages = {331--345},
title = {{A paradox from randomization-based causal inference}},
volume = {32},
year = {2017}
}
@book{FellerV1,
address = {New York},
author = {Feller, William},
edition = {Third},
file = {:Users/ekatsevi/papers/Unknown/Feller - 1968 - An Introduction to Probability Theory and Its Applications.pdf:pdf},
publisher = {John Wiley {\&} Sons Inc.},
title = {{An Introduction to Probability Theory and Its Applications}},
year = {1968}
}
@article{Weinstein2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2007.15346v1},
author = {Weinstein, Asaf and Su, Weijie J and Bogdan, Malgorzata and Barber, Rina F and Candes, Emmanuel J},
eprint = {arXiv:2007.15346v1},
file = {:Users/ekatsevi/papers/arXiv/Weinstein et al. - 2020 - A Power Analysis for Knockoffs with the Lasso.pdf:pdf},
journal = {arXiv},
keywords = {AMP,high-dimensional regression,knockoffs},
mendeley-tags = {AMP,high-dimensional regression,knockoffs},
title = {{A Power Analysis for Knockoffs with the Lasso}},
year = {2020}
}
@book{Weak_Convergence_Book,
abstract = {This book explores weak convergence theory and empirical processes and their applications to many applications in statistics. Part one reviews stochastic convergence in its various forms. Part two offers the theory of empirical processes in a form accessible to statisticians and probabilists. Part three covers a range of topics demonstrating the applicability of the theory to key questions such as measures of goodness of fit and the bootstrap.},
address = {New York},
author = {{Van der Vaart}, Aad W. and Wellner, Jon A.},
doi = {10.2307/2965734},
file = {:Users/ekatsevi/papers/Unknown/Van der Vaart, Wellner - 1996 - Weak Convergence and Empirical Processes.pdf:pdf},
isbn = {9781475725476},
issn = {01621459},
publisher = {Springer-Verlag},
title = {{Weak Convergence and Empirical Processes}},
year = {1996}
}
@article{Huang2019,
abstract = {The recent paper Cand$\backslash$`es et al. (2018) introduced model-X knockoffs, a method for variable selection that provably and non-asymptotically controls the false discovery rate with no restrictions or assumptions on the dimensionality of the data or the conditional distribution of the response given the covariates. The one requirement for the procedure is that the covariate samples are drawn independently and identically from a precisely-known (but arbitrary) distribution. The present paper shows that the exact same guarantees can be made without knowing the covariate distribution fully, but instead knowing it only up to a parametric model with as many as {\$}\backslashOmega(n{\^{}}{\{}*{\}}p){\$} parameters, where {\$}p{\$} is the dimension and {\$}n{\^{}}{\{}*{\}}{\$} is the number of covariate samples (which may exceed the usual sample size {\$}n{\$} of labeled samples when unlabeled samples are also available). The key is to treat the covariates as if they are drawn conditionally on their observed value for a sufficient statistic of the model. Although this idea is simple, even in Gaussian models conditioning on a sufficient statistic leads to a distribution supported on a set of zero Lebesgue measure, requiring techniques from topological measure theory to establish valid algorithms. We demonstrate how to do this for three models of interest, with simulations showing the new approach remains powerful under the weaker assumptions.},
archivePrefix = {arXiv},
arxivId = {1903.02806},
author = {Huang, Dongming and Janson, Lucas},
eprint = {1903.02806},
file = {:Users/ekatsevi/papers/Annals of Statistics, to appear/Huang, Janson - 2020 - Relaxing the Assumptions of Knockoffs by Conditioning.pdf:pdf},
journal = {Annals of Statistics, to appear},
keywords = {ery rate,false discov-,fdr,graphical model,high-dimensional inference,knockoffs,model-x,sufficient statistic,topological measure},
mendeley-tags = {knockoffs},
title = {{Relaxing the Assumptions of Knockoffs by Conditioning}},
url = {http://arxiv.org/abs/1903.02806},
year = {2020}
}
