\documentclass[12pt]{article}
 
 
 \usepackage[vmargin=1.18in,hmargin=1.1in]{geometry}
 \usepackage{amssymb,amsfonts,graphicx,amsthm,nicefrac,mathtools,bm, bbm}
 \usepackage[hidelinks]{hyperref}
 \usepackage[normalem]{ulem} %to strike the words
 \usepackage{mathrsfs}
 
% \usepackage[numbers]{natbib}
 % \usepackage[english]{babel}
% \usepackage{times}
%  \usepackage[OT1]{fontenc}  
% \usepackage{bbm,mdframed}
 \usepackage[dvipsnames]{xcolor}
% \usepackage{xspace}
 % \usepackage{rotating,multirow}
 \usepackage{graphics,tikz}

\usepackage[]{caption}

\usepackage[linesnumbered,ruled]{algorithm2e}

\usepackage[uniquename=false, uniquelist = false, url = false, doi = false, isbn = false, sorting = none, natbib = true, backend = bibtex]{biblatex}

\addbibresource{~/.references/crtpower}

% \usepackage{grffile}
 % \usepackage{subcaption}
 % \usepackage{epstopdf} % for postscript graphics files 
 
 
% \arxiv{arXiv:1803.06790}
 
 % \startlocaldefs
 
 
 
 %\setlength{\parindent}{10pt}
 %\setlength{\parskip}{5pt}
 
 
 %----------------------------------------------------------
 % Theorem etc macros
 %----------------------------------------------------------
 \newtheorem{theorem}{Theorem}
 \newtheorem{proposition}{Proposition}
 \newtheorem{claim}{Claim}
 \newtheorem{lemma}{Lemma}
 \newtheorem{corollary}{Corollary}
 \newtheorem{question}{Question}
 \newtheorem{assumption}{Assumption}
 \theoremstyle{definition}
 \newtheorem{definition}{Definition}
   \newtheorem{setting}{Setting}
 \theoremstyle{remark}
 \newtheorem{remark}{Remark}
 \newtheorem{example}{Example}
 % \makeatletter
 
 % \newtheorem*{rep@theorem}{\rep@title}
 % \newcommand{\newreptheorem}[2]
 % {\newenvironment{rep#1}[1]
 % 	{\def\rep@title{#2 \ref{##1}} \begin{rep@theorem}}%
 % 		{\end{rep@theorem}}}
 % \makeatother
 % \newreptheorem{theorem}{Theorem}
 % \newreptheorem{lemma}{Lemma}
 % \newreptheorem{corollary}{Corollary}
 % \newreptheorem{proposition}{Proposition}
 
 
 
 %----------------------------------------------------------
 % Reference macros
 %----------------------------------------------------------
 \newcommand{\rmkref}[1]{Remark~\ref{rmk:#1}}
 \newcommand{\assumpref}[1]{Assumption~\ref{assump:#1}}
 \newcommand{\assumpsref}[1]{Assumptions~\ref{assump:#1}}
 \newcommand{\assumpssref}[1]{\ref{assump:#1}}
 \newcommand{\figref}[1]{Figure~\ref{fig:#1}}
 \newcommand{\figsref}[1]{Figures~\ref{fig:#1}}
 \newcommand{\figssref}[1]{\ref{fig:#1}}
 \newcommand{\chapref}[1]{Chapter~\ref{chap:#1}}
 \newcommand{\secref}[1]{Section~\ref{sec:#1}}
 \newcommand{\appref}[1]{Appendix~\ref{app:#1}}
 \newcommand{\eqssref}[1]{\ref{eqn:#1}}
 \newcommand{\defref}[1]{Definition~\ref{def:#1}}
 \newcommand{\exref}[1]{Example~\ref{ex:#1}}
 \newcommand{\clmref}[1]{Claim~\ref{clm:#1}}
 \newcommand{\lemref}[1]{Lemma~\ref{lem:#1}}
 \newcommand{\lemsref}[1]{Lemmas~\ref{lem:#1}}
 \newcommand{\lemssref}[1]{\ref{lem:#1}}
 \newcommand{\propref}[1]{Proposition~\ref{prop:#1}}
 \newcommand{\propsref}[1]{Propositions~\ref{prop:#1}}
 \newcommand{\propssref}[1]{\ref{prop:#1}}
 \newcommand{\thmref}[1]{Theorem~\ref{thm:#1}}
 \newcommand{\thmwref}[1]{Theorem~#1}
 \newcommand{\thmsref}[1]{Theorems~\ref{thm:#1}}
 \newcommand{\thmssref}[1]{\ref{thm:#1}}
% \newcommand{\corref}[1]{Corollary~\ref{cor:#1}}
 \newcommand{\tabref}[1]{Table~\ref{tab:#1}}
 \newcommand{\tabsref}[1]{Tables~\ref{tab:#1}}
 \newcommand{\tabssref}[1]{\ref{tab:#1}}
 \newcommand{\pwref}[1]{p.~{#1}}
 \newcommand{\ppwref}[1]{pp.~{#1}}
 \newcommand{\eqnref}[1]{\eqref{eqn:#1}}
 
 \setlength{\headheight}{35pt} 
 
 
 %----------------------------------------------------------
 % Math operator macros
 %----------------------------------------------------------
 % \DeclareMathOperator{\rank}{rank}
 % \DeclareMathOperator{\diag}{diag}
 % \DeclareMathOperator{\trace}{trace}
 % \DeclareMathOperator{\cov}{Cov}
 % \DeclareMathOperator{\var}{Var}
 % \DeclareMathOperator{\sign}{sign}
 % \DeclareMathOperator{\spn}{span}
 % \DeclareMathOperator{\supp}{support}
 % \DeclareMathOperator{\conv}{conv} % Convex hull
 % \DeclareMathOperator{\cone}{cone} % Conic hull
 % \DeclareMathOperator*{\argmax}{arg\,max}
 % \DeclareMathOperator*{\argmin}{arg\,min}
 % \providecommand{\myfloor}[1]{\left \lfloor #1 \right \rfloor }
 % \providecommand{\myceil}[1]{\left \lceil #1 \right \rceil }
 
 
 %----------------------------------------------------------
 % Other macros
 %----------------------------------------------------------
 \newcommand{\iid}[0]{i.i.d.\xspace}
 \newcommand{\dist}[2]{\mathrm{dist}\!\left({#1},{#2}\right)} % distance
 \newcommand{\one}[1]{{\mathbbm{1}}_{{#1}}}
 \newcommand{\inner}[2]{\langle{#1},{#2}\rangle} % Inner product
 \newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
 %\newcommand{\PP}[1]{\mathbb{P}\left\{{#1}\right\}} % Probability
 \newcommand{\PP}[1]{\textnormal{Pr}\!\left\{{#1}\right\}} % Probability
 %\newcommand{\Pp}[2]{\mathbb{P}_{#1}\left\{{#2}\right\}} % Probability
 \newcommand{\EE}[1]{\mathbb{E}\left[{#1}\right]} % Expectation
 \newcommand{\Ep}[2]{\mathbb{E}_{#1}\left[{#2}\right]}
 \newcommand{\EEst}[2]{\mathbb{E}\left[{#1}\ \middle| \ {#2}\right]} % Conditional expectation
 %\newcommand{\PPst}[2]{\mathbb{P}\left\{{#1}\ \middle| \ {#2}\right\}} % Conditional probability
 \newcommand{\PPst}[2]{\text{Pr}\!\left\{{#1}\ \middle| \ {#2}\right\}} % Conditional probability
 % \newcommand{\TV}[1]{\var\left({#1}\right)} % Variance
 % \newcommand{\Tp}[2]{\mathbb{V}_{#1}\left[{#2}\right]}
 % \newcommand{\TVst}[2]{\var\left({#1}\ \middle|\ {#2}\right)}
 % \renewcommand{\O}[1]{\mathcal{O}\left({#1}\right)}
 % \def\R{\mathbb{R}}
 % \def\Z{\mathbb{Z}}
 \newcommand{\ee}[1]{\mathbf{e}_{{#1}}}
 \newcommand{\ident}{\mathbf{I}}
 \newcommand{\ones}{\mathbf{1}}
 \newcommand{\zeros}{\mathbf{0}}
 \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
 \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
 \newcommand{\iidsim}{\stackrel{\mathrm{iid}}{\sim}}
 \newcommand{\indsim}{\stackrel{\independent}{\sim}}
 \newcommand{\notate}[1]{\textcolor{red}{\textbf{[#1]}}}
 \newcommand{\ignore}[1]{}
 \renewcommand{\Pr}[2]{\mathcal{P}_{{#1}}\left({#2}\right)} % projection
 \newcommand{\Prp}[2]{\mathcal{P}_{{#1}}^{\perp}\left({#2}\right)}
 \newcommand{\pr}[1]{\mathcal{P}_{{#1}}}
 \newcommand{\prp}[1]{\mathcal{P}_{{#1}}^{\perp}}
 \newcommand{\eps}{\epsilon}
 \newcommand{\lam}{\lambda}
 % \newcommand{\T}{\mathcal{T}}
 % \newcommand{\diff}[1]{\text{d}{#1}} % differential
 \let\emptyset\varnothing
 % \newcommand{\tf}{\widetilde{f}}
 % \newcommand{\tbeta}{\widetilde\beta}
 % \newcommand{\bhy}{\textnormal{BHY}}
 
 \newcommand{\prx}{\bm X} 
 \newcommand{\prxu}{\underline{\bm X}} 
 \newcommand{\srx}{X}
 \newcommand{\srxu}{\underline X}
 \newcommand{\sfx}{x}
 \newcommand{\pfx}{\bm x}
 
\newcommand{\prz}{\bm Z} 
 \newcommand{\przu}{\underline{\bm Z}} 
\newcommand{\srz}{Z}
\newcommand{\srzu}{\underline Z}
\newcommand{\sfz}{z}
\newcommand{\pfz}{\bm z}
 
 \newcommand{\prxk}{{{\widetilde{\bm X}}}}
 \newcommand{\srxk}{\widetilde X}
 \newcommand{\sfxk}{\widetilde x}
 
 \newcommand{\pry}{{\bm Y}}
 \newcommand{\pfy}{{\bm y}}
 
  \newcommand{\pryu}{\underline{\bm Y}} 
 \newcommand{\sry}{Y}
 \newcommand{\sryu}{\underline Y}
 \newcommand{\sfy}{y}
 
\newcommand{\seps}{\epsilon}
\newcommand{\peps}{\bm \epsilon}
 \newcommand{\sepsu}{\underline \epsilon}
 \newcommand{\pepsu}{\underline{\bm \epsilon}}

\newcommand{\smu}{\mu}
\newcommand{\pmu}{\bm \mu}
\newcommand{\smuu}{\underline \mu}
\newcommand{\pmuu}{\underline{\bm \mu}}

\newcommand{\sSigma}{\Sigma}
\newcommand{\pSigma}{\bm \Sigma}
\newcommand{\sSigmau}{\underline \Sigma}
\newcommand{\pSigmau}{\underline{\bm \Sigma}}

 
 \newcommand{\D}{\mathcal D}
 
  \def\CRT{\textnormal{CRT}}
 
 
 
 % \newcommand{\thetitle}{Towards ``simultaneous selective inference'':\\
 	% post hoc bounds on the false discovery proportion} % on directed acyclic graphs}
 
 %\newcommand{\thetitle}{p-filter+ : a general framework for \\ improving power and precision in multiple testing using \\   weights, null-proportion adaptivity and group structures}
 
  
 % \usepackage[mmddyy]{datetime}
 % \renewcommand{\dateseparator}{.}
 
 
 
 %----------------------------------------------------------
 % Document-specific macros & settings
 %----------------------------------------------------------
 
 % \newcommand{\ov}{\textnormal{ov}}
 % \newcommand{\grp}{\textnormal{grp}}
 \newcommand{\nulls}{\mathcal{H}^0}
 % \newcommand{\nullsg}{\mathcal{H}^0_{\grp}}
 % \newcommand{\nullsm}{\mathcal{H}^0_m}
 % \newcommand{\unif}{\textnormal{Unif}[0,1]}
 \newcommand{\fdp}{\textnormal{FDP}}
 \newcommand{\fdr}{\textnormal{FDR}}
 \newcommand{\fdx}{\textnormal{FDX}}
 % \newcommand{\fdpo}{\fdp_{\ov}}
 % \newcommand{\fdpg}{\fdp_{\grp}}
 % \newcommand{\fdpm}{\fdp^m}
 \newcommand{\fdphat}{\widehat{\textnormal{FDP}}}
 \newcommand{\fdpbar}{\overline{\textnormal{FDP}}}
 \newcommand{\fdphmu}{\widehat{\fdp^m_u}}
 % \newcommand{\fdpohat}{\fdphat_{\ov}}
 % \newcommand{\fdpghat}{\fdphat_{\grp}}
 % \newcommand{\tov}{t_{\ov}}
 % \newcommand{\tg}{t_{\grp}}
 % \newcommand{\ttov}{\tilde t_{\ov}}
 % \newcommand{\ttg}{\tilde t_{\grp}}
 % \renewcommand{\th}{\widehat{t}}
 % \newcommand{\thov}{\th_{\ov}}
 % \newcommand{\thg}{\th_{\grp}}
 % \newcommand{\qov}{\alpha_{\ov}}
 % \newcommand{\qg}{\alpha_{\grp}}
 \newcommand{\bh}{\textnormal{BH}}
 % \newcommand{\ssbh}{\textnormal{ssBH}}
 % \newcommand{\ppbh}{\textnormal{ppBH}}
 % \newcommand{\Simes}{\textnormal{Simes}}
 % \newcommand{\talpha}{\widetilde{\alpha}}
 % \newcommand{\ssSimes}{\textnormal{ssSimes}}
 % \newcommand{\stbh}{\textnormal{St-BH}}
 % \newcommand{\stssbh}{\textnormal{Storey-ssBH}}
 % \newcommand{\stSimes}{\textnormal{St-Simes}}
 % \newcommand{\pf}{\textnormal{p-filter}}
 % \newcommand{\pl}{P^{( g)}}
 % \newcommand{\pli}{\pl_i}
 % \newcommand{\Sh}{\widehat{S}}
 % \newcommand{\Shm}{\Sh_m}
 % \newcommand{\Shg}{\Sh_{\grp}}
 % \newcommand{\Tset}{\widehat{\mathcal{T}}}
 % \newcommand{\One}[1]{{\bf{1}}\left\{{#1}\right\}}
 % \newcommand{\filt}{\mathcal{F}}
 % \newcommand{\kh}{\widehat{k}}
 % \newcommand{\jh}{\widehat{j}}
 \newcommand{\pih}{\widehat{\pi}}
 \newcommand{\alphah}{\widehat{\alpha}}
 \newcommand{\vh}{\widehat{V}}
 %\def\L{\mathcal{L}}
 \def\S{\mathcal{S}}
 \def\F{\mathcal{F}}
 \def\G{\mathcal{G}}
 \def\M{\mathcal{M}}
 \def\H{\mathcal{H}}
 % \def\Parents{\mathrm{Par}}
 % \def\Depth{\mathrm{Depth}}
 % \def\Subgraph{\mathrm{Sub}}
 % \def\Children{\mathrm{Child}}
 % \def\Leaves{\mathcal{L}}
 \def\cR{\mathcal{R}}
 \def\False{\mathcal{V}}
 
 
 % \makeatletter
 % \newcommand{\dotfrac}[2]{
 % 	\mathchoice
 % 	{\ooalign{$\genfrac{}{}{0pt}{0}{#1}{#2}$\cr\leavevmode\cleaders\hb@xt@ .22em{\hss $\displaystyle\cdot$\hss}\hfill\kern\z@\cr}}
 % 	{\ooalign{$\genfrac{}{}{0pt}{1}{#1}{#2}$\cr\leavevmode\cleaders\hb@xt@ .22em{\hss $\textstyle\cdot$\hss}\hfill\kern\z@\cr}}
 % 	{\ooalign{$\genfrac{}{}{0pt}{2}{#1}{#2}$\cr\leavevmode\cleaders\hb@xt@ .22em{\hss $\scriptstyle\cdot$\hss}\hfill\kern\z@\cr}}
 % 	{\ooalign{$\genfrac{}{}{0pt}{3}{#1}{#2}$\cr\leavevmode\cleaders\hb@xt@ .22em{\hss $\scriptscriptstyle\cdot$\hss}\hfill\kern\z@\cr}}
 % }
 % \makeatother
 
 % \usepackage{algorithm,algorithmic}
 %\newcommand{\theHalgorithm}{\arabic{algorithm}}
 
 % \newcommand{\red}[1]{\textcolor{red}{#1}}
 
 % \newcommand{\mjwcomment}[1]{{\bf{{\red{{MJW --- #1}}}}}}
 % \newcommand{\defn}{\ensuremath{:\, =}}
 
 % \newcommand{\myorder}[2]{\ensuremath{#1_{(#2)}}}
 
 % % Set
 % \newcommand{\Dset}{\ensuremath{\mathcal{D}}}
 
 % \newcommand{\Rplus}{\ensuremath{\R_+}}
 
 % \newcommand{\MYSUM}[2]{\ensuremath{S(#1, #2)}}
 
 % \newcommand{\MYPROB}{\ensuremath{\mbox{Pr}}}
 % \newcommand{\Calg}{\ensuremath{C_{\text{alg}}}}
 % \newcommand{\calg}{\ensuremath{c_{\text{alg}}}}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \endlocaldefs


\begin{document}


\title{On the power of conditional independence testing \\ under model-X}
\author{Eugene Katsevich$^1$ and Aaditya Ramdas$^{1,2}$\\
	[10pt]
	\texttt{ekatsevi@wharton.upenn.edu},
	\texttt{aramdas@stat.cmu.edu} \\
	Department of Statistics and Data Science, University of Pennsylvania$^1$\\
	Department of Statistics and Data Science, 	Carnegie Mellon University$^2$\\
	Machine Learning Department, Carnegie Mellon University$^2$
}


\maketitle
\thispagestyle{empty}

	
\begin{abstract}
For testing conditional independence (CI) of a response $Y$ and a predictor $X$ given covariates $Z$, the recently introduced model-X (MX) framework has been the subject of active methodological research, especially in the context of MX knockoffs and their successful application to genome-wide association studies. In this paper, we study the power of MX CI tests, yielding quantitative explanations for empirically observed phenomena and novel insights to guide the design of MX methodology. We show that any valid MX CI test must also be valid conditionally on $\sry, \srz$; this conditioning allows us to reformulate the problem as testing a point null hypothesis involving the conditional distribution of $X$. The Neyman-Pearson lemma then implies that the conditional randomization test (CRT) based on a likelihood statistic is the most powerful MX CI test against a point alternative. We also obtain a related optimality result for MX knockoffs. Switching to an asymptotic framework with arbitrarily growing covariate dimension, we derive an expression for the limiting power of the CRT against local semiparametric alternatives in terms of the prediction error of the machine learning algorithm on which its test statistic is based. Finally, we exhibit a resampling-free test with uniform asymptotic Type-I error control under the assumption that \textit{only the first two moments of $X$ given $Z$ are known}, a significant relaxation of the MX assumption. 
\end{abstract}
	
%	\begin{keyword}
%		\kwd{conditional independence testing}
%		\kwd{conditional randomization test}
%		\kwd{model-X}
%		\kwd{knockoffs}
%		\kwd{most powerful tests}
%		\kwd{local alternatives}
%		\kwd{dose response curve}
%	\end{keyword}

%\newpage 	
%\footnotesize
%\tableofcontents
%\normalsize

\section{Introduction}

\subsection{Conditional independence testing and the MX assumption}

Given a predictor $\prx \in \mathbb R^{d}$, response $\pry \in \mathbb R^{r}$, and covariate vector $\prz \in \mathbb R^{p}$ drawn from a joint distribution $(\prx, \pry, \prz) \sim \mathcal L$, consider testing the hypothesis of conditional independence (CI),
\begin{equation}
H_0: \pry \independent \prx\ |\ \prz \quad \text{versus} \quad H_1: \pry \not \independent \prx\ |\ \prz,
\label{conditional-independence}
\end{equation}
using $n$ data points 
\begin{equation}
\label{eq:xyz}
(\srx, \sry, \srz) \equiv \{(\srx_i, \sry_i, \srz_i)\}_{i = 1, \dots, n} \overset{\text{i.i.d.}}\sim \mathcal L. 
\end{equation}
This fundamental problem---determining whether a predictor is associated with a response after controlling for a set of covariates---is ubiquitous across the natural and social sciences. To keep an example in mind throughout the paper, consider $\pry \in \mathbb R^1$ cholesterol level, $\prx \in \{0,1,2\}^{10}$ the genotypes of an individual at 10 adjacent polymorphic sites, and $\prz \in \{0,1,2\}^{500,000}$ the genotypes of the individual at other polymorphic sites across the genome. Such data $(\srx, \sry, \srz)$ would be collected in a genome-wide association study (GWAS), with the goal of testing for association between the 10 polymorphic sites of interest and cholesterol while controlling for the other polymorphic sites~\eqref{conditional-independence}. CI testing is also connected to causal inference: with appropriate unconfoundedness assumptions, Fisher's sharp null hypothesis of no effect of a (potentially non-binary) treatment $\prx$ on an outcome $\pry$ implies conditional independence. While we do not work in a causal framework, we draw inspiration from connections to causal inference throughout.

%To keep an example in mind throughout the paper, consider a genome-wide association study (GWAS) based on $n$ individuals. For each individual, a disease status $\pry \in \{0,1\}$ is recorded along with the genotypes $\prx \in \{0,1,2\}$ at a single nucleotide polymorphism (SNP) of interest and $\prz \in \{0,1,2\}^p$ at all other SNPs. SNPs usually have only two possible alleles in the population (the \textit{minor allele} defined as the less common one), so the genotype at a SNP is defined as the number of minor alleles across maternal and paternal chromosomes. For example, $\prx = 1$ for an individual whose maternal and paternal chromosomes have a minor and a major allele, respectively, at the SNP of interest.

%
%consider a neuroscience experiment where we wish to determine if a particular \textit{voxel} (volume pixel) in the visual cortex responds preferentially to faces using an fMRI (functional magnetic resonance imaging) experiment involving showing a subject a sequence of $n$ images. Here, $\pry \in \mathbb R$ would be the fMRI brain recording 6 seconds after showing the image, $\prx \in \mathbb R$ would be a count of the number of faces in the image, and $\prz \in \mathbb R^{100}$ would include other visual features (counts of non-face objects, edge-detectors, color/brightness).
% \textcolor{red}{To keep an example in mind throughout the paper, consider $\pry \in \mathbb R^1$ cholesterol level, $\prx \in \{0,1,2\}^{10}$ the genotypes of an individual at 10 nearby polymorphic sites, and $\prz \in \{0,1,2\}^{500,000}$ the genotypes of the individual at other polymorphic sites across the genome.} 

As formalized by Shah and Peters \cite{Shah2018}, the problem \eqref{conditional-independence} is fundamentally impossible without assumptions on the distribution $\mathcal L(\prx,\pry, \prz)$, in which case no asymptotically uniformly valid test of this hypothesis can have nontrivial power against any alternative. In special cases, the problem is more tractable, for example if $\prz$ has discrete support, or if we were willing to make (semi)parametric assumptions on the form of $\mathcal L(\pry|\prx, \prz)$ (henceforth ``model-$Y|X$''). We will not be making such assumptions in this work.
% A complementary recent line of work has focused on shifting assumptions to the distribution $f_{\prx, \prz}$. In particular, 
Instead, we follow the lead of Candes et al. \cite{CetL16}, who proposed to avoid assumptions on $\mathcal L(\pry|\prx, \prz)$, but assume that we have access to $\mathcal L(\prx|\prz)$:\footnote{Candes et al actually require that the full joint distribution $\mathcal L(\prx,\prz)$ is known, but this is because they also test for conditional associations between $\prz$ and $\pry$. We focus only on the relationship between $\prx$ and $\pry$ given $\prz$ and therefore require a weaker assumption.} 
\begin{equation} \label{eq:modelX}
\textit{model-X (MX) assumption}: \mathcal L(\prx| \prz)=  f^*_{\prx|\prz} \text{ for a known } f^*_{\prx|\prz}. 
\end{equation}
Candes et al argue that while both model-$Y|X$ and MX are strong assumptions---especially when $p,d$ are large---in certain cases much more is known about $\prx | \prz$ than about $\pry|\prx, \prz$. In the aforementioned GWAS example, $\prx|\prz$ reflects the joint distribution of genotypes at SNPs across the genome, which is well described by hidden Markov models from population genetics \cite{SetC17}. On the other hand, the distribution $\pry|\prx,\prz$ represents the genetic basis of a complex trait, about which much less is known. In the context of (stratified) randomized experiment, the distribution $\mathcal L(\prx | \prz)$ is the propensity function \cite{Imai2004} (the analog of the propensity score for non-binary treatments \cite{Rosenbaum1983}) and is experimentally controlled. In general causal inference contexts, the MX assumption can be viewed as the assumption that the propensity function is known. 

\subsection{MX methodology and open questions}

Testing CI hypotheses in the MX framework has been the subject of active methodological research. The most popular methodology is MX knockoffs \cite{CetL16}. This method is based on the idea of constructing synthetic negative controls (knockoffs) for each predictor variable in a rigorous way that is based on the MX assumption; see Section~\ref{sec:knockoffs-overview} for a brief overview. Rapid progress has been made on the construction of knockoffs in various cases \cite{SetC17,Romano2019a,Bates2019,Huang2019} and on the application of this methodology to GWAS \cite{SetC17, SetS19}. The conditional randomization test (CRT) \cite{CetL16}, initially less popular than knockoffs due to its computational cost, is receiving renewed attention as computationally efficient variants are proposed, such as the holdout randomization test (HRT) \cite{Tansey2018}, the digital twin test \cite{Bates2020}, and the distilled CRT (dCRT) \cite{Liu2020}. We describe this methodology next.

We start with any test statistic $T(\srx, \sry, \srz)$ measuring the association between $\prx$ and $\pry$, given $\prz$. Usually, this statistic involves learning some estimate $\widehat f_{\pry|\prx,\prz}$ based on machine learning, e.g. the magnitude of the fitted coefficient for $\prx$ (assuming $\text{dim}(\prx) = 1$) in a cross-validated lasso~\cite{T96} of $\sry$ on $\srx$ and $\srz$ \cite{CetL16} . To calculate the distribution of $T$ under the null hypothesis~\eqref{conditional-independence}, first define a matrix  $\srxk \in \mathbb R^{n \times d}$ where the $i$th row $\srxk_i$ is a sample from $\mathcal L(\prx \mid \prz = \srz_i)$. In other words, for each sample $i$, we resample $\srx_i$ based on its distribution conditional on the observed covariate values $Z_i$ in that sample. We then use these resamples to build a null distribution $T(\srxk, \sry, \srz)$, from which we extract the upper quantile
\begin{equation}
	C_{\alpha}(\sry,\srz) \equiv Q_{1-\alpha}[T(\srxk, \sry, \srz)|\sry,\srz],
	\label{upper-quantile}
\end{equation}
where the randomness is over the resampling distribution $\srxk |  \sry, \srz$. Then, the CRT rejects if the original test statistic exceeds this quantile:

\begin{equation}
	\label{eq:randomized-CRT}
	\phi^{\CRT}_T(\srx,\sry,\srz) \equiv 
	\begin{cases}
		1, \quad &\text{if }  T(\srx, \sry, \srz) > C_\alpha(\sry,\srz); \\
		\gamma, \quad &\text{if }  T(\srx, \sry, \srz) = C_\alpha(\sry,\srz); \\
		0, \quad &\text{if }T(\srx, \sry, \srz) < C_\alpha(\sry,\srz).
	\end{cases}
\end{equation}
In order to accommodate discreteness, the CRT makes a randomized decision when $T(\srx, \sry, \srz) = C_\alpha(\sry,\srz)$ so that the size of the test is exactly $\alpha$.
%rejects with probability \[\gamma \equiv \frac{\alpha - \mathbb {P}[T(\srxk, \sry, \srz) > C_\alpha(\sry,\srz)|\sry,\srz]}{\mathbb {P}[T(\srxk, \sry, \srz) = C_\alpha(\sry,\srz)|\sry,\srz]}\] in cases when the test statistic is exactly equal to the rejection threshold. Note that the above probabilities in the definition of $\gamma$ are over the resampling distribution $\srxk\ |\ \sry, \srz$. 
In practice, the threshold $C_\alpha(\sry,\srz)$ is approximated by computing $T(\srxk^b, \sry, \srz)$ for a large number $B$ of Monte Carlo resamples $\srxk^b \sim \srx|\srz$. % For $b = 1,\dots, B$, a resample $\srxk^b$ is drawn and the statistic $T(\srxk^b, \sry, \srz)$ is recomputed. Then, the CRT threshold is obtained through the empirical quantile
%\begin{equation}
%	\widehat C_\alpha(\sry, \srz) \equiv Q_{1-\alpha}\left\{T(\srx, \sry, \srz), \{T(\srxk^b, \sry, \srz)\}_{b = 1, \dots, B}\right\}.
%	\label{empirical-quantile}
%\end{equation}
%If desired, a size $\alpha$ test can be obtained by computing $\gamma$ in the spirit of definition~\eqref{empirical-quantile}, but usually the CRT is implemented as a non-randomized test rejecting when $T(\srx, \sry, \srz) > \widehat C_\alpha(\sry, \srz)$. 
%The randomization $\gamma$ is not applied in practice, the test rejecting when $T(\srx, \sry, \srz) > \widehat C_\alpha(\sry, \srz)$. The finite-sample validity of the CRT for any $B \leq \infty$ and any $n,d,r,p$ follows from the fact that the resampled triples $(\srxk, \sry, \srz)$ are exchangeable with the original $(\srx, \sry, \srz)$ under the null, much like the argument used for a permutation test. 
For the sake of clarity, in this paper we consider only the ``infinite-$B$" version of the CRT as defined by equations~\eqref{upper-quantile} and \eqref{eq:randomized-CRT}. In the causal inference setting, the CRT can be viewed as a variant of Fisher's exact test for randomized experiments that incorporates strata of covariates \cite{Zheng2008,Hennessy2016}, basing inference on rerandomizing the treatment to the units. 

The conditional independence testing problem under MX has benefited from a variety of methodological innovations, but there are still many open theoretical questions about this problem, including the following three:
\begin{enumerate}
	\item[Q1.] Are there ``optimal" test statistics for MX methods, in any sense?
	\item[Q2.] To what extent can the MX assumption be weakened?
	\item[Q3.] What is the precise connection between the performance of the machine learning (ML) step and the power of the resulting MX method?
\end{enumerate}
In this paper, we shed light on these questions. We summarize our findings next.


%The machine learning procedure used to fit $\widehat f_{\pry|\prx,\prz}$ may be time-consuming, and running the CRT usually requires refitting this model for each resample $(\srxk^b, \sry, \srz)$. This can make the CRT, e.g. as originally a computationally proposed \cite{CetL16}, an expensive procedure. However, this computational cost can be dramatically reduced using special kinds of test statistics $T$. For example, the HRT~\cite{Tansey2018} and the related digital twin test~\cite{Bates2020} proceed by training $\widehat f_{\pry|\prx,\prz}$ on an independent sample, and then defining
%\begin{equation}
%T^{\text{HRT}}(\srx, \sry, \srz) \equiv \sum_{i = 1}^n \log \widehat f_{\pry|\prx,\prz}(\sry_i|\srx_i,\srz_i),
%\label{HRT-statistic}
%\end{equation}
%i.e. the log-likelihood of the data under the trained model. Note that the HRT requires only one model fit and is therefore much faster than the originally-proposed CRT. Notably, the sample used for training $\widehat f_{\pry|\prx,\prz}$ need not come from the same distribution as $\mathcal L$ and can therefore be much larger. For example, in the context of the digital twin test \cite{Bates2020} it was pointed out that large observational datasets can be used to train $\widehat f_{\pry|\prx,\prz}$, while applying a variant of the CRT to a smaller experimental dataset to obtain a causal guarantee. Recently, the dCRT \cite{Liu2020} was proposed as an alternative test statistic for the CRT that allows in-sample training of $\widehat f_{\pry|\prx,\prz}$ while circumventing refitting.

%While there are now a variety of MX methodologies, they are unified by the following guiding principles:
%\begin{enumerate}
%	\item[P1.] (Machine learning) An approximation to the distribution $\pry|\prx,\prz$ is learned, and used to define a test statistic.
%	\item[P2.] (MX calibration) The test statistic is calibrated using the known $\prx|\prz$.
%	\item[P3.] (Conditional inference) Inference is valid conditionally on the observed $\sry$ and $\srz$. %\textcolor{red}{Recall (2)...}
%\end{enumerate}
%These are well-known to anyone familiar with MX methodology, but we find it useful to state them explicitly. P1 has empirically been found to be important for the power of MX methods: the accuracy of the machine learning method employed translates into improved power. Importantly, however, Type-I error control is guaranteed regardless of the quality of the learned distribution for $\pry|\prx,\prz$. This is because of P2: the calibration of the test statistic uses only the known distribution $\prx|\prz$. This leads to P3, conditional inference. Since only the distribution of $\srx$ is used for inference, MX methods calibrate their test statistics while holding $\sry$ and $\srz$ fixed. 
%
%Thus, several elegant methods have been designed, and important scientific applications have been identified where the MX assumption is reasonable. However, the search for powerful MX methodology has thus far not been grounded in a firm theoretical foundation. In particular, the following questions remain open:
%\begin{enumerate}
%	\item[Q1.] Are there ``optimal" test statistics for MX methods, in any sense?
%	\item[Q2.] To what extent can the MX assumption be weakened?
%	\item[Q3.] What is the precise connection between the performance of the machine learning (ML) step and the power of the resulting MX method?
%	\item[Q4.] Can the MX framework be used for estimation? If so, what estimands are of interest?
%\end{enumerate}
%Furthermore, this rapidly growing area remains somewhat disconnected from the vast theory on statistical testing and estimation. Can we leverage existing statistical theory to better understand MX methods? In this paper, we begin to address these questions. We summarize our main findings next.

\subsection{Our contributions}



We find that the CRT is a natural setting to analyze the MX CI problem; it is simpler to analyze than MX knockoffs and is applicable for testing a single conditional independence hypothesis. For these reasons, we focus mainly on the CRT in the present paper. We obtain the following (partial) answers to the questions posed above.

\paragraph{A1: MX CI is a conditional inference problem and a CRT is most powerful against point alternatives.}

While the composite alternative of the CI problem~\eqref{conditional-independence} suggests that we cannot expect to find a uniformly most powerful test, we may still ask what is the most powerful test against a point alternative. We show that any level $\alpha$ test must also have level $\alpha$ conditionally on $(\sry, \srz)$, which allows us to reduce the composite null to a point null. We can therefore apply the Neyman-Pearson lemma to show (Section~\ref{sec:power}) that the optimal test against a point alternative $\mathcal L$ with $\mathcal L(\pry|\prx,\prz) = \bar f_{\pry|\prx,\prz}$ is the CRT based on the likelihood test statistic:
\begin{equation}
	T^{\text{opt}}(\srx; \sry, \srz) \equiv \prod_{i = 1}^n \bar f(\sry_i|\srx_i, \srz_i).
\end{equation}
The same statistic yields the most powerful one-bit $p$-values for MX knockoffs (Section~\ref{sec:knockoffs}). Since the model for $Y|X,Z$ is unknown, this result provides our first theoretical indication of the usefulness of ML models to learn this distribution (Q3). A3 below gives a more quantitative answer to Q3.

\paragraph{A2: The MX assumption can be drastically weakened while retaining asymptotic Type-I error control.}
Huang and Janson \cite{Huang2019} recently showed that finite-sample type-I error control is possible under only the assumption that the model for $(\prx, \prz)$ belongs to a known parametric family. Going further, if asymptotic validity is sufficient, we show in Section~\ref{sec:weakening} that we need only the 
\begin{equation}
\begin{split}
&\textit{MX(2) assumption:} \text{ the first two moments of $\prx|\prz$ are known, i.e.} \\
&\mathbb E_{\mathcal L}[\prx|\prz] = \mu(\prz) \text{ and } \text{Var}_{\mathcal L}[\prx|\prz] = \Sigma(\prz) \text{ for known } \mu(\cdot), \Sigma(\cdot).
\label{MX(2)-intro}
\end{split}
\end{equation}
We show that the CRT, paired with the \textit{generalized covariance measure} statistic of Shah and Peters \cite{Shah2018}, retains asymptotic Type-I error control under the MX(2) assumption. Requiring knowledge of just the first two moments of the conditional distribution $\prx|\prz$, rather than the distribution itself, promises to broaden the scope of application of MX-style methodology.


\paragraph{A3: The prediction error of the ML method impacts the asymptotic efficiency of the CRT but not its consistency.}

It has been widely observed that the better the ML method approximates $\pry|\prx,\prz$, the higher power the MX method will have. We put this empirical knowledge on a theoretical foundation by expressing the asymptotic power of the CRT in terms of the prediction error of the underlying ML method (Section~\ref{sec:asymptotic-power}). In particular, we consider semiparametric alternatives of the form
\begin{equation}
H_1: \mathcal L(\pry|\prx,\prz) = N(\prx^T\beta + g(\prz),\sigma^2).
\label{parametric-alternative-intro}
\end{equation}
We analyze the power of a CRT variant that employs a separately trained estimate $\widehat g$ in an asymptotic regime where $d = \text{dim}(\prx)$ remains fixed while $p = \text{dim}(\prz)$ grows arbitrarily with the sample size $n$. We find that this test is consistent no matter what $\widehat g$ is used, while its asymptotic power against local alternatives $\beta_n = h/\sqrt{n}$ depends on the limiting mean-squared prediction error of $\widehat g$ (denoted $\mathcal E^2$) and the limiting expected variance $\mathbb E[\text{Var}[\prx | \prz]]$ (denoted $s^2$). For example, if $d = 1$,
\begin{equation*}
\begin{split}
\text{CRT power converges to that of normal location test under alternative } N\smash{\left(\frac{hs}{\sqrt{\sigma^2 + \mathcal E^2}},1\right)}.
\end{split}
\end{equation*}
This represents the first explicit quantification of the impact of ML prediction error on the power of an MX method.
%This result clearly shows how the prediction error of $\widehat g$ impacts the asymptotic efficiency of the CRT. Unlike parallel work in causal inference, the above power result holds conditionally on $\sry, \srz$, almost surely. 

%\paragraph{A4: Nonparametric targets can be consistently estimated under MX using causal inference tools.}
%
%All MX methodology thus far has focused on testing. In Section~\ref{sec:causal}, we approach the subject of estimation under MX by drawing close connections between MX and causal inference, since estimation is a major thrust of the latter field. The MX setting is indeed remarkably similar to that of a randomized experiment with continuous or multivariate treatment and covariates, where the \textit{propensity function} (a generalization of the propensity score) is known. The MX calibration principle (P2) is directly analogous to using randomization as the basis for inference, in both cases obviating the need for any assumptions on $\pry|\prx,\prz$. The question of even defining the target of estimation is a nontrivial one, and several options are possible. A nonparametric generalization of the parameter $\beta$ in the model~\eqref{parametric-alternative-intro} is
%%\begin{equation*}
%$
%\beta(\mathcal L) ~ \equiv ~
%\mathbb E_{\mathcal L}[\text{Var}[\prx|\prz]]^{-1}
%\mathbb E_{\mathcal L}[\text{Cov}[\prx,\pry|\prz]],
%$
%%\end{equation*}
%which is related to the \textit{variance-weighted average treatment effect} in causal inference \cite{Li2011}. 
%%We complement existing results on estimation of this quantity (e.g., \cite{Robins2009, Li2011}) by showing that a standard regression-based estimator is consistent, conditionally on $\sry,\srz$, almost surely in $\sry,\srz$. 
%We also suggest that the \textit{dose-response function} \cite{Hirano2004} may be an interesting estimand in the MX framework; this quantity can be estimated using causal inference tools \cite{Kennedy2017} if the propensity function is known.




\paragraph*{}

These advances shed new light on the nature of the MX problem and can inform methodological design. Our results handle multivariate $\prx$, arbitrarily correlated designs in the model for $\prx$, and any black-box machine learning method to learn $\widehat g$. 
\paragraph{Notation.}

Recalling equations~\eqref{conditional-independence} and \eqref{eq:xyz}, population-level variables (such as $\prx,\pry,\prz$) are denoted in boldface, while samples of these variables (such as $\srx_i,\sry_i,\srz_i$) are denoted in regular font. All vectors are treated as column vectors. 
% For example, $\srx \in \mathbb R^{n \times d}, \sry \in \mathbb R^{n \times r}, \srz \in \mathbb R^{n \times p}$. 
We often use uppercase symbols to denote both random variables and their realizations (for either population- or sample-level quantities), but use lowercase to denote the latter when it is important to make this distinction. We use $\mathcal L$ to denote the joint distribution of $(\prx,\pry,\prz)$, though we sometimes use this symbol to denote the joint distribution of $(\srx,\sry,\srz)$ as well. We use the symbol ``$\equiv$" for definitions. We denote by $c_{d,1-\alpha}$ the $1-\alpha$ quantile of the $\chi^2_d$ distribution, and by $\chi^2_d(\lambda)$ the non-central $\chi^2$ distribution with $d$ degrees of freedom and noncentrality parameter $\lambda$.


%\section{Conditional randomization test methodology} \label{sec:background}
%
%\subsection{Holdout Randomization Test (HRT)} \label{sec:HRT}
%
%The HRT first randomly splits the data into training and test sets of possibly unequal sizes. The model $\widehat \theta = \widehat \theta_{\text{train}}$ is fit on the training set, and then the CRT is applied on the test set, conditionally on $\widehat \theta_{\text{train}}$. In this paper, we keep the training set implicit and use $(\srx, \sry, \srz)$ for the test set. In this notation, the authors \cite{Tansey2018} proposed the following test statistic:
%\begin{equation}
%T(\srx, \sry, \srz) \equiv \sum_{i = 1}^n \log f_{\pry|\prx,\prz}^{\widehat \theta_{\text{train}}}(\sry_i|\srx_i,\srz_i),
%\label{HRT-statistic}
%\end{equation}
%i.e. the log-likelihood of the data under the trained model. Note that the HRT requires only one model fit and is therefore much faster than the full CRT. In fact, the HRT is simply a special case of the CRT based on a test statistic that is easy to compute. The HRT trades some statistical efficiency for this computational speed due to its use of sample splitting. We note that the recently proposed digital twin test for causal inference from trio studies \cite{Bates2020} is an instance of the HRT, since the multivariate model employed there is fit once on a separate source of data from the CRT resampling.
%
%
%\subsection{The Hybrid and MX(2) CRTs \textcolor{red}{do we need this business?}} \label{sec:hybrid-crt}
%The recently-proposed distilled CRT methodology \cite{Liu2020}  njoys computational speed and power approaching those of the HRT and CRT, respectively. This method is based on the observation that, if a part of $\widehat \theta$ is fit on only $(\sry, \srz)$, then there is no need to refit it upon resampling. Therefore, the in-sample data can be used efficiently as long as the computationally intensive part excludes $\srx$. Leveraging out-of-sample data as well, we obtain the Hybrid CRT (Algorithm~\ref{alg:hybrid-crt}), which we formulate in terms of the semiparametric model~\eqref{parametric-alternative-intro}. It is a hybrid of the HRT and CRT, as it fits $g$ only once (like the HRT) but refits $\beta$ after each resampling of $\srx_i | \srz_i$ (like the CRT). In contrast, the HRT fits $\beta,g$ just once, while the full CRT refits $\beta,g$ together in every resampling step.
%
%\noindent
%\begin{center}
%	\begin{minipage}{\linewidth}
%		\begin{algorithm}[H]
%			%			\SetAlgorithmName{Procedure}{}\; %last arg is the title of listing table		
%			\KwData{
%				$\{(\srx_i, \sry_i, \srz_i)\}_{i=1}^n$, loss function $\ell$, machine learning method $g$
%			}
%			Obtain $\widehat g$ by fitting $g$ on $\{(\sry_i,\srz_i)\}_{i=1}^n$ or a different dataset or both\;
%			Solve for $\smash{\widehat \beta \equiv \underset{\beta\in \mathbb{R}}{\text{min}}\ \sum_{i = 1}^n \ell(\sry_i, (\srx_i-\mathbb E[\srx_i|\srz_i]) \beta + \widehat g_n(\srz_i))}$\;
%			Compute $T(\srx,\sry,\srz) \equiv \sum_{i = 1}^n \ell(\sry_i, (\srx_i-\mathbb E[\srx_i|\srz_i]) \widehat \beta + \widehat g_n(\srz_i))$\;
%			\For{$b \in \{1,\dots,B\}$}{
%				Sample $\srxk_i^b$ from $\mathcal L(\srx_i|\srz_i)$ for all $i$\;
%				Compute $T(\srxk^b,\sry,\srz)$ using steps 2,3 with $\srxk_i^b$ replacing $\srx_i$.
%			}
%			% Compute $\widehat p_j$ based on formula~\eqref{CRT}\;
%			\KwResult{Hybrid CRT Monte Carlo $p$-value $\widehat p$ based on formula~\eqref{CRT}.}
%			\textbf{Cost:} One $p$-dimensional model fit, $B$ univariate optimizations.
%			\caption{\bf The Hybrid CRT}
%			\label{alg:hybrid-crt}
%		\end{algorithm}
%	\end{minipage}
%\end{center}
%
%If $\widehat g$ is trained on separate data and $\ell$ is the squared loss, we show that the asymptotic distribution of the Hybrid CRT statistic can be computed exactly and requires only the first two moments of $\prx|\prz$ \eqref{MX(2)-intro}. This yields the \textit{MX(2) $t$-test} (Algorithm~\ref{alg:twomoment-crt}). The exact null distribution facilitates uniform asymptotic Type-I error control under only the MX(2) assumption (Section~\ref{sec:weakening}) as well as the derivation of an explicit power formula (Section~\ref{sec:asymptotic-power}), something that has not yet been accomplished for any variant of the CRT.
%%This is the first power analysis of the CRT.
%
%%Of course, if one desires, the CRT resampling can still be used for finite-sample validity, and the explicit null is still useful to derive power properties of the test, something that has not been accomplished for any variant of the CRT.
%
%
%\begin{center}
%	\begin{minipage}{\linewidth}
%		\begin{algorithm}[H]
%			%			\SetAlgorithmName{Procedure}{}\; %last arg is the title of listing table		
%			\KwData{
%				$\{(\srx_i, \sry_i, \srz_i)\}_{i=1}^n$, $\mu_n(\cdot)$ and $\Sigma_n(\cdot)$ in \eqref{MX(2)-intro}, learning method $g$
%			}
%			Obtain $\widehat g$ by fitting $g$ on  a separate dataset\;
%			Recall $\mu_n(\srz_i) \equiv \mathbb E[\srx_i|\srz_i], \text{ set } \widehat S_n^2 \equiv \frac{1}{n}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))^2\Sigma_n(\srz_i)$\;
%			Set $\smash{T_n \equiv \frac{\widehat S_n^{-1}}{\sqrt{n}}\sum_{i = 1}^n (\srx_{i} - \mu_n(\srz_i))(\sry_{i} - \widehat g_n(\srz_{i}))}$\;
%			% Compute $T(\srx,\sry,\srz) \equiv \sum_{i = 1}^n \ell(\sry_i, (\srx_i-\mu_n(\srz_i)) \widehat \beta + \widehat g_n(\srz_i))$\;
%			% \For{$b \in \{1,\dots,B\}$}{
%			% 	Sample $\srxk_i^b$ from $\mathcal L(\srx_i|\srz_i)$ for all $i$\;
%			% 	Compute $T(\srxk^b,\sry,\srz)$ using steps 3, 4 with $\srxk_i^b$ replacing $\srx_i$\;
%			% }
%			% Compute $\widehat p_j$ based on formula~\eqref{CRT}\;
%			\KwResult{MX(2) $t$-test asymptotic $p$-value $\widehat p \equiv \Phi(T_n)$.}
%			\textbf{Cost:} One $p$-dimensional model fit.
%			\caption{\bf The MX(2) $\bm t$-test}
%			\label{alg:twomoment-crt}
%		\end{algorithm}
%	\end{minipage}
%\end{center}
%



\section{The most powerful test against point alternatives} \label{sec:power}

In this section, we seek the most powerful MX CI test against a point alternative. To accomplish this, we observe that any (marginally) level $\alpha$ test must also have level $\alpha$ conditionally on $\sry, \srz$ (Section~\ref{sec:reduction-by-conditioning}). The latter conditioning step reduces the composite null to a point null. This reduction allows us to invoke the Neyman Pearson lemma to find the most powerful test  (Section~\ref{sec:NP}). Proofs are deferred to Appendix~\ref{sec:proofs-sec2}.

\subsection{Conditioning reduces the composite null to a point null} \label{sec:reduction-by-conditioning}

Let us first formalize the definition of a level $\alpha$ test of the MX CI problem. The null hypothesis is defined as the set of joint distributions compatible with conditional independence and with the assumed model for $\prx|\prz$:
\begin{equation}
	\label{eq:null-under-modelX}
	\begin{split}
		\mathscr L^{\text{MX}}_0(f^*) &\equiv \mathscr L_0 \cap \mathscr L^{\text{MX}}(f^*) \\
		&\equiv \{\mathcal L: \prx \independent \pry \mid \prz\} \cap \{\mathcal L: \mathcal L(\prx|\prz) = f^*_{\prx|\prz}\}\\
		&= \{\mathcal L: \mathcal L(\prx,\pry,\prz) = f_{\prz} \cdot f^*_{\prx|\prz} \cdot f_{\pry|\prz} \text{ for some } f_{\prz}, f_{\pry|\prz} \}.
	\end{split}
\end{equation}
A test $\phi: (\mathbb R^{d} \times \mathbb R^r \times \mathbb R^p)^n \rightarrow [0,1]$ of the MX CI problem is level $\alpha$ if
\begin{equation}
	\sup_{\mathcal L \in \mathscr L^{\text{MX}}_0(f^*)} \mathbb E_{\mathcal L}[\phi(\srx, \sry, \srz)] \leq \alpha.
	\label{marginal-validity}
\end{equation}

Since the CRT calibrates the test statistic $T$ conditionally on the observed $\sry, \srz$ (recall definition~\eqref{upper-quantile}), it is easy to verify that this test not only has level $\alpha$ in the sense of equation~\eqref{marginal-validity} but also has level $\alpha$ \textit{conditionally} on $\sry$ and $\srz$:
\begin{equation}
	\sup_{y,z} \sup_{\mathcal L \in \mathscr L_0^{\textnormal{MX}}(f^*)}\ \mathbb E_{\mathcal L}[\phi^{\textnormal{CRT}}_T(\srx, \sry, \srz)|\sry = \sfy, \srz = \sfz] \leq \alpha.
\label{eq:conditional-validity}
\end{equation}
It turns out that any level $\alpha$ test $\phi$ has this same property:\footnote{We thank Michael Celentano for pointing out this fact to us.}
\begin{proposition} \label{prop:marginal-implies-conditional}
	Any level $\alpha$ test $\phi$ of conditional independence under the MX assumption (i.e. any test satisfying property~\eqref{marginal-validity}) must also have conditional level $\alpha$:
	\begin{equation}
		\sup_{y,z} \sup_{\mathcal L \in \mathscr L_0^{\textnormal{MX}}(f^*)}\ \mathbb E_{\mathcal L}[\phi(\srx, \sry, \srz)|\sry = \sfy, \srz = \sfz] \leq \alpha.
	\end{equation}
\end{proposition}

Proposition~\ref{prop:marginal-implies-conditional} allows us to reframe the MX CI problem as that of testing a null hypothesis with respect to the conditional law $\mathcal L(\srx|\sry,\srz)$, viewing only $\srx$ as random while fixing $\sry$ and $\srz$ at their observed values. It states that any level $\alpha$ test $\phi$, when viewed as a \textit{family} of hypothesis tests $\phi(\srx; y,z)$ indexed by $(\sfy,\sfz)$, has level $\alpha$ in the conditional testing problem for each $(\sfy, \sfz)$. This conditional perspective is useful because it reduces the composite null \eqref{conditional-independence} to a point null. Indeed, note that 
\begin{equation}
	\mathcal L \in \mathscr L_0^{\text{MX}}(f^*) \Longrightarrow \mathcal L(\srx = \sfx|\sry = \sfy, \srz = \sfz) = \prod_{i = 1}^n  f^*(\sfx_i | \sfz_i).
	\label{point-null}
\end{equation}
Therefore, $\mathcal L(\srx|\sry = \sfy, \srz = \sfz)$ equals a fixed product distribution for any null $\mathcal L$. This yields a conditional point null hypothesis. Note that the observations $\srx_i$ are independent \textit{but not identically distributed} due to the different conditioning events in~\eqref{point-null}.

The preceding observations will not be surprising to anyone familiar with MX methodology, and in fact the existence of a single null distribution from which to resample $\srxk$ is central to the very definition of the CRT. Nevertheless, we find it useful to state explicitly what has thus far been largely left implicit. This conditional perspective allows us to derive the optimal test against a point alternative by applying the  Neyman-Pearson lemma in the conditional problem.

%
%However, despite the fact that the hypothesis~\eqref{point-null} is a point null when viewing only $\srx$ as the data, the resulting goodness-of-fit testing problem is not a fully standard one, because the observations are independent \textit{but not identically distributed}. Specifically, under the null, we have
%\begin{equation*}
%\mathcal L(\srx_i|\sry_i, \srz_i) \overset{\text{ind}} \sim  f^*(\srx_i|\srz_i), \quad i = 1, \dots, n.
%\end{equation*}
%The different conditioning events cause $\srx_i$ to have different conditional distributions across $i$.
%
%Nevertheless, from this perspective, the CRT resampling procedure is simply the calibration of any test statistic under the point null~\eqref{point-null}. This establishes a formal connection between the model-X setting and a more traditional hypothesis testing problem with point null. 
%
%
%% We record this equivalence below.
%% {\color{red} to be fixed}
%% \begin{proposition} \label{prop:connection}
%% Consider any function $T(\srx, \sry, \srz)$ as the test statistic. Let $\phi_T(\srx, \sry, \srz)$ represent the level $\alpha$ CRT based on this test statistic. On the other hand, for each fixed $\sry, \srz$, let $\phi'_T(\srx; \sry, \srz)$ represent the level $\alpha$ test of hypothesis~\eqref{point-null} that rejects for large values of $T(\srx, \sry, \srz)$. Then, 
%% \begin{equation}
%% \phi_T(\srx, \sry, \srz) = \phi'_T(\srx; \sry, \srz) \quad \text{for all } \srx, \sry, \srz.
%% \end{equation}
%% In other words, there is a one-to-one correspondence between CRTs and tests of the point null~\eqref{point-null}.
%% \end{proposition}
%%Let us write this conditional hypothesis testing problem more explicitly. First, note that
%%\begin{equation}
%%f_{\prx|\pry, \prz} = f^*_{\prx \mid \prz}\frac{f_{\pry|\prx, \prz}}{f_{\pry|\prz}}.
%%\end{equation}
%%Under the null hypothesis, note that $f_{\pry|\prx, \prz} = f_{\pry|\prz}$, so $f_{\prx|\pry, \prz} = f_{\prx \mid \prz} = f^*_{\prx \mid \prz}$, the latter being fixed by the model-X assumption. Therefore, after conditioning, we may rewrite the hypothesis~\eqref{conditional-independence} as
%%\begin{equation}
%%H_0: (\srx^i|\sry^i, \srz^i) \overset{\text{ind}} \sim  f^*(\srx^i|\srz^i) \ \  \text{vs.} \ \ H_1: (\srx^i|\sry^i, \srz^i) \overset{\text{ind}} \sim  f^*(\srx^i|\srz^i)\frac{f(\sry^i|\srx^i, \srz^i)}{f(\sry^i|\srz^i)},
%%\label{conditional-independence-conditional}
%%\end{equation}
%%where $H_0$ is a point null and the observations $\srx^i$ are independent, but not identically distributed due to the different conditioning events. While a valid test of the original hypothesis~\eqref{conditional-independence} need not be a valid test of the conditional hypothesis~\eqref{conditional-independence-conditional} for every fixed $(\sry, \srz)$, every CRT is. From this perspective, the CRT resampling procedure is simply the calibration of any test statistic under a point null hypothesis. We record this equivalence below.
%
%% \sout{The above proposition shows that not only is the CRT a valid test of the conditional hypothesis \eqref{point-null}, but \emph{every} test of \eqref{point-null} must be a version a CRT.}
%
%% It is important to note that in this conditional problem \eqref{point-null}, the observations $\srx_i$ are independent but not identically distributed, due to the different conditioning events, making it a somewhat nonstandard test of a point null. 
%
%The aforementioned reduction from (composite) CI testing to (point) goodness-of-fit testing is straightforward, and may well be known to those familiar with the CRT, despite not being explicitly written down. This observation is merely the starting point of our analysis, and we leverage this connection to obtain nontrivial results about the power of the CRT. We start doing so by considering optimality against point alternatives.

\subsection{Most powerful test against point alternatives} \label{sec:NP}

The following theorem states that the CRT based on the likelihood with respect to the (unknown) distribution $\pry|\prx, \prz$ is the most powerful test against a point alternative. To prepare for the statement, fix an alternative distribution $\bar{\mathcal L} \in \mathscr L^{\text{MX}}(f^*)$, and let $\bar f_{\pry|\prx, \prz}$ be the density of $\bar{\mathcal L}(\pry|\prx, \prz)$. % Next, note that the MX assumption \eqref{eq:modelX} can be used to determine
%\begin{equation}
%\bar f(\sfy_i|\sfz_i) \equiv \int \bar f(\sfy_i|\sfx_i, \sfz_i)f^*(\sfx_i|\sfz_i)d\sfx_i.
%\end{equation}

\begin{theorem} \label{prop:crt-optimality}
	Let $\bar{\mathcal L} \in \mathscr L^{\text{MX}}(f^*)$ be an alternative distribution, with $\bar{\mathcal L}(\pry|\prx,\prz) = \bar f_{\pry|\prx,\prz}$. The likelihood of the data $(\srx, \sry, \srz)$ with respect to $\bar{\mathcal L}(\pry|\prx,\prz)$ is
	\begin{equation}
	T^{\textnormal{opt}}(\srx, \sry, \srz) \equiv \prod_{i = 1}^n  \bar f(\sry_i|\srx_i, \srz_i).
	\label{log-likelihood-ratio}
	\end{equation}
	The CRT $\phi^{\CRT}_{T^{\textnormal{opt}}}$ based on this test statistic is the most powerful test of $H_0: \mathcal L \in \mathscr L_0^{\textnormal{MX}}(f^*)$ against $H_1: \mathcal L = \bar{\mathcal L}$, i.e. 
	\begin{equation}
	\mathbb E_{\bar{\mathcal L}}[\phi(\srx,\sry,\srz)] \leq \mathbb E_{\bar{\mathcal L}}[\phi^{\CRT}_{T^\textnormal{opt}}(\srx, \sry, \srz)]
	\label{unconditional}
	\end{equation}
	for any level $\alpha$ test $\phi$.
\end{theorem}

%The form of the optimal statistic $T^{\text{opt}}$ may be surprising, as it is a likelihood ratio with respect to $\sry|\srx,\srz$ rather than $\srx|\sry,\srz$. In fact, we see in the proof (Appendix~\ref{sec:proofs-sec2}) that these two likelihood ratios are equivalent, with the former yielding a more explicit expression. Note also that the denominator of $T^{\text{opt}}$ can be omitted if desired, since it is not a function of $\srx$. We keep it for a more direct parallel with the model-$\sry|\srx$ case. % Though this paper is primarily concerned with the CRT, we also obtain a parallel result about MX knockoffs in Section~\ref{sec:knockoffs}. 
The proof of Theorem~\ref{prop:crt-optimality} is based on the reduction in Section~\ref{sec:reduction-by-conditioning} of the composite null to a point null by conditioning. This argument has similar flavor to the theory of unbiased testing (see Lehmann and Romano \cite[Chapter 4]{TSH}), where uniformly most powerful unbiased tests can be found by conditioning on sufficient statistics for nuisance parameters. Our result is also analogous to but different from Lehmann's derivation of the most powerful permutation tests using conditioning followed by the Neyman-Pearson lemma, in randomization-based causal inference (see the rejoinder of Rosenbaum's 2002 discussion paper \cite{Rosenbaum2002}, Section 5.10 of Lehmann (1986), now Lehmann and Romano \cite[Section 5.9]{TSH}).

Inspecting the most powerful test given by Theorem~\ref{prop:crt-optimality}, we find that it depends on $\bar{\mathcal L}$ only through $\bar{\mathcal L}(\pry|\prx,\prz)$. This immediately yields the following corollary.
\begin{corollary} \label{cor:crt-optinality}
	Define the composite class of alternatives
	\begin{equation*}
	\begin{split}
	\mathscr L_1(f^*, \bar f) &= \{\mathcal L \in \mathscr L_0^{\textnormal{MX}}(f^*): \bar{\mathcal L}(\pry|\prx,\prz) = \bar f_{\pry|\prx,\prz}\} \\
	&= \{\mathcal L: \mathcal L(\prx,\pry,\prz) = f_{\prz} \cdot f^*_{\prx|\prz} \cdot \bar f_{\pry|\prx,\prz} \textnormal{ for some } f_{\prz}\}.
	\end{split}
	\end{equation*}
	The CRT $\phi^{\CRT}_{T^{\textnormal{opt}}}$ is the uniformly most powerful test of $H_0: \mathcal L \in \mathscr L_0^{\textnormal{MX}}(f^*)$ against $H_1: \mathcal L \in \mathscr L_1(f^*, \bar f)$.
\end{corollary}

Theorem~\ref{prop:crt-optimality} and Corollary~\ref{cor:crt-optinality} state that the most powerful test against a point alternative is the CRT based on the test statistic defined as the measuring how well the data $(\srx, \sry, \srz)$ fit the distribution $\bar{\mathcal L}(\pry|\prx,\prz)$. For example, if
\begin{equation}
\bar f(\pry|\prx,\prz) = N(\prx^T\beta + \prz^T \gamma, \sigma^2) \text{ for coefficients } \beta \in \mathbb R^d \text{ and } \gamma \in \mathbb R^p,
\label{linear-model}
\end{equation}
then the optimal test rejects for small values of $\|\sry - \srx \beta - \srz \gamma \|^2$. In Section~\ref{sec:knockoffs}, we establish an analogous optimality statement for MX knockoffs as well. Since the optimal test depends on the alternative distribution $\bar{\mathcal L}(\pry|\prx,\prz)$, CRT and MX knockoffs implementations usually employ a machine learning step to search through the composite alternative (not unlike a likelihood ratio test) for a good approximation $\widehat f_{\pry|\prx,\prz}$. These approximate models are then summarized in various ways to define a test statistic $T$. There is no consensus yet on the best test statistic to use, with some authors \cite{CetL16, SetC17, SetS19} using combinations of fitted coefficients $\widehat \beta$ and others  \cite{Tansey2018, Bates2020} using loss-based test statistics. The above optimality results align more closely with the latter strategy. Loss-based test statistics also have the advantage of avoiding ad hoc combination rules for $\widehat \beta \in \mathbb R^d$ in cases where $d > 1$. It remains to be seen whether loss-based or coefficient-based test statistics yield greater power in practice.

%\begin{equation}
%\begin{split}
%-\log T^{\textnormal{opt}}(\srx, \sry, \srz) &=  \sum_{i = 1}^n \log\left(\frac{\bar f(\sry_i|\srx_i, \srz_i)}{\bar f(\sry_i|\srz_i)}\right) \\
%&= -\frac12 \frac{\|\sry - \srx \beta - \srz \gamma \|^2 - \|\sry - \srz \gamma \|^2}{\sigma^2} + C,
%\end{split}
%\label{F-test}
%\end{equation}
%where $C$ is a constant not depending on the data. Since monotone transformations of the test statistic such as the logarithm do not change the test, the log-likelihood ratio $\log T^{\textnormal{opt}}$ is optimal as well. 

%Thus, the optimal CI test against the point alternative with linear $\bar f(\pry|\prx,\prz)$ is based on the familiar $F$ statistic. The analogy with the $F$-test is made even more direct in Section~\ref{sec:MX(2)-regression}, where we construct a CRT with $\widehat \beta$ learned from the data. 

%\begin{remark} \label{rem:randomization}
%	The randomized CRT \eqref{eq:randomized-CRT} differs from the usual CRT~\eqref{CRT} only if discreteness prevents the latter from having size $\alpha$.  In this case, the Neyman-Pearson lemma dictates that a randomized test is the most powerful. Note that the randomization due to $\gamma$ is distinct from the Monte-Carlo randomization arising from approximating $C_\alpha$ based on a finite number of resamples: the former corresponds to the case when ties between the resampled and original statistic occur with nonzero probability. See \cite{Hemerik2018} for a recent discussion of exact resampling-based tests. Theorem~\ref{prop:crt-optimality} must be stated in terms of randomized tests for full rigor, but in most situations, ties are expected to occur very rarely if at all. This is the case when either $\prx|\prz$ or $\pry|\prx,\prz$ have continuous distributions. Therefore, the usual CRT would have very similar power without the extra randomization.
%\end{remark}

% With this optimality result in place, we discuss some of its implications for the design of MX tests.

%\subsection{Weakening the MX assumption}
%
%In cases when the effects of $\prz$ on $\pry$ are not of interest, we can weaken the MX assumption while preserving the point null property~\eqref{point-null}. Instead of assuming we know the joint distribution $\mathcal L(\prx, \prz) = f^*_{\prz} \cdot f^*_{\prx \mid \prz}$, we need only assume that the distribution $\mathcal L(\prx \mid \prz)$ is available. This is clear from the fact that neither the conditional null~\eqref{point-null} nor the conditional alternative~\eqref{conditional-alternative} depends on $f^*_{\prz}$. These observations lead to the following corollary of Theorem~\ref{prop:crt-optimality}.
%
%\begin{corollary} \label{crt-corollary}
%	Suppose we are given distributions $f^*_{\prx \mid \prz}$ and $\bar f_{\pry| \prx, \prz}$, and consider the problem
%	\begin{equation}
%	{\small \smash{
%	H_0: \pry \independent \prx \mid \prz \text{~~vs.~~} H_1: \mathcal L(\prx, \pry, \prz) = f_{\prz} \cdot f^*_{\prx \mid \prz} \cdot \bar f_{\pry|\prx, \prz}, \text{~for some~} f_{\prz}.
%	}}
%	\end{equation}
%	Then, the (randomized) CRT based on the likelihood ratio is uniformly most powerful, conditionally on $\sry = \sfy, \srz = \sfz$, for all $\sfy, \sfz$.
%\end{corollary}
%
%The fact that there is no need to know $\mathcal L(\prz)$ was implicit in the setup and construction of the conditional permutation test \cite{Berrett2019}, another MX method. In most instances of the MX setup, however, we have many variables of interest, denoted $\prx_1, \dots, \prx_p$. For each variable $j$, $\prx_j$ plays the role of $\prx$ and $\prx_{-j}$ plays the role of $\prz$. We are then interested in testing conditional independence for each $j$, in which case the full MX assumption is required.

%The optimality of the likelihood ratio extends to knockoffs as well; see Theorem~\ref{prop:knockoff-optimality} in Section~\ref{sec:knockoffs}. These results are the first quantitative expressions of the ``folk wisdom" that the most powerful MX methods are those that learn a good approximation to the model $\pry|\prx, \prz$. This is the goal of the machine learning component of any MX method (P1). A way of interpreting this in the language of classical hypothesis testing is that the most powerful test depends on the distribution $\pry|\prx, \prz$. Since a point alternative is rarely specified, the ``full'' CRT searches through the space of alternatives to find a good model under the alternative 
%$\widehat f_{\pry|\prx, \prz}$, analogous to the generalized likelihood ratio test.
%
%A missing piece of intuition in the design of model-X methods is how exactly to turn the learned model $\widehat f$ into a test statistic, especially when $\prx$ represents a group of variables. While there is no consensus on this question, the prevailing approach  \cite{CetL16, SetC17, SetS19} appears to be to fit a linear model $\pry = \prx^T \widehat \beta + \prz^T \widehat \gamma$, and then use some combination of the coefficients $\widehat \beta$ to form the test statistic, such as the sum of the magnitudes $\sum_j |\widehat \beta_j|$. However, this choice is usually recognized by the authors as somewhat arbitrary. The results in the preceding section can help fill in this missing piece. The example after Theorem~\ref{prop:crt-optimality} and equation~\eqref{F-test} in particular suggest that a better test statistic would be the (log)likelihood ratio of the fitted model for $\pry|\prx, \prz$, i.e. an F-statistic. For generalized linear models, we would obtain the deviance loss. Recall from equation~\eqref{HRT-statistic} that these loss-based test statistics have recently been proposed in the context of the holdout randomization test \cite{Tansey2018} and the digital twin test \cite{Bates2020}. Therefore, Theorem~\ref{prop:crt-optimality} (and Theorem~\ref{prop:knockoff-optimality} for knockoffs) provide a theoretical justification for their choices, and more generally give clear guidance for how to construct test statistics for MX methods.

\paragraph*{}
Intuitively, the results of this section suggest that the more successful $\widehat f_{\pry|\prx,\prz}$ is at approximating the true alternative $f_{\pry|\prx,\prz}$, the more powerful the corresponding CRT will be. We make this relationship precise in an asymptotic setting in Section~\ref{sec:asymptotic-power}. Before examining the asymptotic power of the CRT, we establish in the next section that asymptotic Type-I error control can be obtained under a weaker form of the MX assumption.

\section{Weakening the MX assumption} \label{sec:weakening}

Instead of assuming knowledge of the entire conditional distribution $\prx|\prz$, assume only 
\begin{equation}
\text{the conditional mean } \mathbb E[\prx|\prz] \text{ and variance } \text{Var}[\prx|\prz] \text{ are known}.
\label{mx-assumption}
\end{equation}
We call this the \textit{MX(2) assumption}. In this section, we show that asymptotic Type-I error can be uniformly controlled under this drastically weaker assumption (together with moment assumptions). We work in an asymptotic regime described by Setting~\ref{setting:asymptotic} below.

\begin{setting}[\bf Arbitrary dimension asymptotics] \label{setting:asymptotic}

For each $n = 1, 2, \dots$, we have a joint law $\mathcal L_n$ over $(\prx, \pry, \prz) \in \mathbb R^{d + r + p}$, where $d = \text{dim}(\prx)$ remains fixed, $r = \text{dim}(\pry) = 1$, and $p = \text{dim}(\prz)$ can vary arbitrarily with $n$. Under the MX(2) assumption, the law $\mathcal L_n$ has known mean and variance functions
\begin{equation}
	\mu_n(\prz) \equiv \mathbb E_{\mathcal L_n}[\prx|\prz]  \text{ and } \Sigma_n(\prz) \equiv \text{Var}_{\mathcal L_n}[\prx|\prz].
	\label{conditional-mean-variance}
\end{equation}
For each $n$, we receive $n$ i.i.d. samples $(\srx, \sry, \srz) = \{(\srx_i, \sry_i, \srz_i)\}_{i = 1}^n$ from $\mathcal L_n$. Note that we leave implicit the dependence on $n$ of $(\prx,\pry,\prz)$ and $(X,Y,Z)$ to lighten the notation. 

\end{setting}

By analogy with definition~\eqref{eq:null-under-modelX}, the MX(2) null hypothesis is defined as 
\begin{equation}
	\mathscr L_0^{\text{MX(2)}} = \mathscr L_0^{\text{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \equiv \mathscr L_{0} \cap \mathscr L^{\text{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)),
	\label{mx2-null}
\end{equation}
where
\begin{equation*}
	\begin{split}
		\mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \equiv \{\mathcal L_n: \mathbb E_{\mathcal L_n}[\prx|\prz] = \mu_n(\prz),\ \text{Var}_{\mathcal L_n}[\prx|\prz] = \Sigma_n(\prz)\}.
	\end{split}
\end{equation*}
In Section~\ref{sec:mx2-f-test}, we propose a test of this MX(2) null hypothesis called the \textit{MX(2) $F$-test}. We establish in Section~\ref{sec:MX(2)-regression} that this test controls Type-I error uniformly over subsets of $\mathscr L_0^{\text{MX(2)}}$ satisfying moment conditions and that this test is asymptotically equivalent to the CRT based on the same test statistic (proofs deferred to Appendix~\ref{sec:proofs-sec45}). We compare these results to existing ones in Section~\ref{sec:comparison-to-existing-results-3}, and then check their finite-sample accuracy via numerical simulations in Section~\ref{sec:simulations}.


\subsection{The MX(2) $F$-test} \label{sec:mx2-f-test}

Suppose we have trained an estimate $\widehat g_n$ of $\mathbb E_{\mathcal L_n}[\pry|\prz]$ on an independent dataset (whose size can vary arbitrarily with $n$ and is not included in the sample size $n$ used for testing). In the next section, $g_n$ will denote be the nonparametric portion of a semiparametric model~\eqref{eq:linearity}. \textit{These training sets across $n$ and resulting estimates $\widehat g_n$ remain fixed throughout.} Importantly, the sample used for training $\widehat g_n$ need not come from the same distribution as $\mathcal L_n$ and can therefore be much larger. For example, in the context of the digital twin test \cite{Bates2020} it was pointed out that large observational datasets can be used to train $\widehat f_{\pry|\prx,\prz}$, while applying a variant of the CRT to a smaller experimental dataset to obtain a causal guarantee. 

With the estimate $\widehat g_n$ in hand, it is natural to base inference on the sample covariance between $\prx$ and $\pry$ after adjusting for $\prz$:
\begin{equation}
\widehat \rho_n \equiv \frac1n\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))(\srx_{i} - \mu_n(\srz_i)).
\label{rho-hat}
\end{equation}
In general, $\widehat \rho_n \in \mathbb R^d$, but for $d = 1$, this coincides with the \textit{generalized covariance measure}, proposed by Shah and Peters \cite{Shah2018} for conditional independence testing. Related quantities also have been studied in the semiparametric \cite{Robinson1988, Robins1992} and doubly robust \cite{Robins2001, VanderLaan2003, Chernozhukov2018} estimation contexts; see Section~\ref{sec:comparison-to-existing-results-3} for more discussion. Constructing an asymptotically valid CI test based on $\widehat \rho_n$ requires us to be able to consistently estimate the limiting mean and variance of this quantity under the null. If we have access to the first two moments of $\prx|\prz$, we can compute for any $\mathcal L_n \in \mathscr L_0^{\text{MX(2)}}$ that
\begin{equation}
\begin{split}
\text{Var}_{\mathcal L_n}[\sqrt{n}\widehat \rho_n] &=\text{Var}_{\mathcal L_n}[(\pry - \widehat g_n(\prz))(\prx - \mu_n(\prz))] \\
&=\text{Var}_{\mathcal L_n}\left[\mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))(\prx - \mu_n(\prz))|\pry,\prz]\right] + \\
&\quad \quad \mathbb E_{\mathcal L_n}\left[\text{Var}_{\mathcal L_n}[(\pry - \widehat g_n(\prz))(\prx - \mu_n(\prz))|\pry,\prz]\right] \\
&\quad = \mathbb E_{\mathcal L_n}\left[(\pry - \widehat g_n(\prz))^2 \Sigma_n(\prz)\right] \\
&\quad\equiv S^2_{n} \in \mathbb R^{d \times d}.
\label{variance-calculation}
\end{split}
\end{equation}
A natural estimate of this limiting variance is
\begin{equation}
\widehat S_n^2 \equiv \frac{1}{n}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))^2\Sigma_n(\srz_i).
\end{equation}
Note that we can compute both $\widehat S_n^2$ and $\widehat \rho_n$ using only the MX(2) assumption. 

Based on the variance calculation~\eqref{variance-calculation}, we would expect the standardized quantity 
\begin{equation}
	U_n(\srx, \sry, \srz) \equiv \widehat S_n^{-1} \sqrt{n} \widehat \rho_n = \frac{\widehat S_n^{-1}}{\sqrt{n}}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))(\srx_{i} - \mu_n(\srz_i)) \in \mathbb R^d.
	\label{u-hat}
\end{equation}
to converge to $N(0,I_d)$ under the MX(2) null~\eqref{mx2-null}. This motivates the \textit{MX(2) $F$-test} 
\begin{equation*}
\phi^{\text{MX(2)}}_n(\srx, \sry, \srz) \equiv \mathbbm 1(T_n(\srx, \sry, \srz) > c_{d,1-\alpha}),
\end{equation*}
where
\begin{equation}
T_n(\srx, \sry, \srz) \equiv \|U_n(\srx, \sry, \srz)\|^2.
\label{t-n}
\end{equation}
See also Algorithm~\ref{alg:MX(2)-F-test}, and recall that $c_{d,1-\alpha}$ is defined as the $1-\alpha$ quantile of $\chi^2_d$. 
\begin{center}
	\begin{minipage}{\linewidth}
		\begin{algorithm}[H]
			%			\SetAlgorithmName{Procedure}{}\; %last arg is the title of listing table		
			\KwData{
				$\{(\srx_i, \sry_i, \srz_i)\}_{i=1}^n$, $\mu_n(\cdot)$ and $\Sigma_n(\cdot)$ in \eqref{conditional-mean-variance}, learning method $g$
			}
			Obtain $\widehat g$ by fitting $g$ on  a separate dataset\;
			Recall $\mu_n(\srz_i) \equiv \mathbb E_{\mathcal L_n}[\srx_i|\srz_i], \text{ set } \widehat S_n^2 \equiv \frac{1}{n}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))^2\Sigma_n(\srz_i)$\;
			Set $\smash{U_n \equiv \frac{\widehat S_n^{-1}}{\sqrt{n}}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))(\srx_{i} - \mu_n(\srz_i))}$ and $T_n = \|U_n\|^2$\;
			% Compute $T(\srx,\sry,\srz) \equiv \sum_{i = 1}^n \ell(\sry_i, (\srx_i-\mu_n(\srz_i)) \widehat \beta + \widehat g_n(\srz_i))$\;
			% \For{$b \in \{1,\dots,B\}$}{
			% 	Sample $\srxk_i^b$ from $\mathcal L(\srx_i|\srz_i)$ for all $i$\;
			% 	Compute $T(\srxk^b,\sry,\srz)$ using steps 3, 4 with $\srxk_i^b$ replacing $\srx_i$\;
			% }
			% Compute $\widehat p_j$ based on formula~\eqref{CRT}\;
			\KwResult{MX(2) $F$-test asymptotic $p$-value $\widehat p \equiv \mathbb P[\chi^2_d > T_n]$.}
			\textbf{Cost:} One $p$-dimensional model fit.
			\caption{\bf The MX(2) $\bm F$-test}
			\label{alg:MX(2)-F-test}
		\end{algorithm}
	\end{minipage}
\end{center}
Note that a one-sided version of this test (the \textit{MX(2) $t$-test}) can be defined for $d = 1$ by rejecting for large values of $U_n(\srx, \sry, \srz)$.

Next, we formally state asymptotic type-I error control for the MX(2) $F$-test and claim that it is asymptotically equivalent to the CRT based on the same test statistic. 

\subsection{Asymptotic type-I error control and equivalence to CRT} \label{sec:MX(2)-regression}

For uniform type-I error control, we must restrict the set of null distributions~\eqref{mx2-null} to those satisfying the following moment conditions for fixed $c_1, c_2 > 0$:
\begin{equation}
	\mathscr L_n(c_1, c_2) \equiv \{\mathcal L_n: \|S_{n}^{-1}\| \leq c_1, \mathbb E_{\mathcal L_n}\left[(\pry - \widehat g_n(\prz))^{4} \mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{4}|\prz]\right] \leq c_2 \}.
\end{equation}

\begin{theorem} \label{thm:asymptotic-alpha-level}
Under Setting~\ref{setting:asymptotic}, if $\mathcal L_n \in \mathscr L_0^{\textnormal{MX(2)}}\cap \mathscr L_n(c_1, c_2)$ for some $c_1, c_2 > 0$, then the standardized generalized covariance measure statistic $U_n(\srx, \sry, \srz)$ converges to the standard normal: 
	\begin{equation}
	U_n(\srx, \sry, \srz) \overset{\mathcal L_n}\rightarrow_d N(0, I_d).
	\label{eq:asymptotic-normality}
	\end{equation}
Therefore, the MX(2) $F$-test controls Type-I error asympotically, uniformly over the above subset of $\mathscr L_0^{\textnormal{MX(2)}}$: 
	\begin{equation}
	\limsup_{n \rightarrow \infty} \sup_{\mathcal L_n \in \mathscr L_0^{\textnormal{MX(2)}}\cap \mathscr L_n(c_1, c_2)} \mathbb E_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz)] \leq \alpha.
	\label{asymptotic-alpha-level}
	\end{equation}
\end{theorem}

We pause to comment on Theorem~\ref{thm:asymptotic-alpha-level}. It implies that much less than the MX assumption is needed if one is satisfied with asymptotic Type-I error control. Obtaining the first two moments of $\prx|\prz$ is of course much easier than obtaining this entire conditional distribution (unless $\prx$ is binary), so the MX(2) assumption is likely to be much easier to satisfy in practice. Furthermore, the MX(2) $F$-test has the computational advantage of not requiring resampling to compute its critical values, which are given explicitly. In fact, \textit{any} method not requiring the full MX assumption must bypass resampling, since just the ability to resample from $\prx|\prz$ requires the MX assumption. 

While the MX(2) $F$-test is quite different from usual MX methods on its surface, the next theorem states that it is asymptotically equivalent to the CRT based on the same test statistic.
\begin{theorem} \label{thm:equivalence}
Under Setting~\ref{setting:asymptotic}, suppose
	\begin{equation}
		\mathcal L_n \in \mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \cap \mathscr L_n(c_1,c_2)
	\end{equation}
	for some $c_1, c_2 > 0$, and define $T_n(\srx, \sry, \srz)$ via equations~\eqref{u-hat} and~\eqref{t-n}. Let $\phi_n^{\textnormal{CRT}}$ denote the CRT based on $T_n$, with threshold $C_n(\sry, \srz)$ defined as in equation~\eqref{upper-quantile}. The CRT threshold converges in probability to the MX(2) threshold:
	\begin{equation}
		C_n(Y,Z) \overset{\mathcal L_n}\rightarrow_p c_{d,1-\alpha}.
		\label{eq:threshold-convergence}
	\end{equation}
Furthermore, if $T_n(\srx, \sry, \srz)$ does not accumulate near $c_{d,1-\alpha}$, i.e.
		\begin{equation}
		\lim_{\delta \rightarrow 0}\limsup_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[|T_n(\srx, \sry, \srz)-c_{d,1-\alpha}| \leq \delta] = 0,
		\label{eq:non-accumulation}
		\end{equation}
	 	then the CRT is asymptotically equivalent to the MX(2) F-test:
		\begin{equation}
			\lim_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz) \neq \phi^{\textnormal{CRT}}_n(\srx, \sry, \srz)] = 0.
			\label{eq:asymptotic-equivalence}
		\end{equation}	
\end{theorem}

Informally, this theorem suggests that the null distribution of $U_n(\srx, \sry, \srz)$ converges to $N(0, I_d)$, even after conditioning on $\sry, \srz$. In the language of the CRT, this means that the resampling distribution of the test statistic $T_n(\srx, \sry, \srz)$ approaches $\chi^2_d$. This is not too surprising in retrospect, since $\text{Var}_{\mathcal L_n}[\sqrt n \widehat \rho_n | \sry, \srz] = \widehat S_n^2$.  Note that this conditional normalization property holds for the specific instance of the CRT based on the statistic $T_n$ defined in via equations~\eqref{u-hat} and~\eqref{t-n}, though other kinds of test statistics may lead to similar large-sample behavior. We can also view Theorem~\ref{thm:equivalence} as a robustness result. Indeed, this theorem implies that the large-sample behavior of the CRT based on $T_n$ depends only on the first two moments of $\prx|\prz$. Therefore, the CRT carried out using a misspecified distribution $\prx|\prz$ that is correct up to at least the first two moments will be asymptotically equivalent to the CRT based on the correct distribution and therefore, will control Type-I error asymptotically as well.

\subsection{Comparison to existing results} \label{sec:comparison-to-existing-results-3}

Theorem~\ref{thm:asymptotic-alpha-level} is reminiscent of results in the literature on semi- and non-parametric inference \cite{Robinson1988,Robins1992,VanderLaan2011, Chernozhukov2018, Shah2018}. There, the goal is to construct asymptotically valid confidence intervals and tests for functionals like $\rho_n \equiv \mathbb E_{\mathcal L_n}[\text{Cov}_{\mathcal L_n}[\prx, \pry|\prz]]$. The connection with conditional independence testing is that $\rho_n = 0$ if $\prx \independent \pry \mid \prz$, and the converse is true under certain semi-parametric models. Inference is usually based on some version of the functional $\widehat \rho_n$ defined in equation~\eqref{rho-hat}, with $\mu_n(\prz) = \mathbb E_{\mathcal L_n}[\prx|\prz]$ estimated alongside $g_n(\prz) \equiv \mathbb E_{\mathcal L_n}[\pry|\prz]$. Asymptotically valid inference is obtained under variants of the ``doubly robust" assumption that the estimates for $\mathbb E_{\mathcal L_n}[\prx|\prz]$ and $\mathbb E_{\mathcal L_n}[\pry|\prz]$ are both consistent, with the product of the estimation errors tending to zero at a rate of $o(n^{-1/2})$. By contrast, the MX(2) $F$-test places more weight on the model for $\prx|\prz$ (assuming both first and second moments of this conditional distribution are known) while placing less weight on the model for $\pry|\prz$ (not assuming a semi-parametric form for $\pry|\prz$ or even consistency for $\mathbb E_{\mathcal L_n}[\pry|\prz]$). The estimate $\widehat S_n^2$ we employ for the variance of $\widehat \rho_n$, when compared to those typically used in the semi-parametric literature, reflects this differing set of assumptions.

Theorem~\ref{thm:equivalence} is a statement about the asymptotic equivalence between the resampling-based CRT and the asymptotic MX(2) $F$-test. The CRT is in the spirit of the finite-population approach to causal inference (Fisher), whereas the MX(2) $F$-test is in the spirit of the asymptotic super-population approach (Neyman). We find that research in these two strands of work on causal inference have proceeded largely separately from each other, and therefore connections between the two have received relatively little attention. However, there has been a recent line of work \cite{Ding2017,Wu2020a,Zhao2021} focusing on the asymptotic behavior of the Fisher randomization test in the context of completely randomized experiments. A similar result to Theorem~\ref{thm:equivalence} is that the Fisher randomization test (analogous to the CRT) is asymptotically equivalent to the Rao score test (analogous to the MX(2) $F$-test) in a completely randomized experiment \cite[Theorem A.1]{Ding2017}. Theorem~\ref{thm:equivalence} can be viewed as an extension of this result to accommodate for non-binary treatments as well as high-dimensional covariates affecting both treatment and response.

% We defer the proofs of Theorems~\ref{thm:asymptotic-alpha-level} and~\ref{thm:equivalence} to the supplement (Section~\ref{sec:proofs-sec45}). Next, we assess these convergence statements in finite samples via numerical simulations.

\subsection{Finite-sample convergence assessment} \label{sec:simulations}

Theorem~\ref{thm:asymptotic-alpha-level} states that $U_n(\srx, \sry, \srz)$ converges to $N(0, I_d)$ unconditionally, while Theorem~\ref{thm:equivalence} is related to the convergence of $U_n(\srx, \sry, \srz)$ to $N(0, I_d)$ conditionally on $\sry, \srz$. In this section, we describe a numerical simulation designed to assess both convergence statements in finite samples. Code to reproduce the simulation is available online at \url{https://github.com/ekatsevi/crtpower-manuscript}.

\paragraph{Simulation setup.}

Note that, if $(\prx,\prz)$ is jointly Gaussian, then $U_n(\srx, \sry, \srz)$ is exactly distributed as $N(0,I_d)$ both unconditionally and conditionally in finite samples; see also Section 2.5 of \cite{Liu2020}. To test the above convergence statements in a nontrivial setting, we instead consider a discrete distribution for $(\prx,\prz)$. In particular, we sample $(\prx, \prz)$ from a Markov chain, as described next.

Let's assume for simplicity that $\text{dim}(\prx) = 1$. Define $(\prx,\prz) \in \{0,1\}^{1+p}$ to have the distribution of a Markov chain with 
\begin{equation*}
	\text{initial state } \prx \sim \text{Ber}(\pi_{\text{init}}) \text{ and transition matrix } \begin{pmatrix}1-\pi_\text{flip} & \pi_{\text{flip}} \\  \pi_{\text{flip}} &  1-\pi_{\text{flip}}\end{pmatrix}.
\end{equation*}
More explicitly, we have
\begin{equation*}
	\mathbb P[\prx = x, \prz = z] = \pi_{\text{init}}^{x}(1-\pi_{\text{init}})^{1-x} \pi_{\text{flip}}^{\mathbbm 1(z_1 \neq x)} (1-\pi_{\text{flip}})^{\mathbbm 1(z_1 = x)} \prod_{j = 2}^p \pi_{\text{flip}}^{\mathbbm 1(z_j \neq z_{j-1})} (1-\pi_{\text{flip}})^{\mathbbm 1(z_j = z_{j-1})}
\end{equation*}
The parameters $(\pi_{\text{init}}, \pi_{\text{flip}})$ describe the distribution of $\prx|\prz$ and are assumed known. Furthermore, let the response $\pry$ be distributed as a random effects model in $\prz$:
\begin{equation*}
	\pry = \prz^T \bm \gamma + \peps, \quad \bm \gamma \sim N(0, \sigma^2_{\gamma}I_p),\ \peps \sim N(0, \sigma^2_\eps I_n).
\end{equation*}
Thus, all simulations are conducted under the null hypothesis $H_0: \prx \independent \pry \mid \prz$. The signal-to-noise ratio in this relationship is defined via
\begin{equation*}
	\text{SNR} = \frac{\mathbb E[\|\prz\|^2]\sigma^2_\gamma}{\sigma^2_\eps}.
\end{equation*}
The function $\widehat g_n$ is defined by running a 10-fold cross-validated ridge regression of $\sry$ on $\srz$ on $n_{\text{train}}$ training samples, and then the statistic $U_n(\srx, \sry, \srz)$ is evaluated based on $n_{\text{test}}$ independent test samples.

\paragraph{Simulation parameters.}

All simulations were run with 
\begin{equation}
	n_{\text{train}} = 100; \quad \pi_{\text{init}} = 0.1; \quad \pi_{\text{flip}} = 0.1; \quad \sigma^2_\eps = 1. 
	\label{fixed-parameters}
\end{equation}
On the other hand, the three parameters $(n_{\text{test}}, \text{SNR}, p)$ were varied as follows:
\begin{equation*}
	n_{\text{test}} \in \{10, 25, \textbf{100}\}; \quad  \text{SNR} \in \{0, \textbf{1}, 5\}; \quad p \in \{20, 100, \textbf{500}\}.
\end{equation*}
The bolded values above represent the \textit{default values} for each parameter. Each of the three parameters was varied while keeping the other two parameters at their default values, giving a total of nine simulation settings. 

For each simulation setting, the training data and the estimate $\widehat g_n$ were generated just once, since our results condition on the training data. The entire test data $(\srx, \sry, \srz)$  were sampled 1000 times to generate the unconditional distribution of $U_n(\srx, \sry, \srz)$. To generate the conditional distribution of this quantity, the data $(\sry, \srz)$ were sampled once per problem setting, and then $\srx|\srz$ was sampled 1000 times. 

\paragraph{Simulation results.}

For each of the nine simulation settings, normal QQ plots of the unconditionally-generated $U_n(\srx, \sry, \srz)$ are shown in Figure~\ref{fig:unconditional} while their conditional counterparts are shown in Figure~\ref{fig:conditional}. Based on these results, we make the following observations. The sample size impacts calibration, but the SNR and the dimension do not. In particular, unconditional and conditional normality are already achieved at $n = 100$, regardless of SNR and dimension. Unconditional convergence to normality tends to be faster than conditional convergence, with the unconditional distribution already normal for $n = 25$ while the conditional distribution has not yet converged for this value of $n$. Our main conclusion is that for even moderate sample sizes, the MX(2) $F$-test controls Type-I error and behaves quite similarly to the CRT. We must bear in mind, however, that different choices of the fixed parameters~\eqref{fixed-parameters} may alter these conclusions. In particular, smaller $\pi_{\text{init}}$ leads to more discreteness in $\srx$ and therefore slower convergence to normality.

\paragraph{}
Next, we study the asymptotic power of the MX(2) $F$-test (and by Theorem~\ref{thm:equivalence}, of the CRT) against semiparametric alternatives. 

\clearpage

\begin{figure}[h!]
	\includegraphics[width = \textwidth]{unconditional.png}
	\caption{Unconditional distribution of $U_n(\srx, \sry, \srz)$ across the nine simulation settings.}
	\label{fig:unconditional}
\end{figure}

\begin{figure}[h!]
	\includegraphics[width = \textwidth]{conditional.png}
	\caption{Conditional distribution of $U_n(\srx, \sry, \srz)|\sry,\srz$ across the nine simulation settings.}
	\label{fig:conditional}
\end{figure}

\clearpage

\section{The asymptotic power of the CRT} \label{sec:asymptotic-power}

In Section~\ref{sec:power}, we saw how to construct the optimal test against point alternatives specified by $\bar f_{\pry|\prx,\prz}$. In practice, of course we do not have access to this distribution, so we usually estimate it via a statistical machine learning procedure. The goal of this section is to quantitatively assess the power of the CRT as a function of the prediction error of this ML procedure. 

\subsection{Power against semiparametric alternatives} \label{sec:power-results}

In this section, we consider semiparametric alternatives as described next.

\begin{setting}[\bf Semiparametric alternatives] \label{setting:semiparametric}
Under Setting~\ref{setting:asymptotic}, assume $\mathcal L_n(\pry|\prx,\prz)$ is such that
\begin{equation}
	\pry = (\prx-\mu_n(\prz))^T \beta_n + g_n(\prz) + \peps; \quad \peps \sim N(0, \sigma^2 ),\ \sigma^2 > 0
	\label{eq:linearity}
\end{equation}
for $\peps \independent (\prx, \pry, \prz)$. Here, $\beta_n \in \mathbb R^d$ is a coefficient vector, $g_n: \mathbb R^p \rightarrow \mathbb R$ a general function, and $\sigma^2 > 0$ the residual variance. 
\end{setting}

Note that the function $\widehat g_n$ from the previous section can be viewed as an approximation to $g_n(\prz)$. The semiparametric model~\eqref{eq:linearity} has been extensively studied (see e.g. the classic works \cite{Robinson1988, Robins1992}), but not in the context of MX methods and mostly focusing on the estimation problem. 

In Theorem~\ref{thm:power} below, we express the asymptotic power of the MX(2) $F$-test against alternatives~\eqref{eq:linearity} in terms of the variance-weighted mean square  error of $\widehat g_n$:
\begin{equation} 
	\mathcal E^2_n \equiv  \mathbb E_{\mathcal L_n}\left[(\widehat g_n(\prz)-g_n(\prz))^2 \cdot \overline \Sigma_n^{-1/2}\Sigma_n(\prz)\overline \Sigma_n^{-1/2}\right], \quad \text{where} \ \overline \Sigma_n\equiv \mathbb E_{\mathcal L_n}[\Sigma_n(\prz)].
\end{equation}
Note that if $(\prx, \prz)$ is jointly Gaussian, then $\Sigma_n(\prz) = \overline \Sigma_n$ for all $\prz$ and therefore $\mathcal E_n^2 = \mathbb E_{\mathcal L_n}[(\widehat g_n(\prz)-g_n(\prz))^2] \cdot I_d$. Our result requires the following moment assumptions:
	\begin{equation}
\sup_{n} \|\overline \Sigma_n^{-1}\| < \infty,
\label{eq:s-n-inverse-assump}
\end{equation}
\begin{equation}
	\sup_n\ \mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^8] < \infty,
	\label{eq:eighth-moment-assump-1}
\end{equation}
and
\begin{equation}
	\sup_n\ \mathbb E_{\mathcal L_n}[(\widehat g_n(\prz)-g_n(\prz))^4\|\prx - \mu_n(\prz)\|^4] < \infty.
	\label{eq:eighth-moment-assump-2}
\end{equation}

\begin{theorem}  \label{thm:power}
Consider semiparametric alternative Setting~\ref{setting:semiparametric}. Suppose $\mathcal L_n$ satisfies the moment conditions~\eqref{eq:s-n-inverse-assump}, \eqref{eq:eighth-moment-assump-1}, and \eqref{eq:eighth-moment-assump-2}, and that the conditional variance and variance-weighted mean squared error converge:
\begin{equation}
	\overline \Sigma_n \rightarrow \overline{\Sigma} \quad \text{and} \quad \mathcal E_n^2 \rightarrow \mathcal E^2 \quad \text{as } n \rightarrow \infty,
	\label{eq:limits}
\end{equation}
Then, we have the following two statements:
\begin{enumerate}
\item[(a)] (Consistency) If $\beta_n = \beta \neq 0$ for each $n$, then the MX(2) $F$-test and the CRT based on the same statistic are consistent:
\begin{equation}
	\begin{split}
		\lim_{n \rightarrow \infty} \mathbb E_{\mathcal L_n}\left[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz)\right] &= \lim_{n \rightarrow \infty} \mathbb E_{\mathcal L_n}\left[\phi^{\textnormal{CRT}}_n(\srx, \sry, \srz)\right] = 1.
		\label{eq:consistency}
	\end{split}
\end{equation}
\item[(b)] (Power against local alternatives) If $\beta_n = h_n/\sqrt{n}$ for a convergent sequence ${h_n \rightarrow h \in \mathbb R^d}$, then
\begin{equation}
	\begin{split}
		\lim_{n \rightarrow \infty} \mathbb E_{\mathcal L_n}\left[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz)\right] &= \lim_{n \rightarrow \infty} \mathbb E_{\mathcal L_n}\left[\phi^{\textnormal{CRT}}_n(\srx, \sry, \srz)\right] \\
		&= \mathbb P[\chi^2_d(\|(\sigma^2I_d +\mathcal E^2)^{-1/2}\overline \Sigma^{1/2} h\|^2) > c_{d,1-\alpha}].
		\label{eq:main-conclusion}
	\end{split}
\end{equation}
\end{enumerate}
\end{theorem}

Recalling that $\chi^2_d(\lambda)$ denotes the noncentral chi-square distribution with $d$ degrees of freedom and non-centrality parameter $\lambda$, the second part of Theorem~\ref{thm:power} states that the MX(2) $F$-test and the CRT based on the same test statistic have power equal to that of a $\chi^2$ test of a multivariate normal random vector having mean zero under the alternative $N((\sigma^2I_d +\mathcal E^2)^{-1/2}\overline \Sigma^{1/2} h, I_d)$. This result establishes a direct link between the estimation error in $\widehat g_n$ and the power of the CRT against local alternatives. In particular, the mean-squared error term $\mathcal E^2$ contributes additively to the irreducible error term $\sigma^2 I_d$. We can gain intuition for this result by considering the regression model
\begin{equation}
\begin{split}
\pry - \widehat g_n(\prz) &= (\prx - \mu_n(\prz))^T\beta_n + (g_n(\prz) - \widehat g_n(\prz) + \bm \eps)
\label{eq:regression-model}
\end{split}
\end{equation}
obtained from the semiparametric model~\eqref{eq:linearity} by subtracting $\widehat g_n(\prz)$ from both sides. The test statistic $T_n$ is based on the quantity $\widehat \rho_n$ defined in equation~\eqref{rho-hat}, which can be viewed as an unnormalized version of the fitted regression coefficients of $\sry - \widehat g_n(\srz)$ on $\srx - \mu_n(\srz)$. The term $g_n(\prz) - \widehat g_n(\prz)$ in the regression model~\eqref{eq:regression-model} contributes additively to the residual error term, so in a traditional regression analysis we would expect the power of the test to depend on the variance of this error term. In fact, standard large-sample OLS theory (see e.g. Section 2.3 of Hayashi's book \cite{Hayashi2000}) states that the power against local alternatives of the $F$-test in the regression model~\eqref{eq:regression-model} is exactly the same as that of the MX(2) $F$-test stated in equation~\eqref{eq:main-conclusion}. Of course, the usual $F$-test applied to the regression~\eqref{eq:regression-model} relies on the validity of this model while the MX(2) $F$-test instead relies on knowledge of $\text{Var}[\prx|\prz]$. Note that \cite{Wang2020b} also find the power of an MX test and a classical OLS test to have the same power (see their Appendix F).

\subsection{Example: Power of lasso-based CRT}

A key ingredient in the power formula~\eqref{eq:main-conclusion} is the limiting variance-weighted mean squared error $\mathcal E^2$. This error depends on the machine learning method used to obtain $\widehat g_n$. We can leverage existing results about the asymptotic behavior of prediction error of machine learning methods in high dimensions. In this section, we consider the case when $\widehat g_n$ is trained using the lasso in the orthogonal design case, which was studied by Bayati and Montanari \cite{Bayati2011}.

\begin{setting}[\bf Linear regression with orthogonal design] \label{setting:orthogonal-design}
Under Setting~\ref{setting:semiparametric}, assume further that $(\prx, \prz) \sim N(0, I_{1+p})$, $\beta_n = h_n/\sqrt{n}$ for some convergent sequence $h_n \rightarrow h \in \mathbb R$, and $g_n(\prz) = \prz^T \gamma_n$. Suppose $\gamma_n \in \mathbb R^p$ is such that the entries of $\sqrt n \gamma_n$ converge weakly to a random variable $\Gamma$ on $\mathbb R$ such that $\mathbb P[\Gamma \neq 0] > 0$ and $\|\sqrt n \gamma_n\|^2/p \rightarrow \mathbb E[\Gamma^2] < \infty$.
\end{setting}

Until now, we have denoted by $n$ the sample size used for constructing tests, leaving unspecified the size of the separate sample used to train $\widehat g_n$. To get concrete expressions for the power of the MX(2) $F$-test and the CRT based on a specific machine learning method to obtain $\widehat g_n$, we must take the training sample size into account. We therefore define tests $\varphi_n^{\text{MX(2)}}(\srx, \sry, \srz)$ and $\varphi_n^{\text{CRT}}(\srx, \sry, \srz)$, which for some training proportion $\pi \in (0,1)$ split the data into $\pi n$ training observations $(\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}})$ and $(1-\pi)n$ test observations $(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})$. These tests both proceed by first running a lasso of $\sry_{\text{train}}$ on $\srz_{\text{train}}$ with regularization parameter $\lambda$ to obtain an estimate $\widehat \gamma_{\pi n}$. The tests $\varphi_n^{\text{MX(2)}}(\srx, \sry, \srz)$ and $\varphi_n^{\text{CRT}}(\srx, \sry, \srz)$ are then obtained by running the MX(2) $F$-test and the CRT on the test data $(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})$ using the estimate $\widehat g_n(\prz) = \prz^T \widehat \gamma_{\pi n}$:
\begin{equation*}
	\varphi_n^{\text{MX(2)}}(\srx, \sry, \srz) \equiv \phi_{(1-\pi)n}^{\text{MX(2)}}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}}) \ \ \text{and} \ \ \varphi_n^{\text{CRT}}(\srx, \sry, \srz) \equiv \phi_{(1-\pi)n}^{\text{CRT}}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}}).
\end{equation*}
Note that the dependence of $\phi_{(1-\pi)n}^{\text{MX(2)}}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})$ and $\phi_{(1-\pi)n}^{\text{CRT}}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})$ on the training data $(\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}})$ is left implicit. 

Under Setting~\ref{setting:orthogonal-design}, we can directly use Bayati and Montanari's theory \cite{Bayati2011} to obtain 
\begin{equation}
\lim_{n \rightarrow \infty}\mathcal E_n^2 = \tau_*^2 - \sigma^2 \quad \text{a.s. in } (\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}),
\label{eq:bm-result}
\end{equation}
where $(\alpha_*,\tau_*)$ is the unique solution of the system below:
\begin{equation}
	\begin{split}
		\lambda &= \alpha \tau (1-(\pi\delta)^{-1}\mathbb E[\eta'(\sqrt \pi \Gamma + \tau W; \alpha \tau)]) \\
		\tau^2 &= \sigma^2 + (\pi\delta)^{-1}\mathbb E[(\eta(\sqrt \pi\Gamma + \tau W; \alpha \tau) - \sqrt \pi\Gamma)^2].
	\end{split}
	\label{eq:amp-system}
\end{equation}
Here, $W \sim N(0,1)$ is independent of $\Gamma$ and $\eta(x; \theta) = (|x|-\theta)_+\textnormal{sign}(x)$ is the soft threshold function. This leads to the following corollary of Theorem~\ref{thm:power}, proved in Appendix~\ref{sec:proofs-sec45}:
\begin{corollary} \label{cor:lasso}
Under Setting~\ref{setting:orthogonal-design}, the asymptotic power of $\varphi^{\textnormal{CRT}}_{n}$ and $\varphi^{\textnormal{MX(2)}}_{n}$ converges to that of a standard normal location test with alternative mean $\tau_*^{-1} h\sqrt{1-\pi}$:
\begin{equation}
	\begin{split}
	\lim_{n \rightarrow \infty}\mathbb E_{\mathcal L_n}[\varphi^{\textnormal{CRT}}_{n}(\srx, \sry, \srz)] &= \lim_{n \rightarrow \infty}\mathbb E_{\mathcal L_n}[\varphi^{\textnormal{MX(2)}}_{n}(\srx, \sry, \srz)] \\
	&=  \mathbb P[|N(\tau_*^{-1} h\sqrt{1-\pi},1)| > z_{1-\alpha/2}].
	\label{eq:lasso-power}
	\end{split}
\end{equation}
\end{corollary}
Corollary~\ref{cor:lasso} gives the power of these lasso-based methods in a very simple form, with the prediction error of the lasso entering through the effective noise level $\tau_*$. The impact of the splitting proportion $\pi$ on power can be seen in the multiplication of the signal strength $h$ by $\sqrt{1-\pi}$. The splitting proportion implicitly impacts the effective noise level $\tau_*$ as well; smaller $\pi$ lead to greater effective noise levels. Note that the expectations in Corollary~\ref{cor:lasso} are over both training and test sets, while the expectations in Theorem~\ref{thm:power} are over the test set only. 

\subsection{Comparison to existing results} \label{sec:comparison-to-existing-results-4}

Two other power analyses of the CRT have been recently conducted \cite{Wang2020b, Celentano2020}, focusing on the case where $g_n(\prz) = \prz^T \gamma_n$, $\widehat g_n$ is trained using the lasso, $n/p \rightarrow \delta$, and the generalized covariance measure test statistic $\widehat \rho_n$ is used. The former study considers the case of orthogonal design (Setting~\ref{setting:orthogonal-design}), while the latter considers arbitrary joint Gaussian distribution for $(\prx,\prz)$. Assuming $\mathbb E_{\mathcal L_n}[\text{Var}_{\mathcal L_n}[\prx|\prz]] \rightarrow s^2$ (the quantity we called $\overline \Sigma$ in Section~\ref{sec:power-results}, with different notation to clarify that for $\text{dim}(\prx) = 1$ the covariance matrix simply becomes a variance), the works \cite{Wang2020b, Celentano2020} found that the power of the CRT with in-sample lasso fit tends to that of a normal location test with alternative mean $sh/\tau_*$, where $\tau_*$ is the effective noise level from AMP theory (in the orthogonal design case, $(\alpha_*, \tau_*)$ are defined by equation~\eqref{eq:amp-system} with $\pi = 1$). 

This is a similar expression to what we found in Corollary~\ref{cor:lasso} in the orthogonal design case. Furthermore, note that $\tau_*^2 = \sigma^2 + \mathcal E^2$ (i.e. the out-of-sample prediction error of the lasso). It follows that the power expression found by \cite{Wang2020b, Celentano2020} is exactly the same as what we found in part (b) of Theorem~\ref{thm:power}, despite the fact that $\widehat g_n$ is fit in-sample. The power of CRT with $\widehat g_n$ fit in-sample is a more subtle object to analyze, as it may depend on the degree to which $\widehat g_n$ overfits. \cite{Wang2020b} also derive a power expression for the CRT when $\widehat g_n$ is fit in-sample via ordinary least squares (allowing correlated covariates, as we do in Setting~\ref{setting:semiparametric}), which also happens to coincide with the expression~\eqref{eq:main-conclusion}. Such in-sample results have only been obtained only for these two test statistics, however, though it would be interesting whether expression~\eqref{eq:main-conclusion} holds for more general classes of test statistics. By contrast, training $\widehat g_n$ on a separate sample allows us to prove Theorem~\ref{thm:power} for very broad classes of machine learning methods $\widehat g_n$.

Finally, we note a connection between Theorem~\ref{thm:power} and causal inference. It is widely known in causal inference (see e.g. \cite[Section 7.5]{Imbens2015}) that adjustment for covariates $\srz$ in randomized experiments (a) yields consistent estimates despite misspecification of $\mathcal L(\pry|\prx,\prz)$ and (b) improve estimation efficiency to the extent that this adjustment captures the distribution $\mathcal L(\pry|\prx,\prz)$. This fact mirrors the conclusions of Theorem~\ref{thm:power}. The asymptotic variance of the regression-based estimator for the average treatment effect in a completely randomized experiment is a standard result, but we are unaware of a quantitative expression of the asympotic efficiency of covariate-adjusted versions of the Fisher randomization test (though some insight is provided by \cite{Zhao2021}). 

% \section{Estimating effect sizes and connections to causal inference} \label{sec:causal}

%The majority of the paper so far has dealt primarily with hypothesis testing, because that is what the MX framework was introduced for. The role of the semiparametric model~\eqref{semiparametric-model} was to specify a class of alternatives against which to target and evaluate power. If this model were in fact to be accepted, then producing point estimates and confidence intervals for $\beta$ would be a well-defined problem, which is the subject of an extensive semiparametric estimation literature \cite{Robinson1988, Robins1992}. Another approach to estimating $\beta$ is to invert the test of $\beta = \beta_0$ obtained from applying an MX(2) regression test to the triple $(\srx,\sry - \srx \beta_0, \srz)$. In other words, we test every $\beta_0 \in \mathbb{R}$ and only retain those values for which we failed to reject the null.


%However, the point of the MX framework is to abstain from assuming a model on $\pry | \prx,\prz$ to begin with. It is more in keeping with the MX spirit to define the appropriate targets of interest nonparametrically, and then to carry out inference for these targets. This direction has not been explored in the context of the MX framework, but we are inspired by extensive work in related areas of high-dimensional statistics. 

%In particular, we find strong ties to MX in the field of causal inference. This connection has been noted briefly \cite{CetL16}, and the recently proposed digital twin test \cite{Bates2020} is a clever application of the MX framework to derive causal inferences from genetic trio studies. Nevertheless, many basic connections between the MX framework and causal inference have not received much attention.


%Next, we discuss two nonparametric objects of inference that may be suitable for the MX framework: one is a nonparametric analog of $\beta$, and the other directly inspired by the dose response function from causal inference.
%The theory developed previously allows us to produce confidence intervals for $\beta$ in at least two ways. 
%One approach is to directly 
%Another approach is to invert the test of $\beta = \beta_0$ obtained from applying an MX regression test to the triple $(\srx,\sry - \srx \beta_0, \srz)$. In other words, we apply the MX regression test to every $\beta_0 \in \mathbb{R}$ and only retain those values for which we failed to reject the null. This interval (or set in general) has two properties.
%If $\pry \perp \prx \mid \prz$, this interval will confidently contain $\beta = 0$ by the validity of the CRT. If the CI null is false and the semiparametric model \eqref{parametric-alternative}  is valid for some $\beta^*$, then the interval will contain $\beta^*$, since the semiparametric assumption implies that $\pry - \prx^T \beta^* \independent \prx \mid \prz$ and the CRT will not reject the test $\beta = \beta^*$ with high probability. 



%\subsection{A nonparametric effect size} \label{sec:nonparametric-effect-size}
% To begin discussing the problem of estimating effect sizes, we must first answer the question: what exactly are we estimating?
% We must first define an estimand that does not depend on assuming a model for $\pry | \prx,\prz$.
% Suppose that
% \begin{equation}
% (\srx_i, \sry_i, \srz_i) \overset{\textnormal{i.i.d}} \sim \mathcal L, \quad i = 1, 2,\dots
% \end{equation}
% for some joint law $\mathcal L$. 

%Shah and Peters \cite{Shah2018} observed that if $\mathbb E[\prx|\prz]$ and $\mathbb E[\pry|\prz]$ are estimated well enough, the generalized covariance measure $\widehat \rho_n$~\eqref{rho-hat} is a consistent estimator of the population quantity
%\begin{equation}
%\rho(\mathcal L) \equiv \mathbb E_{\mathcal L}[\text{Cov}_{\mathcal L}[\prx,\pry|\prz]].
%\end{equation}
%A related quantity is the \textit{effect size} of $\prx$ on $\pry$, controlling for $\prz$
%\begin{equation}
%\beta(\mathcal L) \equiv \overline\Sigma^{-1} \rho(\mathcal L) =
%\mathbb E_{\mathcal L}[\text{Var}[\prx|\prz]]^{-1} 
%\mathbb E_{\mathcal L}[\text{Cov}[\prx,\pry|\prz]] .
%% = \Sigma^{-1}\mathbb E_{\mathcal L}[\text{Cov}[\pry, \prx|\prz]].
%\end{equation}
%The functional $\beta(\mathcal L)$ has appeared frequently in related literatures like high-dimensional statistics \cite{Buja2019, Berk2019a} and causal inference \cite{Li2011}. If $\pry = \prx \beta + g(\prz) + \bm\eps$, where $\bm \eps \independent \prx$, then $\beta(\mathcal L) = \beta$, but of course $\beta(\mathcal L)$ is defined for any joint distribution $\mathcal L$. Assuming we have access to $\Sigma_n(\prz)$, a natural estimator for this quantity is
%\begin{equation*}
%\widehat \beta_n \equiv \widehat \Sigma_n^{-1}\widehat \rho_n, \quad \text{where} \quad \widehat \Sigma_n \equiv \frac{1}{n}\sum_{i = 1}^{n} \Sigma_n(\srz_i).
%\end{equation*}
%
%In line with the results of the previous section, under the MX(2) assumption, the estimators $\widehat \beta_n$ and $\widehat \rho_n$ are consistent not only unconditionally, but conditionally as well. This holds as long as we have the following second moment condition:
%\begin{equation}
%\mathbb E_{\mathcal L}\left[(\pry - \widehat g_n(\prz))^{2}\|\prx - \mu_n(\prz)\|^{2}\right] < \infty.
%\label{general-moment-condition}
%\end{equation}
%
%\begin{theorem} \label{prop:nonparametric-consistency}
%	Under the MX(2) assumption~\eqref{MX(2)} and the moment condition~\eqref{general-moment-condition}, $\widehat \rho_n$ and $\widehat \beta_n$ are consistent, both conditionally (almost surely in $\{\sry_{(n)}, \srz_{(n)}\}_{n \geq 1})$: 
%	\begin{equation}
%	\mathcal L(\widehat \rho_n | \sry_{(n)}, \srz_{(n)}) \rightarrow \rho(\mathcal L) \quad \text{and} \quad \mathcal L(\widehat \beta_n | \sry_{(n)}, \srz_{(n)}) \rightarrow \beta(\mathcal L)
%	\label{conditional-consistency}
%	\end{equation}
%	and unconditionally: 
%	\begin{equation}
%	\mathcal L(\widehat \rho_n)  \rightarrow \rho(\mathcal L) \quad \text{and} \quad \mathcal L(\widehat \beta_n)  \rightarrow \beta(\mathcal L).
%	\label{unconditional-consistency}
%	\end{equation}
%\end{theorem}
%We prove this result in the supplement (Section~\ref{sup-sec:proofs-effect-size}). 
%\emph{It may be somewhat surprising that even though the parameters $\rho(\mathcal L)$ and $\beta(\mathcal L)$ are defined by averaging over $\srx,\sry,\srz$, the corresponding estimators are consistent even when conditioning on $\sry,\srz$. We obtain this result as a consequence of the strong law of large numbers over $\sry,\srz$.}

%One may hope to obtain a conditional asymptotic normality result for $\widehat \beta_n$, similarly to Lemma~\ref{lem:uniform-clt} but not just under the null. However, we do not know if such a result is true, and leave this as an open question. Instead, we content ourselves with stating an unconditional asymptotic normality result.
%\begin{proposition} \label{prop:nonparametric-normality}
%Under condition~\eqref{general-moment-condition}, define the covariance~estimate
%\begin{equation}
%\widetilde S_n^2 \equiv \frac1n\sum_{i = 1}^n (D_i - \widehat \beta_n)(D_i - \widehat \beta_n)^T; \quad D_i \equiv \widehat \Sigma_n^{-1}(\srx_i - \mu_n(\srz_i))(\sry_i - \widehat g_n(\srz_i)).
%\end{equation}
%Then, 
%\begin{equation}
%\sqrt n \widetilde S_n^{-1}(\widehat \beta_n - \beta(\mathcal L)) \overset{\mathcal L}\rightarrow N(0,I).
%\label{nonparametric-normality}
%\end{equation}
%\end{proposition}
%The asymptotic normality~\eqref{nonparametric-normality} lets us construct asymptotically valid confidence regions (intervals or ellipses) for $\beta(\mathcal L)$.
%
% The related conditional covariance $\rho(\mathcal L) \equiv \mathbb E_{\mathcal L}[\text{Cov}_{\mathcal L}[\prx,\pry|\prz]]$, is equal to $\beta(\mathcal L)$ except the the factor of $\overline\Sigma^{-1}$, was considered by Shah and Peters \cite{Shah2018}. They estimated this functional using the generalized covariance measure, similar to \eqref{rho-hat}. \textcolor{red}{These authors' Theorem 8 resembles Proposition~\ref{prop:nonparametric-normality}.} On the other hand, we are not aware that the conditional consistency result in Proposition~\ref{prop:nonparametric-consistency} is known.

%Another source of interesting nonparametric estimands is causal inference. We now discuss connections between this field and the MX problem.
% and then present the \textit{dose response function}, a central causal estimand.

% For succinctness of notation, we denote $\bar f^*_{\prx}(x) = \int f^*_{\prx|\prz=z}(x) f^*_{\prz}(z) dz$


% \subsection{A pointwise confidence interval for $\theta(x)$} TBD.


%\subsection{Optimal train/test split for ridge regression HRT} \label{sec:ridge-power}
%
%The formula~\eqref{power-local-alternative} can be used to guide CRT design. To demonstrate this in a simple example, consider thefollowing linear mixed model with orthogonal predictors:
%\begin{equation*}
%\pry = \prx^T \beta + \prz w + \bm \eps; \quad (\prx, \prz, \bm \eps) \sim N(0, I_{1 + p + 1}); \quad w \sim N(0, p^{-1}\tau^2 I_{p}),
%\end{equation*}
%where $\tau^2$ is the signal strength. The random effects part of this model is a special case of the setup considered by Dobriban and Wager \cite{dobriban2018a} in the context of high-dimensional ridge regression. While the CRT is not necessary to carry out inference in linear mixed models, suppose for the sake of illustrate that we applied the CRT variant analyzed in Theorem~\ref{prop:hrt-power} with $\widehat g$ being optimally tuned ridge regression. For some training fraction $\lambda$, suppose we use $\lambda n$ of our samples for training $\widehat g$ and the remaining $(1-\lambda)n$ for testing conditional independence. Dobriban and Wager prove that for $p/n \rightarrow \gamma$, 
%\begin{equation*}
%1 + \mathcal E^n_{\tau, \gamma} \rightarrow R^*(\tau^2, \gamma) \equiv \frac{1}{2}\left(1 + \frac{\gamma - 1}{\gamma}\tau^2 + \sqrt{\left(1 - \frac{\gamma - 1}{\gamma}\tau^2\right)^2 + 4\tau^2}\right). 
%\end{equation*}
%
%While Theorem~\ref{prop:hrt-power} only applies in the case when the dimensions are fixed, let us examine the power against local alternatives given by equation~\eqref{power-local-alternative}, substituting the above expression for the test error term. The training fraction $\lambda$ has the effect of changing the aspect ratio $\gamma$ to $\gamma/\lambda$ and decreasing the local alternative signal strength $h$ to $h\sqrt{1-\lambda}$. With these modifications, the power formula becomes
%\begin{equation*}
%\lim_{n \rightarrow \infty} \mathbb P\left[\left.T_n \geq z_{1-\alpha} \right| \sry_{(n)}, \srz_{(n)}\right] \approx \Phi\left(z_\alpha + \frac{h\sqrt{1-\lambda}}{\sqrt{R^*(\tau^2, \gamma/\lambda)}}\right).
%\end{equation*}
%We emphasize the above approximate equality is a heuristic only and is not a consequence of Theorem~\ref{prop:hrt-power} due to the high dimensionality. Nevertheless, we can examine this power expression and numerically find the optimal training fraction $\lambda$ as a function of the aspect ratio $\gamma$ and the signal strength $\tau^2$.
%
%\begin{figure}[h!]
%	\includegraphics[width = 0.7\textwidth]{figures/optimal_training_fraction.pdf}
%	\caption{Optimal training fraction as a function of signal strength for four aspect ratios in the case when $\widehat g$ is obtained via optimally tuned ridge regression.}
%	\label{fig:optimal-training-fraction}
%\end{figure}
%
%
%Figure~\ref{fig:optimal-training-fraction} depicts these relationships, and several interesting patterns emerge. When the aspect ratio is low ($\gamma = 0.25$), it is optimal to use some data for training even for weak signals. As signal strength increases, the optimal training fraction levels out slightly above 0.5. On the other hand, for higher aspect ratios ($\gamma = 0.5, 0.75$), the ridge regression problem is too challenging for weak signals and there is no hope of achieving any variance reduction for weak signals. Therefore, all data should be used for testing. As the signal strength increases, however, the benefit to training outweighs the cost and it is optimal to use training fractions of up to around 0.8. A phase transition is known to occur at $\gamma = 1$ \cite{dobriban2018a}, and this manifests itself in Figure~\ref{fig:optimal-training-fraction}: for this aspect ratio (and higher) the problem is hard enough that it is better to use a null model and reserve all the data for hypothesis testing purposes.


\section{The most powerful one-bit $p$-values for knockoffs}
\label{sec:knockoffs}

MX knockoffs \cite{CetL16} operate differently than the CRT; they simultaneously test the conditional associations of many variables with a response. Given $m$ variables $\prx_1, \dots, \prx_m$ and a response $\pry$, it is of interest to test the CI hypotheses
\begin{equation*}
H_j: \pry \independent \prx_j \mid \prx_{-j}, \quad j = 1, \dots, m.
\end{equation*}
Note that $j$ indexes variables, rather than samples. Comparing to our setup, $\prx_j$ plays the role of $\prx$ and $\prx_{-j}$ plays the role of $\prz$. In particular, we allow $\prx_j$ to be a group of variables. Like HRT, knockoffs only requires one model fit, so it too is computationally faster than the CRT. Among these three MX procedures, knockoffs is currently the most popular. We briefly review it next, and then present an optimality result in the spirit of Theorem~\ref{prop:crt-optimality}. Its proof is given in the supplement (Section~\ref{sec:knockoffs-proofs}).

\subsection{A brief overview of knockoffs} \label{sec:knockoffs-overview}

A set of knockoff variables $\prxk = (\prxk_1, \dots, \prxk_m)$ is constructed to satisfy conditional exchangeability:
\begin{equation}
\mathcal L(\prx_j, \prxk_j | \prx_{-j}, \prxk_{-j}) = \mathcal L(\prxk_j, \prx_j | \prx_{-j}, \prxk_{-j}), \quad j = 1, \dots, m
\label{conditional-exchangeability}
\end{equation}
and conditional independence 
\begin{equation}
\pry \independent \prxk \mid \prx.
\label{knockoff-conditional-independence}
\end{equation}
Given such a construction, a set of knockoff variables $\srxk_{i,\bullet}$ is sampled from  $\mathcal L(\prxk|\prx = \srx_{i,\bullet})$ for each $i$. Knockoff inference is then based on a form of data-carving: variables are given an ordering $\tau(1), \dots, \tau(m)$ determined arbitrarily from $([\srx, \srxk], \sry)$ as long as $\srx_{\bullet, j}$ and $\srxk_{\bullet, j}$ are treated symmetrically, and then tested in that order based on \textit{one-bit $p$-values} $p_j$ measuring the contrast between the strength of association between $\srx_{\bullet, j}$ and $\sry$ and that between $\srxk_{\bullet, j}$ and $\sry$. Given any statistic $T_j([\srx, \srxk], \sry)$ measuring the strength of association between $\srx_j$ and $\sry$, define the one-bit $p$-value
\begin{equation}
p_j([\srx, \srxk], \sry) \equiv 
\begin{cases}
\frac12, \quad &\text{if } T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry);  \\
1, \quad &\text{if } T_j([\srx, \srxk], \sry) \leq T_j([\srx, \srxk]_{\text{swap}(j)}, \sry).
\end{cases}
\label{one-bit-pvalue}
\end{equation}
Here, $[\srx, \srxk]_{\text{swap}(j)}$ is defined as the result of swapping $\srx_{\bullet, j}$ with $\srxk_{\bullet, j}$ in $[\srx, \srxk]$ while keeping all other columns in place. A set of variables with guaranteed false discovery rate control is chosen via the ordered testing procedure \textit{Selective SeqStep}, applied to the $p$-values $p_j$ in the order $\tau$.


\subsection{The most powerful one-bit $p$-value}

It is harder to analyze the power of knockoffs than that of the CRT for several reasons. Knockoffs is fundamentally a \textit{multiple} testing procedure, coupling the analysis of $H_j$ across variables $j$. Furthermore, the qualities of the ordering $\tau$ and of the one-bit $p$-values $p_j$ both contribute to the power of knockoffs. Due to these challenges, no optimality results are currently available for knockoffs. We take a first step in this direction by exhibiting the test statistics $T_j$ that lead to the most powerful one-bit $p$-values against a point alternative. 

\begin{theorem} \label{prop:knockoff-optimality}
	Let $\bar{\mathcal L}$ be a fixed alternative distribution for $(\prx,\pry)$, with $\bar{\mathcal L}(\pry|\prx) = \bar f(\pry|\prx)$. Define the likelihood statistic
	\begin{equation}
	T_j^{\textnormal{opt}}([\srx, \srxk], \sry) \equiv \prod_{i = 1}^n\bar f(\sry_i|\srx_{i,\bullet}).
	\label{log-likelihood-ratio-knockoffs}
	\end{equation}
	%Abbreviating $Z_j \equiv T_j([\srx, \srxk], \sry)$ and $\widetilde Z_j \equiv T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)$ and 
	Assuming that ties do not occur, that is
	\begin{equation}
	\mathbb P_{\bar {\mathcal L}}[T^{\textnormal{opt}}_j([\srx, \srxk], \sry) = T^{\textnormal{opt}}_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry), \srx_{\bullet, j} \neq \srxk_{\bullet, j}] = 0, 
	\label{nondegeneracy-assumption}
	\end{equation}
	we have that the above likelihood statistic yields the optimal one-bit $p$-value:
	\begin{equation}
	T_j^{\textnormal{opt}} \in \underset{T_j}{\arg \max}\ \mathbb P[T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry)].
	\label{unconditional-knockoff-optimality}
	\end{equation}
\end{theorem}

%The optimal test statistic~\eqref{log-likelihood-ratio-knockoffs} has a stunning simplicity: it is the likelihood of the response given the observed variables. 
The reader observes that the optimal test statistic is not a function of the knockoff variables, which may seem paradoxical. Recall from the definition~\eqref{one-bit-pvalue}, however, that the one-bit $p$-value compares the test statistic on the original and swapped augmented design $[\srx, \srxk]$. Therefore, the optimal one-bit $p$-value checks whether the original $j$th variable $\srx_{\bullet, j}$ fits with the rest of the data better than does its knockoff $\srxk_{\bullet, j}$. A simple way of operationalizing Theorem~\ref{prop:knockoff-optimality} is to fit a model $\widehat f(\pry|\prx)$ based on $([\srx, \srxk], \sry)$ in any way that treats original variables and knockoffs symmetrically, and then defining $T_j([\srx, \srxk], \sry) \equiv \widehat f(\sry|\srx)$. The above result continues to hold when $\prx_j$ is a \textit{group} of variables, giving a clean way to combine evidence across multiple variables. A conditional version of the optimality statement~\eqref{unconditional-knockoff-optimality} holds; see equation~\eqref{knockoff-conditional-optimality} in the supplement.


Theorem~\ref{prop:knockoff-optimality} requires that ties occur with probability zero~\eqref{nondegeneracy-assumption}. Proposition~\ref{prop:nondegeneracy-knockoffs} below states that this nondegeneracy condition holds if either $\pry|\prx$ or $\prx_j|\prx_{-j}, \prxk$ has continuous distribution.
\begin{proposition}\label{prop:nondegeneracy-knockoffs}
	Suppose $\bar{\mathcal L}(\pry|\prx) = g_{\bm\eta}$, where $\bm \eta = \prx_j \beta_j + f_{-j}(\prx_{-j})$ and $g_\eta$ is a one-dimensional exponential family with natural parameter $\eta$ and strictly convex, continuous log partition function $\psi$. Suppose also that $\prx_j, \beta_j \in \mathbb R$, with $\beta_j \neq 0$. The nondegeneracy condition~\eqref{nondegeneracy-assumption} holds if either 
	\begin{enumerate}
		\item $\prx_{j}|\prx_{-j}, \prxk$ has a density for each $\prx_{-j}, \prxk$, or
		\item $g_\eta$ has a density,
	\end{enumerate}
	where the densities are with respect to the Lebesgue measure.
\end{proposition}

%We see a close parallel with Theorem~\ref{prop:crt-optimality} if we note that the optimal knockoff statistic is equivalent to the optimal CRT statistic~\eqref{log-likelihood-ratio-knockoffs}, since 
%\begin{equation*}
%\prod_{i = 1}^n \frac{\bar f(\sry_i|\srx_i, \srz_i)}{\bar f(\sry_i|\srxk_i,\srz_i)} > \prod_{i = 1}^n \frac{\bar f(\sry_i|\srxk_i, \srz_i)}{\bar f(\sry_i|\srx_i,\srz_i)} \ \ \Longleftrightarrow \ \  \prod_{i = 1}^n \frac{\bar f(\sry_i|\srx_i, \srz_i)}{\bar f(\sry_i|\srz_i)} > \prod_{i = 1}^n \frac{\bar f(\sry_i|\srxk_i, \srz_i)}{\bar f(\sry_i|\srz_i)}.
%\end{equation*}
%While the likelihood ratio gives the most powerful one-bit $p$-values for a given set of knockoff variables, note that the resulting test is not necessarily the most powerful level-1/2 test of $H_0$. It follows from  Theorem~\ref{prop:crt-optimality} that this most powerful test is the CRT. For level $\alpha = 1/2$, this reduces to the optimal knockoffs test if the knockoffs variables were independently sampled from the originals, conditionally on $\srz$. This reflects the well-known phenomenon that the power of knockoffs degrades as the correlation between $\srx$ and $\srxk$ increases. While in the CRT it is always possible to sample $\srxk$ independently of $\srx$, for knockoffs one must settle for only exchangeability due to the extra constraints that characterize this method when applied simultaneously to test many variables. 
Finally, we remark that there are a few existing power analyses for knockoffs, all in high-dimensional asymptotic regimes and assuming lasso-based test statistics. Weinstein et al \cite{Weinstein2017} analyze the power of a knockoffs variant in the case of independent Gaussian covariates, while Liu and Rigollet \cite{Liu2019} and Fan et al \cite{Fan2020} study conditions for consistency under correlated designs. Our finite-sample optimality result is complimentary to these previous works.




\section{Discussion}
\label{sec:discussion}

In this paper, we gave some answers to the theoretical questions posed in the introduction. We presented the first finite-sample optimality results in the MX framework, exhibited a significantly weakened form of the MX assumption and a methodology valid under only this assumption, and explicitly quantified how the performance of the underlying ML procedure impacts the asymptotic power of the CRT.

%Our work establishes bridges between the recently proposed MX framework and more familiar realms of statistics. Working with the point null arising from the conditional interpretation of the CRT facilitated the application of finite-sample Neyman-Pearson optimality theory. Postulating a semiparametric alternative distribution allowed us to use Le Cam's local asymptotic normality theory to quantify the power of the CRT. Interpreting the MX framework as a kind of randomized experiment opened connections to causal inference and semiparametric estimation theory. We hope that these connections will continue to yield theoretical and methodological insights into the MX framework.

The MX framework is just one setting where black-box prediction methods have been recently employed for the purpose of more powerful statistical inference. Other examples include conformal prediction \cite{FoygelBarber2019}, classification-based two-sample testing \cite{Kim2020} and data-carving based multiple testing \cite{lei2016adapt}. These methods employ ML algorithms to create powerful test statistics, calibrating them for valid inference with no assumptions about the method used. However, the more accurate the learned model, the more powerful the inference. Our finite-sample and asymptotic power results explicitly tie the error of the learning algorithm to the power of the test, and thus put this common intuition on a quantitative foundation and may thus help inform the choice and design of ML methods used for inferential goals. 

Another set of connections we highlighted throughout the paper is to causal inference and semiparametric estimation. The MX CI problem has strong similarities to the problem of testing Fisher's strong null in a randomized experiment with potentially non-binary treatment and known propensity function. Furthermore, the CRT is similar in spirit to the Fisher randomization test. We believe these connections can be further leveraged to address problems in the MX framework that remain open. For example, consider the situation when the MX assumption is only approximately correct. This is analogous to the situation in observational studies, where the propensity score/function must be estimated. There is a vast literature on this topic based on ``double robustness/machine-learning''~\cite{Chernozhukov2018} or targeted learning~\cite{VanderLaan2011}. Similar ideas may help relax the MX assumption \cite{Huang2019} or study robustness to its misspecification \cite{Barber2018}. Another topic that has received little attention in the MX community is that of estimation (with the exception of \cite{Zhang2020}). Causal inference is a rich source of meaningful estimands (such as the \textit{dose response function} \cite{Hirano2004}) and estimators (such as the proposal of Kennedy et al. \cite{Kennedy2017} for doubly-robust dose response function estimation). Such ideas may be directly relevant to the MX framework.


%%\subsection{Causal stuff}
%
%%\subsection{The MX assumption reduces CI testing to inference in a randomized experiment}
%
%With the aforementioned analogy in mind, one can view the MX assumption \eqref{eq:modelX} as essentially reducing the CI testing problem to a randomized experiment with a known propensity function. This connection was briefly alluded to, but not exploited, by Candes et al \cite{CetL16}. This viewpoint implies that the rich literature on causal theory and methodology could directly impact MX theory and methods. For example, it suggests a natural estimation target, the \textit{dose response function} (see e.g. \cite{Hirano2004})
%%Note that the following discussion uses the MX assumption \eqref{eq:modelX} fully, but can be altered to assume that only $f^*_{\prx|\prz}$ is known.
%%\subsection{The dose response function} \label{sec:dose-response}
%%Consider the following example. Let $\pry \in \mathbb R$ be an individual's cholesterol level, $\prx \in \{0,1,2\}$ be the genotype of an individual at one polymorphic site $s^*$, and $\prz$ be their genotypes at other polymorphic sites across the genome. 
%\begin{equation}\label{eq:dose-response}
%	\theta(\pfx) \equiv \int \mathbb{E}[\pry \mid \prx = \pfx, \prz = \pfz] f_{\prz}(\pfz) d\pfz
%\end{equation}
%and a set of tools to estimate it (see e.g. \cite{Kennedy2017}).
%%where we recognize $\mathbb{E}[\pry \mid \prx = \pfx, \prz = \pfz]$ as the regression function. %Note that $\theta(\pfx) \neq \mathbb{E}[\pry|\prx=\pfx]$ since the latter involves integrating with respect to $f_{\prz|\prx=\pfx}$.
%%This quantity is directly borrowed from the potential outcomes literature (e.g. \cite{Hirano2004}). 
%$\theta(\pfx)$ represents the average response $\pry$ over the entire population of individuals $\prz$, if we were to set everyone's treatment $\prx$ to $\pfx$. For binary treatment, $\theta(1)-\theta(0)$ plays the role of the average treatement effect. Importantly, $\theta(\cdot)$ is well-defined even for continuous and multivariate $\prx$. In this case, it contains more information than the parameters $\rho(\mathcal L)$ or $\beta(\mathcal L)$ discussed in Section~\ref{sec:nonparametric-effect-size}, describing the impact of different treatment levels on the outcome. Accordingly, it is harder to estimate; in our language, it can be estimated under the MX assumption (see~\cite{Kennedy2017}), but not under MX(2). Of course, these interventional/counterfactual interpretations can be made rigorous by resorting to the potential outcomes framework. Even in the absence of a formal causal setup, $\theta(\cdot)$ may be an interesting nonparametric target for future work. 

%
%{\color{red}
%We first estimate the regression function $\mu_n(x,z)$ from \eqref{eq:dose-response} using any estimator $\widehat \mu_n(x,z)$ trained on a separate dataset. 
%% For example, as done in the hybrid HRT+CRT approach of the previous section, we may first fit $\widehat g$ using an external dataset, and then treating $\widehat g$ as fixed, we obtain $\widehat \mu$ by performing nonparametric regression of the observed data $Y_i - \widehat g_n(Z_i)$ onto $X_i$. With any such $\widehat \mu$ in hand, 
%We then define the \emph{pseudo-outcome} at treatment level $x$, denoted $\widetilde \pry(x)$, as
%\begin{equation}\label{eq:pseudo-outcome}
%\widetilde \pry (x) ~ \equiv ~ \frac{\pry - \widehat \mu_n(x,\prz)}{f^*_{\prx|\prz}(x)} f^*_{\prx}(x) + \int \widehat \mu_n(x,z) f^*_{\prz}(z)dz.
%\end{equation}
%It is straightforward to show that
%\begin{equation}\label{eq:unbiased-pseudo}
%\mathbb{E}[\widetilde \pry (x) \mid \prx = x] = \theta(x),
%\end{equation}
%and we verify this calculation for the reader's benefit in the supplement (Section~\ref{sup-sec:proofs-effect-size}).
%
%The identity \eqref{eq:unbiased-pseudo} has a simple implication: $\theta(x)$ can be estimated by regressing the pseudo-outcomes $\widetilde \pry(x)$ onto $\prx$. Specifically, we calculate
%\[
%\widetilde Y_i (x) \equiv \frac{Y_i - \widehat \mu_n(x,Z_i)}{f^*_{\prx|\prz = \srz_i}(x)} f^*_{\prx}(x) + \int \widehat \mu_n(x,z) f^*_{\prz}(z)dz, \quad i = 1,\dots, n,
%\]
%and run a nonparametric regression of $\{\widetilde Y_i (x)\}_{i=1}^n$ onto $\{X_i\}_{i=1}^n$ to obtain an estimator $\widehat \theta(x)$.
%Kennedy et al.~\citep{Kennedy2017} use properties of nonparametric regressors like kernel smoothing to derive confidence intervals for $\theta(x)$ by proving asymptotic normality of $\widehat \theta$, and specify rates of convergence depending on the underlying smoothness of $\theta(\cdot)$. We do not pursue further details like asymptotic normality or rates of convergence because these are well studied in causal inference: our aim was to make the connections to causal inference very explicit, allowing tools and techniques to be shared both ways. }

% In the next section, we take a step back and compare our work to the existing causal inference literature.

%\subsection{Relationship between our work and causal inference} \label{sec:relation-to-causal}



% The above construction is simply the starting point of a much deeper exploration to connect MX to causal inference. 

% Despite such parallels, our findings are complementary to standard causal inference results. We carry out our analysis conditional on $\sry$ and $\srz$, a similar framework to the finite-population treatment of randomized experiments going back to Fisher and Neyman. The adjustment for covariates and its benefits have certainly been studied in this setup (see e.g. Rosenbaum~\cite{Rosenbaum2002}), but our quantitative asymptotic results on consistency and efficiency are closer in spirit to the semiparametric superpopulation approach (e.g. \cite{Robins2001, VanderLaan2003}). The latter analysis is usually in the context of observational studies and not carried out conditionally on the response. Furthermore, our work is mainly focused on the testing problem, while most of the emphasis in semiparametric causal inference is on estimation. Therefore, our results stand at an intersection that has not been widely studied in the causal inference literature.
%
%We remark that if $\prx$ is binary, then knowledge of the propensity score $\mathbb E[\prx|\prz] = \mathbb P[\prx = 1 | \prz]$ implies knowledge of the entire propensity function $f^*_{\prx|\prz}$. However, when $\prx$ is continuous and/or multivariate, the first moment is far from sufficient to characterize the conditional distribution. In such cases, the MX(2) assumption~\eqref{MX(2)} can be viewed as knowing more than the propensity score analog $\mathbb E[\prx|\prz]$ but much less than the propensity function. 



% \paragraph{What if the MX assumption is only approximately correct?}
% When applying the MX framework in practice, the following natural question immediately arises: what if the model for $\prx$ is imperfect, or is itself learned from data? 

%
%\paragraph{Reductions between problem classes} Hypothesis tests can be categorized in many different ways. Some common categories include independence testing, two-sample testing, goodness-of-fit testing (one-sample testing), and CI testing. Even though these problem classes are often studied separately, some \textit{reduce} to each other, which informally means that an algorithm for one yields an algorithm for the other. For example, it is well known that independence testing reduces to two sample testing, and vice versa. It is also obvious that independence testing is a very special case of CI testing. In this paper, we showed that the MX assumption \eqref{eq:modelX} reduces the CI testing problem \eqref{conditional-independence} into a goodness-of-fit testing problem \eqref{point-null}. What we exploited about this reduction was that it transformed a highly composite null into a point null. Nevertheless, this reduction perspective may allow algorithms from the vast literature on goodness-of-fit testing (see \cite{Jankova2020} for a recent work in this direction) to be used for CI testing. For example, this may help in settings where the MX assumption is weakened to knowing the distribution $\prx|\prz$ up to a nuisance parameter~\cite{Huang2019}.



Much still remains to be done to systematically understand the theoretical properties of MX methods. One interesting direction is to analyze the case when $\widehat g_n$ is learned on the same data as is used for testing. We saw in Section~\ref{sec:comparison-to-existing-results-4} that Theorem~\ref{thm:power} extends to lasso-based estimators $\widehat g_n$ learned in-sample, but the generality of such results remains an open question. It would also be interesting to consider alternatives beyond the linear model~\eqref{eq:linearity}. A natural next step would be to consider generalized linear models. Furthermore, the connections to causal inference referenced above are tantalizing and deserve a dedicated treatment. Finally, we hope that these new theoretical insights about MX methods will lead to improved methodologies that are both statistically and computationally efficient, along the lines of the CRT variants discussed in this paper and in recent work \cite{Liu2020}.


\section*{Acknowledgments}
We thank Asaf Weinstein, Timothy Barry, and Stephen Bates for detailed comments on earlier versions of the manuscript, as well as Ed Kennedy and Larry Wasserman for discussions of the connections to causal inference.


\printbibliography

\appendix

\section{Proofs for Section~\ref{sec:power}} \label{sec:proofs-sec2}

\begin{proof}[Proof of Proposition~\ref{prop:marginal-implies-conditional}]
	Fix $(y,z)$ and $\mathcal L\in \mathscr L_0^{\textnormal{MX}}(f^*)$. Defining 
	\[
	\mathcal L_{y,z}(\prx, \pry, \prz) \equiv \delta_{(\pry, \prz) = (y,z)} \cdot f^*_{\prx|\prz} \in \mathscr L_0^{\textnormal{MX}}(f^*), 
	\]
	note that
	\begin{equation*}
		\mathbb E_{\mathcal L}[\phi(\srx, \sry, \srz)|\sry = \sfy, \srz = \sfz] = \int \phi(x,y,z)f^*_{\prx|\prz}(x|z)dx = \mathbb E_{\mathcal L_{y,z}}[\phi(\srx, \sry, \srz)] \leq \alpha.
	\end{equation*}
	This completes the proof.
\end{proof}	

\begin{proof}[Proof of Theorem~\ref{prop:crt-optimality}]
	Fix realizations $\sfy, \sfz$. We first claim that $\phi^{\CRT}_{T^{\textnormal{opt}}}$ is the most powerful test in the conditional problem, i.e.
	\begin{equation}
		\mathbb E_{\bar{\mathcal L}}[\phi(\srx,\sfy,\sfz)|\sry = \sfy, \srz = \sfz] \leq \mathbb E_{\bar{\mathcal L}}[\phi^{\CRT}_{T^\textnormal{opt}}(\srx, \sfy, \sfz)|\sry = \sfy, \srz = \sfz]
		\label{conditional-optimality}
	\end{equation}
	for any test $\phi(\srx, \sfy, \sfz)$ satisfying
	\begin{equation}
		\sup_{\mathcal L \in \mathscr L_0^{\text{MX}}(f^*)}\mathbb E_{\mathcal L}[\phi(\srx,\sfy,\sfz)|\sry = \sfy, \srz = \sfz] \leq \alpha.
		\label{eq:conditional-level}
	\end{equation}
	In the conditional problem, the alternative $\bar{\mathcal L}$ induces the following distribution for $\srx$:
	\begin{equation}
		\begin{split}
			\bar{\mathcal L}(\srx = \sfx|\sry = \sfy, \srz = \sfz) = \prod_{i = 1}^n  f^*(\sfx_i|\sfz_i)\tfrac{\bar f(\sfy_i|\sfx_i, \sfz_i)}{\bar f(\sfy_i|\sfz_i)}
			\label{conditional-alternative}
		\end{split}
	\end{equation}
	where
	\begin{equation*}
	\bar f(\sfy_i|\sfz_i) \equiv \int \bar f(\sfy_i|\sfx_i, \sfz_i)f^*(\sfx_i|\sfz_i)d\sfx_i.
	\end{equation*}	
	The conditional problem is therefore a test of 
	\begin{equation*}
		\begin{split}
			&H_0: \mathcal L(\srx = \sfx|\sry = \sfy, \srz = \sfz) = \prod_{i = 1}^n  f^*(\sfx_i | \sfz_i) \quad \text{versus} \\
			&H_1: \mathcal L(\srx = \sfx|\sry = \sfy, \srz = \sfz) = \prod_{i = 1}^n  f^*(\sfx_i|\sfz_i)\tfrac{\bar f(\sfy_i|\sfx_i, \sfz_i)}{\bar f(\sfy_i|\sfz_i)}.
		\end{split}
	\end{equation*}
	This is a simple testing problem, with point null and point alternative. By the Neyman-Pearson lemma, the most powerful test is the one that rejects for large values of the likelihood ratio
	\begin{equation*}
		\prod_{i = 1}^n \frac{P_1(\sfx_i|\sfy_i, \sfz_i)}{P_0(\sfx_i|\sfy_i, \sfz_i)} = \prod_{i = 1}^n \frac{f^*(\sfx_i|\sfz_i)\frac{\bar f(\sfy_i|\sfx_i, \sfz_i)}{\bar f(\sfy_i|\sfz_i)}}{f^*(\sfx_i|\sfz_i)} = \prod_{i = 1}^n \frac{\bar f(\sfy_i|\sfx_i, \sfz_i)}{\bar f(\sfy_i|\sfz_i)} \propto T^{\text{opt}}(\sfx, \sfy, \sfz),
	\end{equation*}
	verifying the conditional optimality claim~\eqref{conditional-optimality}. To obtain the unconditional optimality claim~\eqref{unconditional}, note that by Proposition~\ref{prop:marginal-implies-conditional} any unconditionally level $\alpha$ test $\phi$ must also have level $\alpha$ in the conditional problem~\eqref{eq:conditional-level}. For any such test, we may therefore conclude
	\begin{equation*}
		\begin{split}
			\mathbb E_{\bar{\mathcal L}}[\phi(\srx,\sry,\srz)] &= \mathbb E_{\bar{\mathcal L}}[\mathbb E_{\bar{\mathcal L}}[\phi(\srx,\sry,\srz)|\sry,\srz]] \\
			&\leq \mathbb E_{\bar{\mathcal L}}\left[\mathbb E_{\bar{\mathcal L}}[\phi^{\CRT}_{T^\textnormal{opt}}(\srx,\sry,\srz)|\sry,\srz]\right] = \mathbb E_{\bar{\mathcal L}}[\phi^{\CRT}_{T^\textnormal{opt}}(\srx, \sry, \srz)],
		\end{split}
	\end{equation*}
as desired.
\end{proof}

\section{Proofs for Sections~\ref{sec:weakening} and~\ref{sec:asymptotic-power}} \label{sec:proofs-sec45}

\subsection{Proofs of main results}

\begin{proof}[Proof of Theorem~\ref{thm:asymptotic-alpha-level}]

Fix any sequence $\mathcal L_n \in \mathscr L_0^{\textnormal{MX(2)}}\cap \mathscr L_n(c_1, c_2)$. Because $\mathcal L_n \in \mathscr L_0$, we have $(\srx, \sry, \srz) \overset d = (\srxk, \sry, \srz)$, where $\srxk_i|\sry,\srz \overset{\text{ind}}\sim \mathcal L_n(\prx|\prz = \srz_i)$. By conclusion~\eqref{convergence-2} of Lemma~\ref{lem:clt}, which applies because $\mathcal L_n \in \mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \cap \mathscr L_n(c_1, c_2)$ by assumption, we have $U_n(\srx, \sry,\srz) \overset d = U_n(\srxk, \sry,\srz) \overset{\mathcal L_n}\rightarrow_d N(0, I_d)$. This verifies the asymptotic normality statement~\eqref{eq:asymptotic-normality}.

To show the asymptotic Type-I error control statement~\eqref{asymptotic-alpha-level}, it suffices to show that for any sequence $\mathcal L_n \in \mathscr L_0^{\textnormal{MX(2)}}\cap \mathscr L_n(c_1, c_2)$, we have
	\begin{equation}
		\limsup_{n \rightarrow \infty}\ \mathbb E_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz)] \leq \alpha.
	\label{eq:pointwise-alpha-level}
	\end{equation}
By the continuous mapping theorem it follows from asymptotic normality~\eqref{eq:asymptotic-normality} that $T_n(\srx,\sry,\srz) = \|U_n(\srx, \sry,\srz)\|^2 \overset{\mathcal L_n}\rightarrow_d \chi^2_d$. Therefore,
	\begin{equation*}
		\lim_{n \rightarrow \infty}\ \mathbb E_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz)] = \lim_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[T_n(\srx,\sry,\srz) > c_{d,1-\alpha}] = \mathbb P[\chi^2_d > c_{d,1-\alpha}] = \alpha,
	\end{equation*}
	from which the conclusion~\eqref{eq:pointwise-alpha-level} follows. This completes the proof.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:equivalence}]
	
	First, conclusion~\eqref{convergence-2} of Lemma~\ref{lem:clt}---which applies because of the assumption $\mathcal L_n \in \mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \cap \mathscr L_n(c_1, c_2)$---states that for 
	\[
	\srxk^1_i, \srxk^2_i|\sry,\srz \overset{\text{ind}}\sim {\mathcal L_n(\prx|\prz = \srz_i)}, 
	\]
	we have the convergence
	\begin{equation}
		{U_n(\srxk^1, \sry, \srz) \choose U_n(\srxk^2, \sry, \srz)} \overset{\mathcal L_n}\rightarrow_d N\left({0 \choose 0},
		\begin{pmatrix}
			I_d & 0 \\
			0 & I_d
		\end{pmatrix}
		\right).
	\end{equation}
	By the continuous mapping theorem, we find that 
	\begin{equation}
		(T_n(\srxk^1, \sry, \srz), T_n(\srxk^2, \sry, \srz)) \overset{\mathcal L_n}\rightarrow_d  \chi^2_d \times \chi^2_d. 
	\end{equation}
	Since $\chi^2_d$ has a continuous and strictly increasing distribution function, we conclude using Lemma~\ref{lem:lucas} that $C_n(\sry,\srz)  \overset{\mathcal L_n}\rightarrow_p Q_{1-\alpha}[\chi^2_d] =  c_{d,1-\alpha}$, proving the statement~\eqref{eq:threshold-convergence}.
	
	Next, note that for any $\delta > 0$,
	\small
	\begin{equation*}
		\begin{split}
			&\mathbb P_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz) \neq \phi^{\textnormal{CRT}}_n(\srx, \sry, \srz)] \\
			&\quad = \mathbb P_{\mathcal L_n}[\min(c_{d,1-\alpha},C_n(\sry, \srz))  < T_n(\srx, \sry, \srz) \leq \max(c_{d,1-\alpha},C_n(\sry, \srz))] \\
			&\quad=\mathbb P_{\mathcal L_n}[\min(c_{d,1-\alpha},C_n(\sry, \srz))  < T_n(\srx, \sry, \srz) \leq \max(c_{d,1-\alpha},C_n(\sry, \srz)), |C_n(\sry, \srz)-c_{d,1-\alpha}| \leq \delta] \\
			&\quad \quad +  \mathbb P_{\mathcal L_n}[\min(c_{d,1-\alpha},C_n(\sry, \srz))  < T_n(\srx, \sry, \srz) \leq \max(c_{d,1-\alpha},C_n(\sry, \srz)), |C_n(\sry, \srz)-c_{d,1-\alpha}| > \delta]\\
			&\quad\leq \mathbb P_{\mathcal L_n}[|T_n(\srx, \sry, \srz)-c_{d,1-\alpha}| \leq \delta] + \mathbb P_{\mathcal L_n}[|C_n(\sry, \srz)-c_{d,1-\alpha}| > \delta].
		\end{split}
	\end{equation*}
	\normalsize
	To justify the last step, suppose without loss of generality that $c_{d,1-\alpha} \leq C_n(\sry, \srz)$. Then, note that if $c_{d,1-\alpha} < T_n(\srx, \sry, \srz) \leq C_n(\sry, \srz)$ and $C_n(\sry, \srz)-c_{d,1-\alpha} \leq \delta$ then
	\begin{equation*}
		|T_n(\srx, \sry,\srz)-c_{d,1-\alpha}| = T_n(\srx, \sry,\srz)-c_{d,1-\alpha} \leq C_n(\sry, \srz)- c_{d,1-\alpha} \leq \delta.
	\end{equation*}
	Taking a $\limsup$ on both sides in the display before the last and using the convergence $C_n(\sry,\srz)  \overset{\mathcal L_n}\rightarrow_p c_{d,1-\alpha}$, we find that
	\begin{equation*}
		\begin{split}
			&\limsup_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_n(\srx, \sry, \srz) \neq \phi^{\textnormal{CRT}}_n(\srx, \sry, \srz)] \\
			&\quad \leq \limsup_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[|T_n(\srx, \sry, \srz)-c_{d,1-\alpha}| \leq \delta]+ \limsup_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[|C_n(\sry, \srz)-c_{d,1-\alpha}| > \delta] \\
			&\quad = \limsup_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[|T_n(\srx, \sry, \srz)-c_{d,1-\alpha}| \leq \delta].
		\end{split}
	\end{equation*}
	Letting $\delta \rightarrow 0$ and using the assumption~\eqref{eq:non-accumulation}, we arrive at the claimed asymptotic equivalence~\eqref{eq:asymptotic-equivalence}. This completes the proof.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:power}]
	
	We start by proving consistency. To this end, we claim that
	\begin{equation}
	\widehat \rho_n \overset{\mathcal L_n}\rightarrow _p \overline \Sigma \beta.
	\label{eq:estimation-consistency}
	\end{equation}
	Indeed, $\widehat \rho_n$ is the mean of i.i.d. terms with expectation 
	\begin{equation}
		\begin{split}
	&\mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))(\prx - \mu_n(\prz))]  \\
	&\quad = \mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T\beta + \peps + g_n(\prz) - \widehat g_n(\prz))(\prx - \mu_n(\prz))] = \overline \Sigma_n \beta.
	\end{split}
	\end{equation}
	These terms also have bounded second moment, since
	\begin{equation}
		\begin{split}
		&\mathbb E_{\mathcal L_n}[\|(\pry - \widehat g_n(\prz))(\prx - \mu_n(\prz))\|^2] \\
		&\quad= 	
		\mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T\beta + \peps + g_n(\prz) - \widehat g_n(\prz))^2\|\prx - \mu_n(\prz)\|^2] \\
		&\quad\leq
		C\mathbb E_{\mathcal L_n}[(\|\prx - \mu_n(\prz)\|^2\|\beta\|^2 + \peps^2 + (g_n(\prz) - \widehat g_n(\prz))^2)\|\prx - \mu_n(\prz)\|^2] \\
		&\quad=
		C\|\beta\|^2\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4] + C\sigma^2\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^2] \\
		&\quad \quad + C\mathbb E_{\mathcal L_n}[(g_n(\prz) - \widehat g_n(\prz))^2\|\prx - \mu_n(\prz)\|^2].
		\end{split}
	\end{equation}	
Here, $C$ is a constant so that $(a+b+c)^2 \leq C(a^2 + b^2 + c^2)$ for any $a, b, c \geq 0$.
Taking a supremum over $n$ and using the assumptions~\eqref{eq:eighth-moment-assump-1} and~\eqref{eq:eighth-moment-assump-2} yields
\begin{equation}
	\sup_n\ \mathbb E_{\mathcal L_n}[\|(\pry - \widehat g_n(\prz))(\prx - \mu_n(\prz))\|^2] < \infty.
\end{equation}
Therefore, the weak law of large numbers implies that
\begin{equation}
	\widehat \rho_n - \overline \Sigma_n \beta \overset{\mathcal L_n}\rightarrow _p  0,
\end{equation}
from which the statement~\eqref{eq:estimation-consistency} follows by the assumed convergence $\overline \Sigma_n \rightarrow \overline \Sigma$. Next, we derive that
\begin{equation*}
T_n(\srx, \sry, \srz) = \|\sqrt n\widehat S_n^{-1}\widehat \rho_n\|^2 = \|\sqrt n\widehat S_n^{-1}S_n S_n^{-1}\widehat \rho_n\|^2 \geq \left(\sqrt{n}\lambda_{\min}(\widehat S_n^{-1}S_n)\lambda_{\min}(S_n^{-1}) \|\widehat \rho_n\|\right)^2.
\end{equation*}
Now, we have $\widehat S_n^{-1}S_n \overset{\mathcal L_n}\rightarrow _p  I_d$ by conclusion~\eqref{eq:conv-s-neg-1} of Lemma~\ref{lem:aux}, so the continuous mapping theorem implies that $\lambda_{\min}(\widehat S_n^{-1}S_n) \overset{\mathcal L_n}\rightarrow _p 1$. Furthermore, $\inf_n \lambda_{\min}(S_n^{-1}) > 0$ by conclusion~\eqref{eq:s-n-2-limit-a} of Lemma~\ref{lem:fourth-moment}. Finally, $\|\widehat \rho_n\| \overset{\mathcal L_n}\rightarrow _p \|\overline \Sigma \beta\|$ by equation~\eqref{eq:estimation-consistency}, and 
\[
\|\overline \Sigma \beta\| \geq \lambda_{\min}(\overline \Sigma)\|\beta\| = \|\overline \Sigma^{-1}\|^{-1}\|\beta\| \geq \left(\sup_n \|\overline \Sigma_n^{-1}\|\right)^{-1}\|\beta\| > 0
\]
since $\beta \neq 0$ by assumption and assumptions~\eqref{eq:s-n-inverse-assump} and \eqref{eq:limits} imply that $\|\overline \Sigma^{-1}\| \leq \sup_n \|\overline \Sigma_n^{-1}\| < \infty$. Putting these facts together implies that $\sqrt{n}\lambda_{\min}(\widehat S_n^{-1}S_n)\lambda_{\min}(S_n^{-1}) \|\widehat \rho_n\|\overset{\mathcal L_n}\rightarrow _p \infty$, and therefore $T_n(\srx, \sry, \srz) \overset{\mathcal L_n}\rightarrow _p \infty$. Hence,
\begin{equation}
\begin{split}
\mathbb E_{\mathcal L_n}[\phi_n^{\text{MX(2)}}(\srx, \sry, \srz)] = \mathbb P_{\mathcal L_n}[T_n(\srx, \sry, \srz) > c_{d,1-\alpha}] \rightarrow 1.
\end{split}
\end{equation}
The fact that $T_n(\srx, \sry, \srz) \overset{\mathcal L_n}\rightarrow _p \infty$ also implies that $\limsup_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[|T_n(\srx, \sry, \srz)-c_{d,1-\alpha}| \leq \delta] = 0$ for any $\delta > 0$. Hence, the condition~\eqref{eq:non-accumulation} of Theorem~\ref{thm:equivalence} is satisfied, so the conclusion~\eqref{eq:asymptotic-equivalence} implies that
\begin{equation*}
\lim_{n \rightarrow \infty}\mathbb E_{\mathcal L_n}[\phi_n^{\text{CRT}}(\srx, \sry, \srz)] = \lim_{n \rightarrow \infty}\mathbb E_{\mathcal L_n}[\phi_n^{\text{MX(2)}}(\srx, \sry, \srz)] = 1.
\end{equation*} 
Thus, we have shown the claimed consistency~\eqref{eq:consistency}, so we have finished the proof of part (a) of the theorem.
	
To prove part (b), we claim that it suffices to establish that
\begin{equation}
	T_n(\srx, \sry, \srz)\overset{\mathcal L_n}\rightarrow_d \chi^2_d(\|(\sigma^2I_d +\mathcal E^2)^{-1/2}\overline \Sigma^{1/2} h\|^2).
	\label{eq:convergence-of-T}
\end{equation}
Indeed, the limiting power of the MX(2) $F$-test would directly follow from this statement. To establish that the CRT has the same limiting power, by Theorem~\ref{thm:equivalence} it suffices to verify the non-accumulation condition~\eqref{eq:non-accumulation}. Letting $T$ be the limiting distribution in claim~\eqref{eq:convergence-of-T}, this claim implies that for any $\delta > 0$,
\begin{equation*}
 \lim_{n \rightarrow \infty}\ \mathbb P_{\mathcal L_n}[|T_n(\srx, \sry, \srz)-c_{d,1-\alpha}| \leq \delta] = \mathbb P[|T - c_{d,1-\alpha}| \leq \delta].
\end{equation*}
Because $T$ has a continuous distribution function, the limit above tends to zero. Therefore, it is indeed sufficient to verify the claimed convergence~\eqref{eq:convergence-of-T}. This statement, in turn, will follow if we prove that
	\begin{equation}
		U_n(\srx, \sry, \srz) \overset{\mathcal L_n}\rightarrow_d N((\overline \Sigma^{1/2}(\sigma^2I_d +\mathcal E^2)\overline \Sigma^{1/2})^{-1/2}\overline \Sigma h, I_d).
		\label{eq:U-conv}
	\end{equation}
	Indeed, note that
	\begin{equation*}
		h^T \overline \Sigma (\overline \Sigma^{1/2}(\sigma^2I_d +\mathcal E^2)\overline \Sigma^{1/2})^{-1}\overline \Sigma h = h^T \overline \Sigma^{1/2} (\sigma^2I_d +\mathcal E^2)^{-1}\overline \Sigma^{1/2} h = \|(\sigma^2I_d +\mathcal E^2)^{-1/2}\overline \Sigma^{1/2} h\|^2.
	\end{equation*}
	
	To show the statement~\eqref{eq:U-conv}, we first rewrite $U_n(\srx, \sry,\srz)$ as follows:
	\begin{equation*}
		\begin{split}
			&U_n(X,Y,Z) \\
			&\quad= \frac{\widehat S^{-1}_n}{\sqrt{n}}\sum_{i = 1}^n ((X_i - \mu_n(Z_i))^T \beta_n + \eps_i + g_n(\srz_i) - \widehat g_n(\srz_i))(\srx_i - \mu_n(\srz_i)) \\
			&\quad= \frac{\widehat S^{-1}_n}{n}\sum_{i = 1}^n (\srx_i - \mu_n(\srz_i))(X_i - \mu_n(Z_i))^T h_n + \frac{\widehat S^{-1}_n}{\sqrt{n}}\sum_{i = 1}^n (Y'_i - \widehat g_n(Z_i))(\srx_i - \mu_n(\srz_i)) \\
			&\quad \equiv A_n + B_n,
		\end{split}
	\end{equation*}
	where $Y'_i \equiv g_n(\srz_i) + \seps_i.$ It therefore suffices to show that
	\begin{equation}
		A_n \overset{\mathcal L_n}\rightarrow_p (\overline \Sigma^{1/2}(\sigma^2 I_d + \mathcal E^2)\overline \Sigma^{1/2})^{-1/2}\overline \Sigma h\quad \text{and} \quad B_n \overset{\mathcal L_n}\rightarrow_d N(0, I_d).
		\label{eq:sufficient}
	\end{equation}
	
	By conclusion~\eqref{eighth-moment} of Lemma~\ref{lem:fourth-moment}, there exist $c_1, c_2$ for which $\mathcal L_n \in \mathscr L(c_1, c_2)$ for each $n$. Therefore, we can apply Lemma~\ref{lem:aux} to conclude that 
	\begin{equation}
		\widehat S_n^{-1}S_n  \overset{\mathcal L_n}\rightarrow_p I_d. 
		\label{observation-1}
	\end{equation}
	By conclusion~\eqref{eq:s-n-2-limit} of Lemma~\ref{lem:fourth-moment}, we have that $S_n^2 \rightarrow \overline \Sigma^{1/2}(\sigma^2 I_d + \mathcal E^2)\overline \Sigma^{1/2}$, so 
	\begin{equation}
		S_n^{-1} \rightarrow (\overline \Sigma^{1/2}(\sigma^2 I_d + \mathcal E^2)\overline \Sigma^{1/2})^{-1/2}.
		\label{observation-2}
	\end{equation} 
	Now, we apply the WLLN to find the limit of $A_n$. Since $(\srx_i - \mu_n(\srz_i))(X_i - \mu_n(Z_i))^T$ has expectation $\overline \Sigma$ and second moment uniformly bounded by the eighth moment assumption~\eqref{eq:eighth-moment-assump-1}, we can apply the weak law of large numbers as well as the statements~\eqref{observation-1} and \eqref{observation-2} to conclude that
	\begin{equation*}
		A_n = (\widehat S^{-1}_nS_n)\frac{S_n^{-1}}{n}\sum_{i = 1}^n (\srx_i - \mu_n(\srz_i))(X_i - \mu_n(Z_i))^T h_n \overset{\mathcal L_n}\rightarrow_p (\overline \Sigma^{1/2}(\sigma^2 I_d + \mathcal E^2)\overline \Sigma^{1/2})^{-1/2}\overline \Sigma h.
	\end{equation*}
	Next, we seek to find the limit of $B_n$. Defining $\pry', S'^2_n, \mathcal L'_n$ according to \eqref{prime-definitions} below, we may rewrite
	\begin{equation}
		B_n = (\widehat S_n^{-1}S_n)(S_n^{-1}S'_n)\frac{S'^{-1}_n}{\sqrt{n}}\sum_{i = 1}^n (Y'_i - \widehat g_n(Z_i))(\srx_i - \mu_n(\srz_i)).
	\end{equation}
	By conclusion~\eqref{eq:s-n-2-limit} of Lemma~\ref{lem:fourth-moment}, we have
	\begin{equation}
		\quad S_n^{-1}S'_n \rightarrow I_d.
		\label{display-1}
	\end{equation}
	Furthermore, conclusion~\eqref{eighth-moment} of Lemma~\ref{lem:fourth-moment} gives $\mathcal L'_n \in \mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \cap \mathscr L_n(c_1, c_2)$. Therefore, $\mathcal L'_n$ satisfies the assumptions of Lemma~\ref{lem:clt}, statement~\eqref{convergence-1} of which gives
	\begin{equation}
		\begin{split}
			\frac{S'^{-1}_n}{\sqrt{n}}\sum_{i = 1}^n (Y'_i - \widehat g_n(Z_i))(\srxk_i - \mu_n(\srz_i)) \overset{\mathcal L_n}\rightarrow_d N(0, I_d).
			\label{display-2}
		\end{split}
	\end{equation}
	Furthermore, $\mathcal L'_n \in \mathscr L_0$ implies that $(\prx, \pry', \prz) \overset d = (\prxk, \pry', \prz)$, which together with the convergence~\eqref{display-2} implies that
	\begin{equation}
		\begin{split}
			\frac{S'^{-1}_n}{\sqrt{n}}\sum_{i = 1}^n (Y'_i - \widehat g_n(Z_i))(\srx_i - \mu_n(\srz_i)) \overset{\mathcal L_n}\rightarrow_d N(0, I_d).
			\label{display-3}
		\end{split}
	\end{equation}
	Finally, putting together displays~\eqref{observation-1}, \eqref{display-1} and~\eqref{display-3} yields that $B_n \overset{\mathcal L_n}\rightarrow_d N(0, I_d)$. This verifies the claimed convergences~\eqref{eq:sufficient} and therefore completes the proof.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:lasso}]

First we verify the statement~\eqref{eq:bm-result}. To this end, first note that
\begin{equation}
	\begin{split}
		\mathcal E_n^2 &=  \mathbb E_{\mathcal L_n}[(\widehat g_n(\prz) - g_n(\prz))^2\overline \Sigma_n^{-1/2}\Sigma_n(\prz)\overline \Sigma_n^{-1/2} \mid \srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \\
		&= \mathbb E_{\mathcal L_n}[(\widehat g_n(\prz) - g_n(\prz))^2 \mid \srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \\
		&= \mathbb E_{\mathcal L_n}[(\widehat \gamma_{\pi n} - \gamma_n)^T \prz \prz^T (\widehat \gamma_{\pi n} - \gamma_n) \mid \srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \\
		&= (\widehat \gamma_{\pi n} - \gamma_n)^T\mathbb E_{\mathcal L_n}[ \prz \prz^T](\widehat \gamma_{\pi n} - \gamma_n) \\
		&= \|\widehat \gamma_{\pi n} - \gamma_n\|^2.
		\label{eq:error}
	\end{split}
\end{equation}
The second equality holds because for $(\prx, \prz)$ jointly Gaussian, $\Sigma_n(\prz)$ is constant in $\prz$, so $\overline \Sigma_n^{-1/2}\Sigma_n(\prz)\overline \Sigma_n^{-1/2} = 1$.
Therefore, the variance-weighted mean-squared error $\mathcal E_n^2$ of $\widehat g_n$ reduces to the squared error in the estimate $\widehat \gamma_{\pi n}$. To obtain the limit of the latter quantity, we appeal to Bayati and Montanari's Corollary 1.6 \cite{Bayati2011}. To verify the conditions of this corollary, it suffices to verify part (b) of their Definition 1: that the empirical distribution of the noise terms $\seps'_i \equiv \sry_i - \srz_i^T \gamma_n = \srx_i \beta_n + \seps_i$ in the training set (say $1 \leq i \leq \pi n$) converges weakly to a random variable $\Lambda$ and $\frac{1}{\pi n}\sum_{i = 1}^{\pi n} \eps'^2_i \rightarrow \mathbb E[\Lambda^2]$. These statements hold almost surely in the training data by the strong law of large numbers if we assume without loss of generality that $\srx_i$ and $\seps_i$ are both defined as the first $\pi n$ elements of infinite i.i.d. sequences with distributions $N(0,1)$ and $N(0,\sigma^2)$, respectively. Therefore, Bayati and Montanari's Corollary 1.6 gives 
\begin{equation}
	\frac{1}{p}\|\sqrt{\pi n}\widehat \gamma_{\pi n} - \sqrt{\pi n}\gamma_n\|^2 \overset{\text{a.s.}}\rightarrow \pi\delta(\tau_*^2 - \sigma^2),
\end{equation}
where the almost sure statement is with respect to the training data $(\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}})$. Since $\pi n/p \rightarrow \pi \delta$, we can cancel these terms from the above equation to obtain
\begin{equation}
	\|\widehat \gamma_{\pi n} - \gamma_n\|^2 \overset{\text{a.s.}}\rightarrow \tau_*^2 - \sigma^2.
	\label{eq:BM11}
\end{equation}
Putting together equations~\eqref{eq:error} and~\eqref{eq:BM11} gives the claimed statement~\eqref{eq:bm-result}. 

To apply this result, we must verify the assumptions of Theorem~\ref{thm:power}. The bounded inverse assumption~\eqref{eq:s-n-inverse-assump} holds because $\overline \Sigma_n = 1$ for all $n$ in the orthogonal design setting. The eighth moment assumption~\eqref{eq:eighth-moment-assump-1} holds due to the boundedness of the eighth moments of Gaussian random variables. To verify the moment assumption~\eqref{eq:eighth-moment-assump-2}, we note that, almost surely in the training data,
\begin{equation*}
\begin{split}
&\sup_n\ \mathbb E_{\mathcal L_n}[(\widehat g_n(\prz)-g_n(\prz))^4\|\prx - \mu_n(\prz)\|^4|\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \\
&\quad= \sup_n\ \mathbb E_{\mathcal L_n}[(\prz \widehat \gamma_{\pi n} - \prz \gamma_n)^4\|\prx - \mu_n(\prz)\|^4|\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \\
&\quad\leq \sup_n\|\widehat \gamma_{\pi n} - \gamma_n\|^4  \mathbb E_{\mathcal L_n}[\|\prz\|^4\|\prx - \mu_n(\prz)\|^4] \\
&\quad\leq \sup_n\|\widehat \gamma_{\pi n} - \gamma_n\|^4  \mathbb E_{\mathcal L_n}[\|\prz\|^8]^{1/2} \mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^8]^{1/2} \\
&\quad < \infty.
\end{split}
\end{equation*}
The last inequality holds because $\|\widehat \gamma_{\pi n} - \gamma_n\|^4$ has a finite limit according to~\eqref{eq:BM11} and because $\prz$ and $\prx$ have bounded eighth moments since they are Gaussian. Finally, we verify assumption~\eqref{eq:limits} by noting that $\overline \Sigma_n\rightarrow \overline \Sigma \equiv 1$ and $\mathcal E_n^2 \rightarrow \tau_*^2 - \sigma^2 \equiv \mathcal E$, the latter by statement~\eqref{eq:bm-result}. Therefore, Theorem~\ref{thm:power} gives 
\begin{equation*}
	\begin{split}
		\mathbb E_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_{(1-\pi)n}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})|\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \overset{\text{a.s.}}\rightarrow \mathbb P[\chi^2_1(\|\tau_*^{-1} h\sqrt{1-\pi}\|^2) > c_{1,1-\alpha}]
	\end{split}
\end{equation*}
and
\begin{equation*}
	\begin{split}
		\mathbb E_{\mathcal L_n}[\phi^{\textnormal{CRT}}_{(1-\pi)n}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})|\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}] \overset{\text{a.s.}}\rightarrow \mathbb P[\chi^2_1(\|\tau_*^{-1} h\sqrt{1-\pi}\|^2) > c_{1,1-\alpha}].
	\end{split}
\end{equation*}
The extra factor of $\sqrt{1-\pi}$ reflects the fact that a sample size of $(1-\pi)n$ is used for testing, so $\beta_n = h_n/\sqrt{n} = h_n\sqrt{1-\pi}/\sqrt{(1-\pi)n}$. In other words, reducing the number of samples for testing from $n$ to $(1-\pi)n$ has the effect of reducing the alternative signal strength from $h_n$ to $h_n \sqrt{1-\pi}$. Noting that $c_{1,1-\alpha} = z^2_{1-\alpha/2}$, we conclude using the dominated convergence theorem that
\begin{equation*}
	\begin{split}
		\mathbb E_{\mathcal L_n}[\varphi^{\textnormal{MX(2)}}_{n}(\srx, \sry, \srz)] &=  
		\mathbb E_{\mathcal L_n}\left[\mathbb E_{\mathcal L_n}[\phi^{\textnormal{MX(2)}}_{(1-\pi)n}(\srx_{\text{test}}, \sry_{\text{test}}, \srz_{\text{test}})|\srx_{\text{train}}, \sry_{\text{train}}, \srz_{\text{train}}]\right] \\
		&\rightarrow \mathbb P[|N(\tau_*^{-1} h\sqrt{1-\pi},1)| > z_{1-\alpha/2}],
	\end{split}
\end{equation*}
and likewise that
\begin{equation*}
	\begin{split}
		\mathbb E_{\mathcal L_n}[\varphi^{\text{CRT}}_{n}(\srx, \sry, \srz)] \rightarrow \mathbb P[|N(\tau_*^{-1} h\sqrt{1-\pi},1)| > z_{1-\alpha/2}].
	\end{split}
\end{equation*}
This completes the proof of the corollary.
\end{proof}

\subsection{Technical lemmas}

First, we state a lemma that gives a sufficient condition for the convergence of the CRT threshold, which follows directly from  Lemmas 2 and 3 of \cite{Wang2020b}.
\begin{lemma}[\cite{Wang2020b}] \label{lem:lucas}
	Let $\mathcal L_n$ be a sequence of laws over $(\prx,\pry,\prz)$, from which $(\srx,\sry,\srz)$ are sampled. Furthermore, for each $i$ sample two independent copies
	\begin{equation}
		\srxk_i^1,\srxk_i^2 \overset{\text{i.i.d.}}\sim \mathcal L_n(\prx|\prz = \srz_i) \quad \text{such that, given } \srz,  (\srxk_1^1,\srxk_1^2) \independent \cdots \independent (\srxk_n^1,\srxk_n^2) \independent \sry.
		\label{eq:two-resamples}
	\end{equation}
	Suppose that $T_n(\srx,\sry,\srz)$ is a test statistic satisfying 
	\begin{equation}
		(T_n(\srxk^1,\sry,\srz), T_n(\srxk^2, \sry,\srz)) \overset{\mathcal L_n}\rightarrow_d  \widetilde T \times \widetilde T
	\end{equation}
	for some limiting random variable $\widetilde T$ with continuous and strictly increasing distribution function. Then, the CRT threshold converges in probability to the upper quantile of $\widetilde T$:
	\begin{equation}
		C_n(\sry, \srz) \equiv Q_{1-\alpha}[T_n(\srxk,\sry,\srz)|\sry,\srz]  \overset{\mathcal L_n}\rightarrow_p Q_{1-\alpha}[\widetilde T].
	\end{equation}
	
\end{lemma}


\begin{lemma}\label{lem:aux}
	
	Fix any $c_1, c_2 > 0$. For any sequence
	\begin{equation}
		\mathcal L_n \in \mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \cap \mathscr L_n(c_1, c_2),
	\end{equation}
	we have
	\begin{equation}
		\widehat S_n^2 - S_n^2 \overset{\mathcal L_n}\rightarrow_p  0
		\label{eq:conv-s-2}
	\end{equation}
	and
	\begin{equation}
		\widehat S_n^{-1}S_n \overset{\mathcal L_n}\rightarrow_p I_d.
		\label{eq:conv-s-neg-1}
	\end{equation}
	
	\begin{proof}
		To show the first convergence~\eqref{eq:conv-s-2}, we apply the WLLN to the triangular array $\{(\sry_{i} - \widehat g_n(\srz_{i}))^2\Sigma_n(\srz_i)\}_{i,n}$. We first verify the second moment condition:
		\begin{equation}
			\begin{split}
				&\sup_{n}\ \mathbb E_{\mathcal L_n}[\|(\pry - \widehat g_n(\prz))^2\Sigma_n(\prz)\|^2] \\
				&\quad= \sup_{n}\ \mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))^4\|\Sigma_n(\prz)\|^2] \\
				&\quad\leq \sup_{n}\ \mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))^4\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^2|\prz]^2] \\
				&\quad\leq \sup_{n}\ \mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))^4\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4|\prz]] \\
				&\quad \leq  c_2 < \infty.
			\end{split}
			\label{eq:eighth-moment-calculation}
		\end{equation} 
		Therefore, by the WLLN we obtain the convergence
		\begin{equation}
			\widehat S_n^{2} - S_n^2 =  \frac{1}{n}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i}))^2\Sigma_n(\srz_i) - \mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))^2\Sigma_n(\prz)]\overset{\mathcal L_n}\rightarrow_p 0.
		\end{equation}
		
		To show the second convergence~\eqref{eq:conv-s-neg-1}, note first that 
		\begin{equation}
			\begin{split}
				\sup_n\ \|S_n^2\| &= \sup_n\ \|\mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))^2\Sigma_n(\prz)]\| \\
				&\leq  \sup_n\ \mathbb E_{\mathcal L_n}[\|(\pry - \widehat g_n(\prz))^2\Sigma_n(\prz)\|^2]^{1/2} \leq c_2^{1/2},
			\end{split}
		\end{equation}
		the last step having been derived in equation~\eqref{eq:eighth-moment-calculation}. Therefore, for every $n$, we have
		\begin{equation}
			S_n^2 \in \mathcal S \equiv \{S^2: \|S^{-1}\| \leq c_1, \|S^2\| \leq c_2^{1/2}\}.
		\end{equation}
		Since $\mathcal S$ is a compact subset of the open set of positive definite matrices, there exists a $\delta > 0$ such that
		$\mathcal S_\delta = \{S^2: \|S^2-S_0^2\| \leq \delta \text{ for some } S_0^2 \in \mathcal S\}$ is also a compact subset of the set of positive definite matrices. Since the function $S^2 \mapsto S^{-1}$ is continuous on the compact set $\mathcal S_\delta$, it must be uniformly continuous on this set as well. Fix $\gamma > 0$. By uniform continuity, there exists an $\eta > 0$ such that $\|S^2_1 - S^2_2\| \leq \eta$ implies that $\|S^{-1}_1-S^{-1}_2\| \leq \gamma$ for all $S_1^2, S_2^2 \in \mathcal S_\delta$. We therefore have that
		\begin{equation*}
			\begin{split}
				\mathbb P_{\mathcal L_n}[\|\widehat S_n^{-1} - S_n^{-1}\| > \gamma] &= \mathbb P_{\mathcal L_n}[\|\widehat S_n^{-1} - S_n^{-1}\| > \gamma, \widehat S_n^2 \in \mathcal S_\delta] +  \mathbb P_{\mathcal L_n}[\|\widehat S_n^{-1} - S_n^{-1}\| > \gamma, \widehat S_n^2 \not \in \mathcal S_\delta] \\
				&\leq \mathbb P_{\mathcal L_n}[\|\widehat S_n^{2} - S_n^{2}\| > \eta] +  \mathbb P_{\mathcal L_n}[\widehat S_n^2 \not \in \mathcal S_\delta] \\
				&\leq \mathbb P_{\mathcal L_n}[\|\widehat S_n^{2} - S_n^{2}\| > \eta] +  \mathbb P_{\mathcal L_n}[\|\widehat S_n^2-S_n^2\| > \delta].
			\end{split}
		\end{equation*}
		Using the convergence~\eqref{eq:conv-s-2}, we find that the last expression tends to zero as $n \rightarrow \infty$, from which it follows that $\mathbb P_{\mathcal L_n}[\|\widehat S_n^{-1} - S_n^{-1}\| > \gamma]  \rightarrow 0$ as $n \rightarrow \infty$. Therefore,
		\begin{equation*}
			\widehat S_n^{-1} - S_n^{-1}  \overset{\mathcal L_n}\rightarrow_p 0.
		\end{equation*}
		Multiplying this relation on the right by the bounded quantity $S_n$, we arrive at the statement~\eqref{eq:conv-s-neg-1}, which concludes the proof.
	\end{proof}
	
\end{lemma}

\begin{lemma}\label{lem:clt}
	Consider generating $(\srxk^1, \srxk^2, \sry, \srz)$ according to~\eqref{eq:two-resamples} for a sequence of laws
	\begin{equation}
		\mathcal L_n \in \mathscr L^{\textnormal{MX(2)}}(\mu_n(\cdot), \Sigma_n(\cdot)) \cap \mathscr L_n(c_1, c_2).
	\end{equation}
	We have 
	\begin{equation}
		n^{-1/2}
		\begin{pmatrix}
			S_n^{-1} & 0 \\
			0 & S_n^{-1}
		\end{pmatrix}\sum_{i = 1}^n (\sry_{i} - \widehat g_n(\srz_{i})){\srxk^1_{i} - \mu_n(\srz_i) \choose \srxk^2_{i} - \mu_n(\srz_i)} \overset{\mathcal L_n}\rightarrow_d N\left({0 \choose 0}, \begin{pmatrix}
			I_d & 0 \\
			0 & I_d
		\end{pmatrix}
		\right)
		\label{convergence-1}
	\end{equation}
	and
	\begin{equation}
		{U_n(\srxk^1, \sry, \srz) \choose U_n(\srxk^2, \sry, \srz)} \overset{\mathcal L_n}\rightarrow_d N\left({0 \choose 0},
		\begin{pmatrix}
			I_d & 0 \\
			0 & I_d
		\end{pmatrix}
		\right).
		\label{convergence-2}
	\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:clt}]
	
	Note that
	\begin{equation*}
		{U_n(\srxk^1, \sry, \srz) \choose U_n(\srxk^2, \sry, \srz)} = \begin{pmatrix}
			\widehat S_n^{-1}S_n & 0 \\
			0 & \widehat S_n^{-1}S_n
		\end{pmatrix} \cdot n^{-1/2}
		\begin{pmatrix}
			S_n^{-1} & 0 \\
			0 & S_n^{-1}
		\end{pmatrix}\sum_{i = 1}^n(\sry_{i} - \widehat g_n(\srz_{i})) {\srxk^1_{i} - \mu_n(\srz_i) \choose \srxk^2_{i} - \mu_n(\srz_i)}.
	\end{equation*}
	By Lemma~\ref{lem:aux}, we have that $\widehat S_n^{-1}S_n \overset{\mathcal L_n}\rightarrow_p I_d$, so by Slutsky we find that the second statement~\eqref{convergence-2} follows from the first~\eqref{convergence-1}. Therefore, it suffices to prove the latter convergence. To this end, we apply the CLT to the triangular array of vectors
	\begin{equation}
		\left\{ (\sry_{i} - \widehat g_n(\srz_{i}))\begin{pmatrix}
			S_n^{-1} & 0 \\
			0 & S_n^{-1}
		\end{pmatrix}{\srxk^1_{i} - \mu_n(\srz_i) \choose \srxk^2_{i} - \mu_n(\srz_i)}\right\}_{i,n}. 
	\end{equation}
	To apply the CLT, we first verify the Lyapunov condition with $\delta = 1$: 
	\begin{equation}
		\begin{split}
			&\sup_{n}\ \mathbb E_{\mathcal L_n}\left[\left\|(\pry- \widehat g_n(\prz))
			\begin{pmatrix}
				S_n^{-1} & 0 \\
				0 & S_n^{-1}
			\end{pmatrix}{\prxk^1 - \mu_n(\prz) \choose \prxk^2 - \mu_n(\prz)}\right\|^3\right] \\
			&\quad \leq \sup_{n}\ \|S_n^{-1}\|^3 \mathbb E_{\mathcal L_n}\left[|\pry- \widehat g_n(\prz)|^3(\|\prxk^1 - \mu_n(\prz)\|^2 + \|\prxk^2 - \mu_n(\prz)\|^2)^{3/2}\right] \\
			&\quad \leq \sup_{n}\ \|S_n^{-1}\|^3 \mathbb E_{\mathcal L_n}\left[|\pry- \widehat g_n(\prz)|^3C\left(\|\prxk^1 - \mu_n(\prz)\|^3 + \|\prxk^2 - \mu_n(\prz)\|^3\right)\right] \\
			&\quad = 2C\sup_{n}\ \|S_n^{-1}\|^3 \mathbb E_{\mathcal L_n}\left[|\pry- \widehat g_n(\prz)|^3\|\prxk^1 - \mu_n(\prz)\|^3\right] \\
			&\quad \leq 2C\sup_{n}\ \|S_n^{-1}\|^3 \mathbb E_{\mathcal L_n}\left[(\pry- \widehat g_n(\prz))^4\|\prxk^1 - \mu_n(\prz)\|^4\right]^{3/4} \\
			&\quad = 2C\sup_{n}\ \|S_n^{-1}\|^3 \mathbb E_{\mathcal L_n}\left[(\pry- \widehat g_n(\prz))^4\mathbb E_{\mathcal L_n}[\|\prxk^1 - \mu_n(\prz)\|^4|\prz]\right]^{3/4} \\
			&\quad = 2C\sup_{n}\ \|S_n^{-1}\|^3 \mathbb E_{\mathcal L_n}\left[(\pry- \widehat g_n(\prz))^4\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4|\prz]\right]^{3/4} \\
			&\quad \leq 2C c_1^3 c_2^{3/4} < \infty.
		\end{split}
	\end{equation}
	Here $C$ is chosen such that $(a+b)^{3/2} \leq C(a^{3/2} + b^{3/2})$ for all $a,b \geq 0$.
	Next, it is easy to verify that
	\begin{equation}
		\mathbb E_{\mathcal L_n}\left[(\pry - \widehat g_n(\prz))\begin{pmatrix}
			S_n^{-1} & 0 \\
			0 & S_n^{-1}
		\end{pmatrix}{\prxk^1 - \mu_n(\prz) \choose \prxk^2 - \mu_n(\prz)}\right] = {0 \choose 0}
	\end{equation}
	and
	\begin{equation}
		\text{Var}_{\mathcal L_n}\left[(\pry - \widehat g_n(\prz))\begin{pmatrix}
			S_n^{-1} & 0 \\
			0 & S_n^{-1}
		\end{pmatrix}{\prxk^1 - \mu_n(\prz) \choose \prxk^2 - \mu_n(\prz)}\right] = \begin{pmatrix}
			I_d & 0 \\
			0 & I_d
		\end{pmatrix}.
	\end{equation}
	By the CLT, the convergence~\eqref{convergence-1} now follows.
\end{proof}



\begin{lemma} \label{lem:fourth-moment}
	In the setting of Theorem~\ref{thm:power}, define
	\begin{equation}
		\pry' \equiv g_n(\prz) + \peps, \quad S'^2_{n} \equiv \mathbb E_{\mathcal L_n}[(\pry' - \widehat g_n(\prz))^2 \Sigma_n(\prz)], \quad \text{and} \quad \mathcal L'_n \equiv \mathcal L_n(\prx, \pry', \prz).
		\label{prime-definitions}
	\end{equation}
	Under the assumptions of Theorem~\ref{thm:power}(a) or~\ref{thm:power}(b), 
	\begin{equation}
		\text{there exist } c_1, c_2 > 0 \text{ such that } \mathcal L_n, \mathcal L'_n \in \mathscr L(c_1, c_2).
		\label{eighth-moment}
	\end{equation} 
	Under the assumptions of Theorem~\ref{thm:power}(a), we have
	\begin{equation}
	\inf_{n}\ \lambda_{\min}(S_n^{-1}) > 0,
	\label{eq:s-n-2-limit-a}
	\end{equation}
while under the assumptions of Theorem~\ref{thm:power}(b), we have
	\begin{equation}
		\lim_{n \rightarrow \infty} S_n^2 = \lim_{n \rightarrow \infty}  S'^2_n = \overline \Sigma^{1/2}(\sigma^2 I_d + \mathcal E^2)\overline \Sigma^{1/2}.
		\label{eq:s-n-2-limit}
	\end{equation}
	
	%\begin{equation}
	%	\sup_n\ \mathbb E_{\mathcal L_n}[\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4|\prz](\pry - \widehat g_n(\prz))^4] < \infty,
	%			\label{eq:eighth-moment-1}
	%\end{equation}
	%\begin{equation}
	%	\sup_n\ \mathbb E_{\mathcal L_n}[\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4|\prz](\pry' - \widehat g_n(\prz))^4] < \infty,
	%			\label{eq:eighth-moment-2}
	%\end{equation}
\end{lemma}
\begin{proof}
First, we show that under the assumptions of Theorem~\ref{thm:power}(a) or~\ref{thm:power}(b), we have $\mathcal L_n \in \mathscr L(c_1, c_2)$ for some $c_1, c_2 > 0$. It suffices to show that
\begin{equation}
\sup_{n} \|S_n^{-1}\| < \infty
\label{eq:bounded-inverse}
\end{equation}
and
\begin{equation}
\sup_n\ \mathbb E_{\mathcal L_n}\left[(\pry - \widehat g_n(\prz))^{4} \mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{4}|\prz]\right] < \infty.
\label{eq:bounded-eighth-moment}
\end{equation}
To show the statement~\eqref{eq:bounded-inverse}, first note that
\begin{equation}
	\begin{split}
		S_n^2 &= \mathbb E_{\mathcal L_n}[(\pry - \widehat g_n(\prz))^2 \Sigma_n(\prz)] \\
		&=   \mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T \beta_n + \pry'-\widehat g_n(\prz))^2 \Sigma_n(\prz)] \\
		&= \mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T \beta_n)^2\Sigma_n(\prz)]  \\
		&\quad+2\mathbb E_{\mathcal L_n}[(\prx - \mu_n(\prz))^T \beta_n( \pry'-\widehat g_n(\prz))\Sigma_n(\prz)] \\
		&\quad + \mathbb E_{\mathcal L_n}[( \pry'-\widehat g_n(\prz))^2 \Sigma_n(\prz)] \\
		&= \mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T \beta_n)^2\Sigma_n(\prz)] + \mathbb E_{\mathcal L_n}[(\pry'-\widehat g_n(\prz))^2 \Sigma_n(\prz)],
		\label{eq:s-n-2}
	\end{split}
\end{equation}
where in the last step we used the fact that
\begin{equation*}
	\begin{split}
		&\mathbb E_{\mathcal L_n}[(\prx - \mu_n(\prz))^T \beta_n( \pry'-\widehat g_n(\prz))\Sigma_n(\prz)] \\
		&\quad=\mathbb E_{\mathcal L_n}[\mathbb E_{\mathcal L_n}[(\prx - \mu_n(\prz))|\prz]^T \beta_n( g_n(\prz)-\widehat g_n(\prz))\Sigma_n(\prz)] = 0. \\
	\end{split}
\end{equation*}
Furthermore, 
\begin{equation}
	\begin{split}
		S'^2_n &= \mathbb E_{\mathcal L_n}[(\pry'-\widehat g_n(\prz))^2 \Sigma_n(\prz)] \\
		&=\mathbb E_{\mathcal L_n}[( \peps + g_n(\prz)-\widehat g_n(\prz))^2 \Sigma_n(\prz)] \\
		& = \mathbb E_{\mathcal L_n}[\peps ^2 \Sigma_n(\prz)] + \mathbb E_{\mathcal L_n}[2\peps(g_n(\prz)-\widehat g_n(\prz))\Sigma_n(\prz)]  \\
		&\quad + \mathbb E_{\mathcal L_n}[(g_n(\prz)-\widehat g_n(\prz))^2\Sigma_n(\prz)] \\
		&= \sigma^2 \overline \Sigma_n + \overline \Sigma_n^{1/2}\mathcal E^2_n \overline \Sigma_n^{1/2}  = \overline \Sigma_n^{1/2}(\sigma^2 I_d + \mathcal E^2_n) \overline \Sigma_n^{1/2}.
		\label{eq:s-n-2-prime}
	\end{split}
\end{equation}
It follows that $S_n^2 \succcurlyeq \sigma^2 \overline \Sigma_n$, which together with assumption~\eqref{eq:s-n-inverse-assump} implies that
\begin{equation}
\sup_n \|S_n^{-1}\| \leq \sup_n \|\sigma^{-1}\overline \Sigma_n^{-1/2}\| = \sigma^{-1}\left(\sup_n \|\overline \Sigma_n^{-1}\|\right)^{1/2} < \infty.
\end{equation}
This verifies statement~\eqref{eq:bounded-inverse}.	To prove statement~\eqref{eq:bounded-eighth-moment}, we write
	\begin{equation*}
		\begin{split}
			&\mathbb E_{\mathcal L_n}\left[ (\pry - \widehat g_n(\prz))^{4}\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{4}|\prz]\right] \\
			&\quad= \mathbb E_{\mathcal L_n}\left[ ((\prx - \mu_n(\prz))^T \beta_n + \peps + g_n(\prz)-\widehat g_n(\prz))^4\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{4}|\prz]\right] \\
			&\quad\leq C\mathbb E_{\mathcal L_n}\left[(((\prx - \mu_n(\prz))^T \beta_n)^4 + \peps^4 + (g_n(\prz)-\widehat g_n(\prz))^4) \mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{4}|\prz]\right] \\
			&\quad\leq C\|\beta_n\|^4\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{8}] + 3C\sigma^4\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^{4}] \\
			&\quad \quad + C \mathbb E_{\mathcal L_n}\left[(g_n(\prz) - \widehat g_n(\prz))^{4} \|\prx - \mu_n(\prz)\|^{4}\right].
		\end{split}
	\end{equation*}
	Here, $C$ a constant such that $(a + b + c)^4 \leq C(a^4 + b^4 + c^4)$ for all $a,b,c \geq 0$. Taking a supremum over $n$ and using the moment assumptions~\eqref{eq:eighth-moment-assump-1} and \eqref{eq:eighth-moment-assump-2} along with the boundedness of the sequence $\beta_n$ yields the statement~\eqref{eq:bounded-eighth-moment}. 
	
	Therefore, we have verified that $\mathcal L_n \in \mathscr L(c_1, c_2)$ for some $c_1, c_2$ under the assumptions of Theorem~\ref{thm:power}(a) or~\ref{thm:power}(b). The fact that $\mathcal L'_n \in \mathscr L(c_1, c_2)$ under these assumptions follows by a similar argument (omitted for the sake of brevity), which finishes the proof of statement \eqref{eighth-moment}.
	
	Next, we turn to proving the claim~\eqref{eq:s-n-2-limit-a}. Using calculations~\eqref{eq:s-n-2} and~\eqref{eq:s-n-2-prime}, we write
	\begin{equation}
	S_n^2 = \mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T \beta)^2\Sigma_n(\prz)] + \overline \Sigma_n^{1/2}(\sigma^2 I_d + \mathcal E^2_n) \overline \Sigma_n^{1/2}.
\end{equation}
	Note that 
	\begin{equation}
		\begin{split}
			&\sup_n\ \|\mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T \beta)^2\Sigma_n(\prz)]\| \\
			&\quad\leq \sup_n\ \|\beta\|^2\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^2\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^2|\prz]] \\
			&\quad= \sup_n\ \|\beta\|^2\mathbb E_{\mathcal L_n}[\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^2|\prz]^2] \\
			&\quad\leq \sup_n\ \|\beta\|^2\mathbb E_{\mathcal L_n}[\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4|\prz]] \\
			&\quad\leq \sup_n\ \|\beta\|^2\mathbb E_{\mathcal L_n}[\|\prx - \mu_n(\prz)\|^4] < \infty, \\
			\label{eq:boundedness}
		\end{split}
	\end{equation}
	the last step using the eighth moment bound~\eqref{eq:eighth-moment-assump-1}. Furthermore, 
	\begin{equation}
	\sup_n \ \|\overline \Sigma_n^{1/2}(\sigma^2 I_d + \mathcal E^2_n) \overline \Sigma_n^{1/2}\| < \infty
	\end{equation}
	because $\overline \Sigma_n^{1/2}(\sigma^2 I_d + \mathcal E^2_n) \overline \Sigma_n^{1/2}$ is a convergent sequence by assumption. Hence, $\sup_n \|S_n^2\| < \infty$ and therefore
	\begin{equation*}
	\inf_n \lambda_{\min}(S_n^{-1}) = \inf_n \|S_n\|^{-1} = \inf_n \|S_n^2\|^{-1/2} = \left(\sup_n \|S_n^2\|\right)^{-1/2} > 0.
	\end{equation*}
	This completes the proof of claim~\eqref{eq:s-n-2-limit-a}.	
	
	Finally, we turn to proving claim~\eqref{eq:s-n-2-limit}. The claimed convergence of $S'^2_n$ follows immediately from the derivation~\eqref{eq:s-n-2-prime} and the assumption~\eqref{eq:limits}. To show that $S_n^2$ has the same limit, note that the derivation~\eqref{eq:s-n-2} implies that 
	\begin{equation*}
		S_n^2 - S'^2_n	= \mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T \beta_n)^2\Sigma_n(\prz)] = \frac{1}{n}\mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T h_n)^2\Sigma_n(\prz)].
	\end{equation*}
	The boundedness of the quantity $\mathbb E_{\mathcal L_n}[((\prx - \mu_n(\prz))^T h_n)^2\Sigma_n(\prz)]$ follows by an argument analagous to that in equation~\eqref{eq:boundedness}, which shows that
	\begin{equation*}
		S_n^2 - S'^2_n \rightarrow 0.
	\end{equation*}	
	This completes the proof of statement~\eqref{eq:s-n-2-limit}, so we are done.
\end{proof}

%\section{Proofs for Section 6 \textcolor{red}{Are we keeping these?}}
%\label{sup-sec:proofs-effect-size}
%
%Here, we prove the consistency of $\widehat \rho_n$ and $\widehat \beta_n$.
%
%\begin{proof}[Proof of Theorem~\ref{prop:nonparametric-consistency}]
%	
%	%	Both these propositions are consistency statements. We start by proving that Proposition~\ref{prop:nonparametric-consistency} implies Proposition~\ref{prop:consistency}. To show this, it suffices to prove that the moment conditions~\eqref{moment-conditions} imply the condition~\eqref{general-moment-condition}. Indeed, if $\pry = \prx \beta + g(\prz) + \peps$, we have
%	%	\begin{equation*}
%	%	\begin{split}
%	%	&\mathbb E_{\mathcal L}\left[(\pry - \widehat g_n(\prz))^{2}\|\prx - \mu_n(\prz)\|^{2}\right] \\
%	%	&\quad= \mathbb E\left[(\prx \beta + \peps + g(\prz)  - \widehat g_n(\prz))^{2}\|\prx - \mu_n(\prz)\|^{2}\right] \\
%	%	&\quad\leq 3\mathbb E\left[(\prx \beta)^2\|\prx - \mu_n(\prz)\|^{2} + \peps^2\|\prx - \mu_n(\prz)\|^{2} + (g(\prz)  - \widehat g_n(\prz))^{2}\|\prx - \mu_n(\prz)\|^{2}\right] \\
%	%	&< \infty,
%	%	\end{split}
%	%	\end{equation*}
%	%	where we have used the fourth moment assumption to bound the first term, where the finiteness of the expectation stated in the last line follows from the moment assumptions~\eqref{moment-conditions}.
%	
%	We start by proving the consistency of $\widehat \rho_n$, which we write as the sum of two terms:
%	\begin{equation*}
%	\begin{split}
%	\widehat \rho_n &= \frac{1}{n}\sum_{i = 1}^n \underbrace{(\srx_{i} - \mu_{i}) (\sry_{i} - \widehat g_n(\srz_{i})) - \mathbb E[\srx_{i} - \mu_{i}|\sry_{i}, \srz_{i}] (\sry_{i} - \widehat g_n(\srz_{i}))}_{\equiv~B_{i} \in \mathbb{R}^d} \\
%	&\quad \quad + \frac{1}{n}\sum_{i = 1}^n \underbrace{\mathbb E[\srx_{i} - \mu_{i}|\sry_{i}, \srz_{i}] (\sry_{i} - \widehat g_n(\srz_{i}))}_{\equiv~C_{i} \in \mathbb{R}^d} 
%	% \\
%	% &\equiv  \Sigma^{-1}\frac{1}{n}\sum_{i = 1}^n B_{i} + \Sigma^{-1}\frac{1}{n}\sum_{i = 1}^n C_{i}.
%	\end{split}
%	\end{equation*}
%	We claim that 
%	\begin{equation}
%	\mathcal L\left(\left.\frac{1}{n}\sum_{i = 1}^n B_{i} \right| \sry_{(n)}, \srz_{(n)}\right)  \rightarrow 0 \quad \text{and} \quad \frac1n\sum_{i = 1}^n C_{i} \rightarrow \rho, \quad \text{a.s. in } \{\sry_{(n)}, \srz_{(n)}\}_{n \geq 1},
%	\label{B-C-convergence}
%	\end{equation}
%	from which the conclusion will follow by Slutsky's theorem. To show the convergence in probability, we apply the WLLN to the mean zero, independent, but not identically distributed sequence $B_{i}$. We first verify the second moment condition. To do so, we compute that
%	\begin{equation*}
%	\begin{split}
%	\frac{1}{n}\sum_{i = 1}^n\text{Var}[B_{i}|\sry_{i}, \srz_{i}]  &= \frac{1}{n}\sum_{i = 1}^n \text{Var}[(\prx - \mu_n(\prz)) (\pry - \widehat g_n(\prz))|\pry = \sry_{i}, \prz = \srz_{i}] \\
%	&\overset{\text{a.s}}\rightarrow \mathbb E\left[\text{Var}[(\prx - \mu_n(\prz)) (\pry - \widehat g_n(\prz))|\pry,\prz]\right] \\
%	&\leq \mathbb E\left[\|\prx - \mu_n(\prz)\|^2|\pry - \widehat g_n(\prz)|^2\right] \\
%	&< \infty,
%	\end{split}
%	\end{equation*}
%	which follows from the SLLN~\eqref{SLLN} and the moment condition~\eqref{general-moment-condition}. This allows us to verify the WLLN second moment condition~\eqref{WLLN-condition}:
%	\begin{equation*}
%	\frac{1}{n^2}\sum_{i = 1}^{n}\text{Var}[B_{i}|\sry_i,\srz_i]\overset{\text{a.s}}\rightarrow 0.
%	\end{equation*}
%	Therefore, the WLLN~\eqref{WLLN} yields
%	\begin{equation*}
%	\begin{split}
%	\mathcal L\left(\left.\frac{1}{n}\sum_{i = 1}^n B_{i} \right| \sry_{(n)}, \srz_{(n)}\right)  \rightarrow 0.
%	\end{split}
%	\end{equation*}
%	Next, we apply the SLLN~\eqref{SLLN} to the i.i.d. sequence $C_i$:
%	\begin{equation*}
%	\begin{split}
%	\frac1n\sum_{i = 1}^n C_{i} &= \frac1n\sum_{i = 1}^n  \mathbb E[\srx_{i} - \smu_{i}|\sry_i, \srz_{i}] (\sry_i- \widehat g_n(\srz_{i})) \\
%	&\overset{\text{a.s.}}\rightarrow \mathbb E[\mathbb E[\prx - \mu_n(\prz)|\pry, \prz](\pry - \widehat g_n(\prz))] \\
%	&=\mathbb E[\mathbb E[(\prx - \mu_n(\prz))(\pry - \widehat g_n(\prz))|\prz]] \\
%	&=\mathbb E[\mathbb E[(\prx - \mu_n(\prz))(\pry - \mathbb E[\pry|\prz])|\prz]] \\
%	& = \rho.
%	\end{split}
%	\end{equation*}
%	This verifies statement~\eqref{B-C-convergence}, thus implying that $\mathcal L(\widehat \rho_n | \sry_{(n)}, \srz_{(n)}) \rightarrow \rho(\mathcal L)$. By the dominated convergence theorem, we get the unconditional consistency statement as well:
%	\begin{equation*}
%	\begin{split}
%	\lim_{n \rightarrow \infty}\mathbb P_{\mathcal L}\left[|\widehat \rho_n - \widehat \rho| > \eps\right] &= \lim_{n \rightarrow \infty}\mathbb E_{\mathcal L}\left[\mathbb P_{\mathcal L}\left[|\widehat \rho_n - \widehat \rho| > \eps | \sry_{(n)}, \srz_{(n)}\right]\right] \\
%	&= \mathbb E_{\mathcal L}\left[\lim_{n \rightarrow \infty}\mathbb P_{\mathcal L}\left[|\widehat \rho_n - \widehat \rho| > \eps | \sry_{(n)}, \srz_{(n)}\right]\right] = 0.
%	\end{split}
%	\end{equation*}
%	The corresponding conditional and unconditional consistency statements for $\beta(\mathcal L)$ follow from the fact that $\widehat \Sigma_n \overset{\text{a.s.}}\rightarrow \overline \Sigma$ (by the SLLN) and Slutsky's theorem.
%\end{proof}

%{\color{red}
%	
%	First, we prove the unconditional asymptotic normality of the estimator $\widehat \beta_n$ in the nonparametric case.
%	\begin{proof}[Proof of Proposition~\ref{prop:nonparametric-normality}]
%		
%		First, note that
%		\begin{equation*}
%		\widehat \beta_n = \frac{1}{n}\sum_{i = 1}^n D_i,
%		\end{equation*}
%		where $D_i$ are i.i.d. with 
%		\begin{equation*}
%		\mathbb E[\bm D] = \mathbb E[\Sigma^{-1}(\prx - \mu_n(\prz))^T(\pry - \widehat g_n(\prz))] = \beta(\mathcal L).
%		\end{equation*}
%		Furthermore, $\text{Var}[\bm D] = \widetilde S^2 < \infty$ by the moment condition~\eqref{general-moment-condition}. Given these observations, the conclusion follows by standard i.i.d. theory; we omit the details for the sake of brevity.
%		%First, we claim that
%		%\begin{equation}
%		%\sqrt{n}(\widehat \beta_n - \beta(\mathcal L)) \overset{\mathcal L} \rightarrow N(0, \widetilde S^2); \quad \widetilde S^2 \equiv \text{Var}[\Sigma^{-1}(\prx - \mu_n(\prz))^T(\pry - \widehat g_n(\prz))].
%		%\end{equation}
%		%We apply the standard i.i.d. central limit theorem to show this result. 
%	\end{proof}
%}
\section{Proofs for Section 7} \label{sec:knockoffs-proofs}

\begin{proof}[Proof of Theorem~\ref{prop:knockoff-optimality}]
	% We claim that an even stronger version of the statement~\eqref{conditional-optimality-knockoffs} holds:
	% \begin{equation}
	% T^{\textnormal{opt}} \in \underset{T}{\arg \max}\ \mathbb P[T(\srx, \srxk; \sry, \srz) > T(\srxk, \srx; \sry, \srz)\ |\ \{\srx, \srxk\}, \sry, \srz],
	% \label{conditional-optimality-knockoffs-stronger}
	% \end{equation}
	% where the extra conditioning is on . 
	Let us denote
	\begin{equation*}
	[\srx, \srxk]_{?} \equiv (\{\srx_j, \srxk_j\}, \srx_{-j}, \srxk_{-j}),
	\end{equation*}
	where $\{\srx_j, \srxk_j\}$ represents the \textit{unordered} pair. In other words, $[\srx, \srxk]_{?}$ specifies $[\srx, \srxk]$ up to a swap, hence the ``?" notation: 
	\begin{equation*}
	[\srx, \srxk]_{?} = [\sfx, \sfxk]_{?} \quad \Longleftrightarrow \quad [\srx, \srxk] \in \{[\sfx, \sfxk], [\sfx, \sfxk]_{\text{swap}(j)}\}.
	\end{equation*}
	With this notation, we claim that
	\begin{equation}
	T_j^{\textnormal{opt}} \in \underset{T_j}{\arg \max}\ \mathbb P\left[\left. T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)\ \right|\ [\srx, \srxk]_? = [\sfx, \sfxk]_?, \sry = \sfy\right] 
	\label{knockoff-conditional-optimality}
	\end{equation}
	for every $([\sfx, \sfxk], \sfy)$ in the set 
	\begin{equation}
	\mathcal A \equiv \left\{([\sfx, \sfxk], \sfy): T^{\text{opt}}_j([\sfx, \sfxk], \sfy) \neq T^{\text{opt}}_j([\sfx, \sfxk]_{\text{swap}(j)}, \sfy)\right\}. 
	\label{nondegeneracy}
	\end{equation}
	The conclusion~\eqref{unconditional-knockoff-optimality} will follow because for any $T_j$, 
	\begin{equation*}
	\begin{split}
	&\mathbb P[T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)] \\
	&= \mathbb P[T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry), \srx_j \neq \srxk_j] \\
	&= \mathbb P[T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry), ([\srx, \srxk], \sry) \in \mathcal A] \\
	&= \mathbb P\left[\left.T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)\right| ([\srx, \srxk], \sry) \in \mathcal A\right]\mathbb P[([\srx, \srxk], \sry) \in \mathcal A] \\
	&= \mathbb E\left[\left.\mathbb P\left[\left.T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)\right| [\srx, \srxk]_?, \sry\right]\right|([\srx, \srxk], \sry) \in \mathcal A\right]\mathbb P[([\srx, \srxk], \sry) \in \mathcal A] \\
	&\leq \mathbb E\left[\left.\mathbb P\left[\left.T^{\text{opt}}_j([\srx, \srxk], \sry) > T^{\text{opt}}_j([\srx, \srxk]_{\text{swap}(j)}, \sry)\right| [\srx, \srxk]_?, \sry\right]\right|([\srx, \srxk], \sry) \in \mathcal A\right]\mathbb P[([\srx, \srxk], \sry) \in \mathcal A] \\
	&=\mathbb P\left[T^{\text{opt}}_j([\srx, \srxk], \sry) > T^{\text{opt}}_j([\srx, \srxk]_{\text{swap}(j)}, \sry)\right].
	\end{split}
	\end{equation*}
	The first step holds because $T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)$ implies that $\srx_j \neq \srxk_j$, the second by the assumption~\eqref{nondegeneracy-assumption}, the third and fourth by probability manipulations, the fifth by the claimed conditional optimality~\eqref{knockoff-conditional-optimality}, and the sixth by the same logic as the first four steps.
	
	
	To prove equation~\eqref{knockoff-conditional-optimality}, fix $([\sfx, \sfxk], \sfy) \in \mathcal A$. Consider the simple hypothesis testing problem
	\begin{equation}
	H_0: (\srx_j, \srxk_j) = (\sfxk_j, \sfx_j) \quad \text{versus} \quad H_1: (\srx_j, \srxk_j) = (\sfx_j, \sfxk_j),
	\label{knockoffs-simple}
	\end{equation}
	where $(\srx_j, \srxk_j)$ are endowed with their law conditional on 
	\begin{equation*}
	([\srx, \srxk]_?, \sry) = ([\sfx, \sfxk]_?, \sfy).
	\end{equation*}
	We seek the most powerful test of level $\alpha = 1/2$. Note that under the null distribution, the knockoff exchangeability property makes both events equally likely: $\mathbb P_0[(\srx_j, \srxk_j) = (\sfx_j, \sfxk_j)] = \mathbb P_0[(\srx_j, \srxk_j) = (\sfxk_j, \sfx_j)] = 1/2$. Therefore, given any statistic $T_j$, the level 1/2 test of the simple hypothesis~\eqref{knockoffs-simple} rejects when $T_j([\srx, \srxk], \sry) > T_j([\srx, \srxk]_{\text{swap}(j)}, \sry)$. The optimal knockoff statistic $T^{\text{opt}}$ defined in equation~\eqref{knockoff-conditional-optimality} thus coincides with the most powerful test for the hypothesis~\eqref{knockoffs-simple}, which by Neyman-Pearson is given by
	\begin{equation*}
	\begin{split}
	&T^{\text{opt}}_j([\sfx, \sfxk], \sfy) \\
	&\quad= \frac{\mathbb P\left[\left.(\srx_j, \srxk_j) = (\sfx_j, \sfxk_j)\right|[\srx, \srxk]_? = [\sfx, \sfxk]_?, \sry = \sfy\right]}{\mathbb P\left[\left.(\srx_j, \srxk_j) = (\sfxk_j, \sfx_j)\right|[\srx, \srxk]_? = [\sfx, \sfxk]_?, \sry = \sfy\right]} \\
	&\quad= \frac{\mathbb P\left[\left.(\srx_j, \srxk_j) = (\sfx_j, \sfxk_j)\right| [\srx, \srxk]_? = [\sfx, \sfxk]_?\right]\mathbb P\left[\sry = \sfy\left|[\srx, \srxk] = [\sfx,\sfxk]\right.\right]}{\mathbb P\left[\left.(\srx_j, \srxk_j) = (\sfxk_j, \sfx_j)\right| [\srx, \srxk]_? = [\sfx, \sfxk]_?\right]\mathbb P\left[\sry = \sfy\left|[\srx, \srxk] = [\sfx,\sfxk]_{\text{swap}(j)}\right.\right]} \\
	&\quad= \frac{\mathbb P\left[\sry = \sfy\left|[\srx, \srxk] = [\sfx,\sfxk]\right.\right]}{\mathbb P\left[\sry = \sfy\left|[\srx, \srxk] = [\sfx,\sfxk]_{\text{swap}(j)}\right.\right]} = \frac{\mathbb P\left[\sry = \sfy\left|\srx_j = \sfx_j, \srx_{-j} = \sfx_{-j}\right.\right]}{\mathbb P\left[\sry = \sfy\left|\srx_j = \sfxk_j, \srx_{-j} = \sfx_{-j}\right.\right]}.
	\end{split}
	\end{equation*}
	The first step is given by Neyman-Pearson, the second by an application of Bayes rule, the third by the conditional exchangeability of knockoffs~\eqref{conditional-exchangeability}, and the last by the conditional independence of knockoffs~\eqref{knockoff-conditional-independence}. Finally, it is easy to verify that
	\begin{equation*}
	\begin{split}
	&T_j^{\text{opt}}([\srx, \srxk], \sry) > T_j^{\text{opt}}([\srx, \srxk]_{\text{swap}(j)}, \sry) \quad \Longleftrightarrow \\
	& \mathbb P[\sry = \sfy|\srx_j = \sfx_j, \srx_{-j} = \sfx_{-j}] >  \mathbb P[\sry = \sfy|\srx_j = \sfxk_j, \srx_{-j} = \sfx_{-j}],
	\end{split}
	\end{equation*}
	from which we conclude that the likelihood given in equation~\eqref{log-likelihood-ratio-knockoffs} is optimal for the problem~\eqref{conditional-optimality}. This completes the proof.
\end{proof}

%\begin{proposition}\label{prop:nondegeneracy-crt}
%	Suppose $\bar{\mathcal L}(\pry|\prx, \prz) = g_{\bm\eta}$, where $\bm \eta = \prx^T \beta + g(\prz)$ and $g_\eta$ is a one-dimensional exponential family with natural parameter $\eta$ and strictly convex log partition function $\psi$. Suppose also that $\beta_j \neq 0$. Consider the following two conditions. 
%	\begin{enumerate}
%		\item $\prx|\prz$ has a density with respect to the Lebesgue measure for each $\prz$;
%		\item $g_\eta$ has a density with respect to Lebesgue measure.
%	\end{enumerate}
%	If either of these two conditions holds, then 
%	\begin{equation*}
%	\mathbb P_{\bar{\mathcal L}}[T^{\textnormal{opt}}(\srx, \sry, \srz) = C_\alpha(\sry,\srz) \mid \sry,\srz] = 0,
%	\end{equation*}
%	i.e. the most powerful CRT is deterministic.
%\end{proposition}

\begin{proof}[Proof of Proposition~\ref{prop:nondegeneracy-knockoffs}]
	
	Suppose $\prx_{j}|\prx_{-j}, \prxk$ has a density with respect to the Lebesgue measure. Since
	\begin{equation*}
	\begin{split}
	&\mathbb P[T^{\textnormal{opt}}_j([\srx, \srxk], \sry) = T^{\textnormal{opt}}_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry), \srx_{\bullet,j} \neq \srxk_{\bullet,j}] \\
	&\quad= \mathbb E[\mathbb P[T^{\textnormal{opt}}_j([\srx, \srxk], \sry) = T^{\textnormal{opt}}_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry), \srx_{\bullet,j} \neq \srxk_{\bullet,j}\ |\ \srx_{\bullet,-j}, \sry, \srxk]],
	\end{split}
	\end{equation*}
	it suffices to show that 
	\begin{equation*}
	\mathbb P[T^{\textnormal{opt}}_j([\srx, \srxk], \sry) = T^{\textnormal{opt}}_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry)\ |\ \srx_{\bullet,-j}, \sry, \srxk] = 0
	\end{equation*}
	for all $\srx_{\bullet,-j}, \sry, \srxk_j$. Since $\mathcal L(\prx_j|\prx_{-j}, \prxk)$ has a density with respect to the Lebesgue measure, so do $\mathcal L(\prx_j|\pry, \prx_{-j}, \prxk)$ and $\mathcal L(\srx_j|\sry,  \srx_{\bullet,-j}, \srxk)$. Therefore, it suffices to show that the set
	\begin{equation*}
	S(c; \sfx_{\bullet,-j}, \sfy) \equiv \{x_{\bullet,j} : \mathbb P(\sry = \sfy|\srx_{\bullet,j} = \sfx_{\bullet,j}, \srx_{\bullet,-j} = \sfx_{\bullet,-j}) = c\} \subseteq \mathbb R^{n}
	\end{equation*}
	has Lebesgue measure zero for all $c, \sfx_{\bullet,-j}, \sfy$. To see this, note that if $\sfx_{\bullet,j} \in S(c; \sfx_{\bullet,-j}, \sfy)$, then
	\begin{equation*}
	\begin{split}
	c &= \mathbb P(\sry = \sfy|\srx_{\bullet,j} = \sfx_{\bullet,j}, \srx_{\bullet,-j} = \sfx_{\bullet,-j})\\
	&= \prod_{i = 1}^n \exp(\eta_i \sfy_i - \psi(\eta_i))g_0(\sfy_i) \\
	&= \exp\left(\sum_{i = 1}^n (x_{ij}\beta_j  + f_{-j}(\sfx_{i,-j}))\sfy_i - \psi(\sfx_{ij}\beta_j  + f_{-j}(\sfx_{i,-j})) + \log g_0(\sfy_i) \right).
	\end{split}
	\end{equation*}
	It follows that
	\begin{equation}
	\begin{split}
	&S(c; \sfx_{\bullet,-j}, \sfy) \\
	&= \left\{x_{\bullet,j}: \sum_{i = 1}^n [\sfx_{ij}\beta_j \sfy_i  - \psi(\sfx_{ij} \beta_j + f_{-j}(\sfx_{i,-j}))] =  \log c- \sum_{i = 1}^n [f_{-j}(\sfx_{i,-j})\sfy_i  + \log g_0(\sfy_i)]\right\}. 
	\end{split}
	\label{likelihood-expression}
	\end{equation}
	Since $\psi$ is strictly convex and $\beta_j \neq 0$, the left hand side is a strictly concave function of $x_{\bullet,j}$, while the right hand side is a constant (with respect to $x_{\bullet,j}\beta_j$). Thus, $S(c; \sfx_{\bullet,-j}, \sfy)$ 
	is the level set of a strictly concave function, and hence has measure zero. Indeed, the level set of a strictly convex function is the boundary of the corresponding super-level set (which must be convex), and the boundary of any convex set has measure zero~\cite{Lang1986}. Thus, the  conclusion~\eqref{nondegeneracy-assumption} thus follows.
	
	%Next, consider knockoffs and assume condition (1b). A very similar argument shows that
	%\begin{equation*}
	%\mathbb P[T^*_j([\srx \ \srxk], \sry) = T^*_j([\srx \ \srxk]_{\text{swap}(j)}, \sry) | \srx_{-j}, \sry, \srxk] = 0,
	%\end{equation*}
	%from which the conclusion~\eqref{knockoffs-no-ties} follows.
	
	Now, assume that $g_\eta$ has a density with respect to Lebesgue measure. Since
	\begin{equation*}
	\begin{split}
	&\mathbb P[T^{\textnormal{opt}}_j([\srx, \srxk], \sry) = T^{\textnormal{opt}}_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry), \srx_{\bullet,j} \neq \srxk_{\bullet,j}] \\
	&\quad= \mathbb E[\mathbb P[T^{\textnormal{opt}}_j([\srx, \srxk], \sry) = T^{\textnormal{opt}}_j([\srx, \srxk]_{\textnormal{swap}(j)}, \sry), \srx_{\bullet,j} \neq \srxk_{\bullet,j}\ |\ \srx, \srxk]],
	\end{split}
	\end{equation*}
	it suffices to show that
	\begin{equation}
	\mathbb P[P(\sry|\srx_{\bullet,j}, \srx_{\bullet,-j}) = P(\sry|\srxk_{\bullet,j}, \srx_{\bullet,-j})\ |\ \srx, \srxk] = 0
	\label{sufficient-knockoffs}
	\end{equation}
	for all $\srx_{\bullet,j} \neq \srxk_{\bullet,j}$. From expression~\eqref{likelihood-expression}, we see that $P(\sry|\srx_{\bullet,j}, \srx_{\bullet,-j}) = P(\sry|\srxk_{\bullet,j}, \srx_{\bullet,-j})$ if and only if
	\begin{equation*}
	\underbrace{\beta_j (\srx_{\bullet,j} - \srxk_{\bullet,j})^T}_{\text{slope}} \sry - \underbrace{\psi(\beta_j \srx_{i,j} + f_{-j}(\srx_{i,-j})) + \psi(\beta_j \srxk_{i,j} + f_{-j}(\srx_{i,-j}))}_{\text{intercept}} = 0.
	\end{equation*}
	Since $\beta_j \neq 0$ by assumption, the slope $\beta_j (\srx_{\bullet,j} - \srxk_{\bullet,j}) \neq 0$ and therefore, the set $\{\sry: P(\sry|\srx_{\bullet,j}, \srx_{\bullet,-j}) = P(\sry|\srxk_{\bullet,j}, \srx_{\bullet,-j})\}$ is a hyperplane (and hence has Lebesgue measure zero). Together with the fact that $\sry$ has a density with respect to Lebesgue measure, this implies the relation~\eqref{sufficient-knockoffs}, so the conclusion~\eqref{nondegeneracy-assumption} follows. 
\end{proof}

\end{document}



